<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Compositional analysis with scaled minhash and gather

\chaptermark{Scaled}

## Introduction

<!--
 CTBQ: What are the goals of compositional analysis in biological systems?
-->

### MinHash sketch: similarity and containment

A data sketch is a representative proxy for the original data focused on queries
for specific properties.
It can also be viewed as a probabilistic data structure (in contrast to
deterministic data structures),
since it uses hashing techniques to provide statistical guarantees on the
precision of the answer for a query.
This allows a memory/accuracy trade-off:
using more memory leads to more accurate results,
but in memory-constrained situations it still bound results to an expected error rate.

The MinHash sketch [@broder_resemblance_1997] was developed at Altavista in the context of document clustering and deduplication.
It provides an estimate of the Jaccard similarity
(called **resemblance** in the original article)
and the **containment** of two documents,
estimating how much of document $A$ is contained in document $B$.
These estimates depend on two processes:
converting documents to sets ("Shingling"),
and transforming large sets into short signatures,
while preserving similarity ("Min-Hashing").
In the original use case the *$w$-shingling* $\Omega$ of a document $D$ is defined as the set of all
continuous subsequence of $w$ words contained in $D$.
*Min-hashing* is the process of creating $W = \{\,h(x) \mid \forall x \in \Omega\,\}$,
where $h(x)$ is an uniform hash function,
and then either

a) keeping the $n$ smallest elements ($\mathbf{MIN}_n(W)$), or
b) keeping elements that are $0 \mod m$ ($\mathbf{MOD}_m(W)$).

$\mathbf{MIN}_n(W)$ is fixed-sized (length $n$) and supports similarity estimation,
but doesn't support containment.
$\mathbf{MOD}_m(W)$ supports both similarity and containment,
with the drawback of not being fixed-sized anymore,
growing with the complexity of the document.

### mash and genomic minhash

Mash [@ondov_mash:_2016] was the first implementation of MinHash in genomic contexts,
relating the $w$-shingling of a document to the $k$-mer composition of genomic
datasets,
and using the $\mathbf{MIN}_n(W)$ fixed-sized formulation for the signature.
Mash needs extra information (the genome size for the organism being queried) to account for genomic complexity in datasets and derive a new score,
the Mash distance,
to bring it closer to previously existing similarity measures in biology
(Average Nucleotide Identity).
This extra information is required because using a fixed-size MinHash leads to
different degrees of accuracy when comparing across highly-diverged organisms
(bacteria to animals, for example),
and it is even more extreme when taking more complex datasets into account (like metagenomes).

## Scaled MinHash

CTBQ: define scaled algorithm here. [From this draft](https://hackmd.io/rxlN-Nv6Q-qhtim7EXCofw?view),
* choose a density parameter D (--scaled)
* divide hash space into bands of size H / D, where H = hash space size (2**64)

CTBQ: it is not clear to me that we should use "Scaled MinHash", as MinHash is explicitly not scaled :). I like DensityHash, based on [durbin's nomenclature of "inverse density](https://github.com/richarddurbin/modimizer) ([also ref](https://github.com/richarddurbin/modimizer/issues/1)).

What is the difference with modulo hash (per Broder 1997)? Modulo is expensive vs less-than operator; convertible between minhash and modhash.
address brad’s comment on f1000 paper: the existing modulo approach has no guarantees on equal-sized (or even equal-fraction as the manuscript claims elsewhere) sub-sampling

Features of scaled:
* subsampling to lower densities
* streaming guarantees (never lose hash value - containment never decreases as you get more data)
* add and subtract hash values (category, “operations directly on sketches”?)
* abundance filter hash values (operations directly on sketches)
* modimizer additions per durbin
  + addition of fwd/rc “sign” in sketch info (see comment)
* poisson statistics / random sequence foo
  + “A value of 2000 means that we are 99.98% likely to choose one k-mer per 10kb window, or to rephrase it, 9998 of 10000 windows of size 10kb will have at least one chosen k-mer in them.” see [numbers from charcoal](https://github.com/dib-lab/charcoal/blob/master/stats/stats.ipynb).

Could also alternatively phrase scaled as lossy compression, more so than minhash.

----

In order to calculate containment

$\mathbf{MIN}_n(W)$ fixed-sized formulation

### Comparison with mod hash

MinHash containment: mash works only on datasets of similar genome size/complexity
ModHash is a solution (more complex dataset will have larger sketch), but falls apart
with very small dataset (might not contain enough hashes for meaningful results)

(intro CMash here)

## Decomposition of queries with gather
<!--
CTB suggestion: put gather in {\tt form} or {\sf form} to distinguish it.
-->

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[ht]
   \DontPrintSemicolon
   \SetKwInOut{Input}{Input}
   \SetKwInOut{Output}{Output}
   \SetKwBlock{Loop}{Loop}{}
   \SetKwFunction{FindBestContained}{FindBestContained}
   \SetKwFunction{Subtract}{Subtract}
   \SetKwFunction{AppendToMatches}{AppendToMatches}
   \Input{query $Q$}
   \Input{a collection $C$ of reference datasets}
   \Input{a containment threshold $T$}
   \Output{a list of matches $M$ from $C$ contained in $Q$}
   \BlankLine
   $M \leftarrow \emptyset$\;
   $Q' \leftarrow Q$\;
   \Loop {
       $best = \FindBestContained(Q', C, T)$\;
       \If{$best = \emptyset$ }{
           break\;
       }
       $\AppendToMatches(M, best)$\;
       $Q' \leftarrow \Subtract(M, Q')$\;
   }
   \KwRet{matches}
   \caption{The gather method}
\end{algorithm} 
 
\begin{algorithm}[ht]
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwFunction{containment}{containment}
  \SetKwBlock{Loop}{Loop}{}
  \Input{query $Q$}
  \Input{a list $C$ of reference datasets}
  \Input{a containment threshold $T$}
  \Output{the match $m$ for $m \in C$ with best containment in $Q$, or $\emptyset$ if no match above threshold}
  \BlankLine
  $best\_containment \leftarrow T$\;
  $m \leftarrow \emptyset$\;
  \For{$c \in C$}{
     $containment \leftarrow \containment(c,Q)$\;
     \If{$containment \ge best\_containment$ }{
       $best\_containment \leftarrow containment$\;
       $m \leftarrow c$\;
     }
  }
  \KwRet{$m$}
  \caption{a \emph{FindBestContained} implementation for a list}
\end{algorithm} 


### Limitations

- increased size (compared with minhash, which is constant size)
- detection threshold (viruses)

## Implementation

### smol

`smol` is a minimal implementation for the Scaled MinHash sketch and the gather method for simulation and verifying results with more featureful tools.
There are two compatible versions,
one in Python and another in Rust,
due to performance requirements when processing large datasets (like metagenomes).
Both versions of the Scaled MinHash implementations use each language standard library sets
(`set` for Python, `HashSet` for Rust)
for storing hashes and efficient set operations (intersection and difference).
Experiments used the Rust version for calculating Scaled MinHash sketches,
and the Python version for running gather and reporting containment scores.
Since they serialize the sketches to a compatible JSON format,
they can be used interchangeably and while computing Scaled MinHash sketches is
orders of magnitude faster in Rust,
for gather running time are similar and in the order of seconds.

The Python version has two external dependencies:
`screed` for sequence parsing,
and `mmh3` for the MurmurHash3 hash function.
Other modules from the standard library are used for JSON serialization (`json`)
and command line parsing (`argparse`).

The Rust version has four direct external dependencies:
`needletail` for sequence parsing and normalization
(similar to what `screed` does in the Python version),
`murmurhash3` for the MurmurHash3 hash function,
`serde_json` for JSON serialization and `structopt` for command line parsing.

### exact

`exact` is an exact k-mer counting implementation in Rust.
It also uses the standard library `HashSet` to store the $k$-mer composition of
a dataset,
and Needletail for sequence parsing.
It differs from `smol` because it stores all canonical $k$-mers,
instead of a subset.
The `bincode` library is used for serialization,
since it provides a simpler binary format output for Rust data types.
The goal is not to have a general purpose tool,
but only a way to calculate the ground truth for containment in a dataset,
and so a more compatible serialization format
(like JSON in the `smol` case)
is not required.

## Evaluation

### Average Scaled MinHash sketch sizes across Genbank domains

<!--
Use genbank signatures
-->

### Containment score methods

<!--
- containment score experiments
CTBQ: Write up these results.
-->



(show method works, even if slow -> leading to introduction of other indices)

(use scaled=1 and scaled=1000, show results are similar.
Use small datasets, since scaled=1 will be huge...)

(maybe use podar dataset?)

(compare with cmash/mash screen here?)

(cmash and mash screen can do `gather` too,
but still require the original data available.
Scaled MinHash allows using the sketches for gather,
which are a fraction of the original data in size.
Additional benefit:
if there are collection updates you don't need to recalculate the query sketch,
and both cmash and mash screen would need the original data for the query
again)

### Implementation for experiments

Experiments are implemented in `snakemake` workflows and use `conda` for
managing dependencies,
allowing reproducibility of the results with one command:
`snakemake --use-conda`.
This will download all data,
install dependencies and generate the data used for analysis.

The analysis is contained in a Jupyter Notebook,
and can be executed in any place where it is supported,
including in a local installation or using Binder,
a service that deploy a live Jupyter environment in cloud instances.
Instructions are available at https://github.com/luizirber/phd
<!-- TODO: replace with zenodo archival DOI -->

<!-- TODO: related work section? It's sort of covered with cmash and mash screen
already -->

## Conclusion and Future Work
