# Efficient indexing of collections of signatures

\chaptermark {MHBT}

## Introduction

<!-- TODO
 - Public genome databases
 - Exponential growth
 - Challenges in indexing, searching and updating the indices for collections of datasets
 - Methods for indexing genomic datasets
-->

Searching for matches in large collection of datasets is challenging when hundreds of thousands of them are available,
especially if they are partitioned and the data is not all present at the same place,
or too large to even be stored in a single system.

Efficient methods for sequencing datasets use exact $k$-mer matching instead of relying on sequence alignment,
but sensitivity is reduced since they can't deal with sequencing errors and biological variation as alignment-based methods can.

<!-- cite some methods, including SBT and Mantis -->

<!-- 
CTBQ: Additional points to raise: in-memory representation of sketches may be too big (!!),
goal here is on disk storage/low minimum memory for "extremely large data" situation.
Also/in addition, want ability to do incremental loading of things.
Note we are not talking here about situations where the indices themselves are too big to download,
could maybe include forward pointer to chp4.

Note, in this chapter you could also include distinction in performance between SBT and LCA DB,
to whit: large scaled works well with LCA (small DB, ~tolerable memory, load all at once, then quite fast)
but low scaled may work (much) better with SBT.
-->

[@marchet_data_2019] classifies indexing strategies for querying large collections of
sequencing datasets into $k$-mer aggregative methods and color aggregative methods.
$k$-mer aggregative methods index the $k$-mer composition for each individual dataset,
and then build structures for retrieving the datasets where the query $k$-mers are present.
Color aggregative methods index the full $k$-mer composition for all datasets,
and then assign a color representing an intersection of datasets where a $k$-mer is present.
This allows reduced space requirements,
since each $k$-mer is stored only once,
but needs extra structures for storing what datasets each color represents.

Both strategies allow the same class of queries,
but with different trade-offs and optimizations:
$k$-mer aggregative methods favor threshold queries
("what datasets contain more than 60% of the query $k$-mers?")
while color aggregative methods tend to be more efficient for specific $k$-mer
queries ("what datasets contain this query $k$-mer?").

<!-- how to dive into hierarchical index and inverted index below? -->

<!-- ctb mar 31

CTBQ: Additional points to raise: in-memory representation of sketches may be
too big (!!), goal here is on disk storage/low minimum memory for "extremely
large data" situation. Also/in addition, want ability to do incremental loading
of things. Note we are not talking here about situations where the indices
themselves are too big to download, could maybe include forward pointer to chp4.

Note, in this chapter you could also include distinction in performance between
SBT and LCA DB, to whit: large scaled works well with LCA (small DB, ~tolerable
memory, load all at once, then quite fast) but low scaled may work (much) better
with SBT.
-->

### Hierarchical index

<!-- 'k-mer aggregative methods in (marchet 2019)' -->

Bloofi [@crainiceanu_bloofi:_2015] is an example of an hierarchical index structure that
extends the Bloom Filter basic query to collections of Bloom Filters.
Instead of calculating the union of all Bloom Filters in the collection
(which would allow answering if an element is present in any of them)
it defines a tree structure where the original Bloom Filters are leaves,
and internal nodes are the union of all the Bloom Filters in their subtrees.
Searching is based on a breadth-first search,
with subtrees being pruned from the search when no matches are found at an internal level.
Bloofi can also be partitioned in a network,
with network nodes containing a subtree of the original tree and only being
accessed if the search requires it.

For genomic contexts,
an hierarchical index is a $k$-mer aggregative method,
with datasets represented by the $k$-mer composition of the dataset and stored in a data structure that allows querying for $k$-mer presence.
The Sequence Bloom Tree [@solomon_fast_2016] adapts Bloofi for genomics and rephrasing the search problem as experiment discovery:
given a query sequence $Q$ and a threshold $\theta$,
which experiments contain at least $\theta$ of the original query $Q$?
Experiments are encoded in Bloom Filters containing the $k$-mer composition of transcriptomes,
and queries are transcripts.

Further developments of the SBT approach focused on clustering similar datasets to prune search
early [@sun_allsome_2017] and developing more efficient representations for the
internal nodes [@solomon_improved_2017] [@harris_improved_2018] to use less storage space and memory.

<!--
example figure for SBT:
http://www.texample.net/tikz/examples/merge-sort-recursion-tree/
-->

### Inverted index {#inverted-index}

<!-- 'color- aggregative methods in (marchet 2019)' -->

An inverted index is a mapping from words in a document back to its location inside the document,
and is commonly used in information retrieval system to find the occurrences of
words in a text [@ziviani_compression_2000].
Another example is the index in the back of a book,
containing a list of topics and in which page they are present.

When indexing the $k$-mer decomposition of genomic datasets,
the inverted index is a color aggregative method,
representable with a map of all hashed $k$-mers in the $k$-mer composition of the datasets in the collection back to
the dataset from where they originated.
Just as words can appear more than once in a text,
hashes can show up in more than one dataset,
and so the inverted index maps a hash to a list of datasets.

kraken [@wood_kraken:_2014] has a special case of this structure,
using a taxonomic ID (taxon) for representing dataset identity.
Datasets share the same ID if they belong to the same taxon,
and if a hash is present in more than one dataset
kraken reduces the list of taxons to the lowest common ancestor (LCA),
which lowers memory requirements for storing the index.
[@nasko_refseq_2018] explores how this LCA approach leads to decreased precision and sensitivity over time,
since more datasets are frequently added to reference databases and the chance of a k-mer being present in multiple datasets increases.

Efficient storage of the list of signatures IDs can also be achieved via representation of the list as colors,
where a color can represent one or more datasets (if a hash is present in many of them).
Mantis [@pandey_mantis:_2018] uses this hash-to-color mapping
(and an auxiliary color table) to achieve reduced memory usage,
as well as Counting Quotient Filters [@pandey_general-purpose_2017] to store the data,
an alternative to Bloom Filters that also support counting and resizing.

### Specialized indices for Scaled MinHash sketches

sourmash [@titus_brown_sourmash:_2016] is a software for large-scale sequence data comparisons based on MinHash sketches.
Initially implementing operations for computing,
comparing and plotting distance matrices for MinHash sketches,
version 2 [@pierce_large-scale_2019] introduced Scaled MinHash sketches
and indices for this new sketch format.
Indices support a common set of operations
(insertion, search and returning all signatures are the main ones),
allowing them to be used interchangeably depending on the use case,
performance requirements and computational resources available.

The simplest index is the `LinearIndex`,
a list of signatures.
Search operations are executed sequentially,
and insertions append new signatures to the end of the list.
Internally sourmash uses LinearIndex as the default index for lists of
signatures provided in the command-line.

#### MinHash Bloom Tree

The MinHash Bloom Tree (`MHBT`) is a variation of the Sequence Bloom Tree (SBT)
that uses Scaled MinHash sketches as leaf nodes instead of Bloom Filters as in
the SBT. 
The search operation in SBTs is defined as a breadth-first search starting in the root of the tree,
using a threshold of the original k-mers in the query to decide when to prune the search.
MHBTs use a query Scaled MinHash sketch instead,
but keep the same search approach.
The threshold of a query $Q$ approach introduced in [@solomon_fast_2016] is
equivalent to the containment 
$$C(Q, S) = \frac{\vert Q \cap S \vert }{\vert S \vert}$$
described in [@broder_resemblance_1997],
where $S$ is a Scaled MinHash sketch.
For internal nodes $n$ (which are Bloom Filters) the containment of the query Scaled MinHash sketch $Q$ is
$$C(Q, n) = \frac{\vert \{\,h \in n \mid \forall h \in Q\,\} \vert}{\vert Q \vert}$
This is the same containment score defined in [@koslicki_improving_2019] for the Containment MinHash to Bloom Filter comparison.

MHBTs support both containment and similarity queries.
For internal nodes the containment $C(Q,n)$ is used as an upper-bound of the similarity $J(Q, n)$:
\begin{equation}
\begin{split}
  C(Q, n) &\ge J(Q, n) \\
  \frac{\vert Q \cap n \vert }{\vert Q \vert} &\ge \frac{\vert Q \cap n \vert }{\vert Q \cup n \vert}
\end{split}
\end{equation}
since $\vert Q \cup n \vert \ge \vert Q \vert$.
When a leaf node is reached then the similarity $J(Q, S)$ is calculated for the Scaled MinHash sketch $S$
and declared a match if it is above the threshold $t$.
Because the upper-bound is being used,
this can lead to extra nodes being checked,
but it simplifies implementation and provides better correctness guarantees.

#### LCA index

The LCA index in sourmash is an inverted index that stores a mapping from hashes
in a collection of signatures to a list of IDs for signatures containing the hash.
Despite the name,
the list of signature IDs is not collapsed to the lowest common ancestor (as in kraken),
the LCA calculated as needed by downstream methods using the taxonomy information
that is also stored separately in the LCA index.

The mapping from hashes to signature IDs in the LCA index is an implicit representation of the original signatures used to build the index,
and so returning the signatures is implemented by rebuilding the original signatures on-the-fly. 
Search in an LCA index matches the $k$-mers in the query to the list of signatures IDs containing them,
using a counter data structure to sort results by number of hashes per signature ID.
The rebuilt signatures are then returned as matches based on the signature ID,
with containment or similarity to the query calculated against the rebuilt signatures.

mash screen [@ondov_mash_2019] has a similar index,
but it is constructed on-the-fly using the distinct hashes in a sketch collection as keys,
and values are counters initialized set to zero.
As the query is processed,
matching hashes have their counts incremented,
and after all hashes in the query is processed then all the sketches in the collection are
checked in other again to quantify the containment/similarity of each sketch in the query.
The LCA index uses the opposite approach,
opting to reconstruct the sketches on-the-fly.

## Results

### Index construction and updating

<!-- TODO
- resource usage (time, cpu, mem)
-->

<!--
  CAMI 2 refseq, 141k signatures, scaled=2000
  LCA: 18.3 GB
  SBT: 524 MB
    sig.name(): 5078 MB
-->


### Efficient similarity and containment queries

<!-- TODO
- discuss COST (cost of single thread)
- sourmash indices benefit from more data. Inverted index
-->

## Discussion

### Choosing an index

The Linear index is appropriate for operations that must check every signature,
since it doesn't have any indexing overhead.
An example is building a distance matrix for comparing signatures all-against-all.
Search operations greatly benefit from extra indexing structure.
The MHBT index and $k$-mer aggregative methods in general are appropriate for searches with query thresholds,
like searching for similarity or containment of a query in a collection of datasets.
The LCA index and color aggregative methods are appropriate for querying which datasets contain a specific query $k$-mer.

As implemented in sourmash,
the MHBT index is more memory efficient because the data can stay in external memory and only the tree structure for the index
need to be loaded in main memory,
and data for the datasets and internal nodes can be loaded and unloaded on demand.
The LCA index must be loaded in main memory before it can be used,
but once it is loaded it is faster,
especially for operations that need to summarize $k$-mer assignments or required repeated searches.

Due to these characteristics,
and if memory usage is not a concern,
then the LCA index is the most appropriate choice since it's faster.
The MHBT index is currently recommended for situations where memory is limited,
such as with smaller scaled values ($s\le2000$)
that increase the size of signatures,
or when there are a large number (hundreds of thousands or more) of datasets to index.

### Converting between indices

Both MHBT and LCA index can recover the original sketch collection.
In the MHBT case,
it outputs all the leaf nodes.
In the LCA index,
it reconstruct each sketch from the hash-to-dataset-ID mapping.
This allows trade-offs between storage efficiency,
distribution,
updating and query performance.

Because both are able to return the original sketch collection,
it is also possible to convert one index into the other.

### Limitations and future directions

<!--
- Other indices
  * Fundamentally a scaled minhash is a subset of the k-mer composition of a
  dataset, so any index from https://www.biorxiv.org/content/10.1101/866756v1
  ("Data structures based on k-mers for querying large collections of sequencing datasets")
  can be used to scale,
  and given that gather is defined over a collection of signatures the indices
  can also be used to improve gather performance.

- sourmash is currently single threaded, but that's an implementation detail.
  Parallel queries are possible (in a shared read-only index)

- The LCA index can be implemented in external memory by using memory-mapped files,
avoiding the need to load it all in memory.
-->

## Conclusion



## Methods

### Implementation

Focused on the user experience via the command-line interface and Python API,
it implemented the core data structures in C++ for efficiency and exposed it to
Python with an extension (written in Cython).
The Python API allows fast prototyping of new ideas and interoperability with
the larger scientific Python ecosystem,
as well as access to better tooling for testing and software distribution.

### Experiments
