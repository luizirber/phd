# Distributed signature calculation with wort

\chaptermark {wort}

## Introduction

\todoin{
- Add one paragraph, "In Chapter XX we introduced YY".
- This showed us the following problems around calculating yada foo.
}


Signature calculation in sourmash is a low memory and streaming process.
Low memory because a signature retains a subset of the original data,
and streaming because once a chunk of the data is processed it is not necessary to keep it or refer to it again.
The calculation process is frequently I/O bound on reading data,
unless very fast access to the data is available.

<!--
CTB: here it is also important to note that sourmash signatures are typically
quite small, and so the calculation process is inherently asymmetric in
terms of communication: the input data is 1000-100000 times larger than the
output data, which is what allows wort-like solutions to work in the first place.

(Maybe throw in a comparison to folding@home, but differentiate because those require significant computation while wort requires significant data.)
-->

On top of fast access to one dataset,
indexing large public genomic databases also requires considering another axis:
since there are millions of datasets in these databases,
embarrassingly parallel approaches to access the data are necessary too,
because even with the fastest possible connection it would still take too long to process them all serially.
Given that processing a dataset into a signature can be done independently
of other datasets,
a system that can spawn a large number of jobs can potentially keep up with the rate of new dataset inclusion,
and also make it feasible to calculate signatures for the current datasets.

In this sense,
getting access to the data and downloading it is the main bottleneck,
especially considering that high-speed network connections are expensive.
At the same time,
given the embarrassingly parallel nature of signature calculation,
multiple workers can be spawned in different computational infrastructure
(academic clusters, public cloud instances, and even browsers)
with potentially independent network connections.
Then the limitation is how fast data can be served from the public genomic database being indexed; this is inherently a limitation of centralized data storage.

<!--
CTB Q: How does this change if the datasets / genomic databases are in the cloud, and so are accessible from multiple independent compute servers? This is the "scale out" approach and avoids the 1000x slower speed of WAN access. Note data egress charges / zone specificity.

Statement of what this chapter is about: soursigs and wort for exploring
(crowdsourced?) ways of calculating signatures for large public databases.
-->

## Methods

The Sequence Read Archive stores raw sequence data for sequencing experiments,
and is part of the International Nucleotide Sequence Database Collaboration (INSDC),
with the European Nucleotide Archive (ENA) and the DNA Database of Japan (DDBJ) being other members.
Submission to any of them are shared with the others,
although with different methods for accessing and potentially distinct data formats.

The Integrated Microbial Genomes and Microbiomes (IMG/M) contains assembled genomes and metagenomes for sequencing done at the Department of Energy's Joint Genome Institute,
focusing on environmental samples.
Because the sequencing data is already assembled,
it is closer to GenBank and RefSeq than the Sequence Read Archive,
which hosts raw sequence data.

<!-- ficus grant
http://ivory.idyll.org/blog/2017-ficus-nersc-jgi-sourmash.html
-->

### Data selection

#### Sequence Read Archive

Given the large amount of data available and the biological diversity of the datasets,
the initial prototype focused on single genome microbial datasets (excluding metagenomes).
The datasets were selected with a query to the Sequence Read Archive to retrieve enough metadata for processing (dataset ID, size and download location).
`wort` doesn't aim to store and provide this metadata explicitly,
instead opting to point to the original database metadata for more information.

#### IMG

Through a FICUS grant we also had access to the JGI IMG/M database,
and access to the NERSC supercomputers to calculate signatures.
<!-- size of IMG at that time
 - 65k genomes
 -->
Metadata was provided independently of online access to IMG,
and represented a snapshot of the data available on the database at that point. <!-- TODO: put date here? -->
The sequencing data was available in a shared storage unit,
and processing was done through NERSC's Cori scheduling system.

<!-- Link IMG snakemake repo
-->

### The worker

Based on the initially retrieved metadata,
each dataset was processed by a worker.
Each worker runs an instance of `sourmash`,
with additional software for accessing the data or uploading it to more permanent storage,
depending on what database is being calculated.

For IMG the data was available in a shared filesystem,
so `sourmash compute` could access it directly.

For the SRA the worker uses `fastq-dump` (from the `sra toolkit`) to download the data and stream it through a UNIX pipe into a `sourmash compute` process.
Since the data is being streamed,
it is not being stored locally by the worker,
so if a signature need to be recalculated the data needs to be downloaded again.
Signatures would only need to be recalculated if the `sourmash compute` parameters are changed,
and the initial parameters tried to accommodate for it and cover more use cases,
which leads to a larger signature (but still a fraction of the size of the original data).

### Coordination

The first prototype (named `soursigs`) used `snakemake` to
1) query metadata from SRA
2) for each dataset, spawn a worker job to process it and generate a signature
3) upload the signature to permanent storage
<!-- Link soursigs repo for initial prototype
https://github.com/dib-lab/soursigs/
-->

\todoin{
- CTB fill in
}

This system was able to process 412k datasets over two weeks,
...
<!-- link to soursigs posts?
https://blog.luizirber.org/2016/12/28/soursigs-arch-1/
http://ivory.idyll.org/blog/2017-sourmash-sra-microbial-wgs.html
 -->
but processing was still limited to one system executing all the workers jobs.
Even with the system being a cluster and providing access to many compute nodes,
the external network connection was still shared between all nodes.

<!--
Note: this initial prototype only read a small part of the stream. This was because our scientific goal was to identify novel data sets, and we could take advantage of the random nature of shotgun sequencing to yada yada. Ref syrah, scaled.
-->

### Storage

Signatures were initially stored locally in the system where they were processed
(NERSC Cori for IMG, MSU HPCC for SRA).
None serve as permanent or public storage,
since they are restricted systems and access can be revoked once projects are concluded.

\todoin{
- CTB Fix below
}

For convenience,
data was uploaded to Amazon S3,
which makes it easier to share the data but also has drawbacks:
	- Storage and data access incur additional costs,
		especially if the data becomes a popular resource and used frequently.
	- Even with all the infrastructure and level of service that AWS provides,
		it is a single point of failure and is not easy to mirror in other places without generating new URLs
		or using some sort of load balancer.

Other data repositories like Zenodo are better for archival purposes,
but not as appropriate for a system that is frequently updated. (=> IPFS chp 4 motivation)

<!-- data was also uploaded to IPFS, but maybe keep it to discuss in depth in the next section? -->

## From soursigs to wort

<!-- how and why did soursigs evolve into wort? -->
<!-- mirror structure from previous section, showing how things changed? -->

<!--
CTBQ: which limitation is being overcome? It's ok to repeat and be specific.
-->
`wort` was created to overcome this (which??) limitation and allow more computational systems to participate in the process,
building an heterogeneous system that doesn't depend on similar resources being available for each worker.
Two major changes for this:
	1) encapsulate workers in a Docker container, which is also executable by Singularity.
		 Docker containers are useful for cloud instances,
		 but are usually not available in academic clusters.
		 Similarly, Singularity tends to be present in academic clusters,
		 but is not as convenient in cloud infrastructures.
		 <!-- encapsulation is also easier to do because all dependencies are available on conda channels,
		 especially bioconda and conda-forge -->
	2) Use a messaging queue for job submission that can be accessed by workers in any computational system. <!-- (here note this is still centralized) -->

Another goal for `wort` is creating a basic API for common tasks that allow external interactions without needing knowledge about internal implementation.
Since the internal details are not exposed it allows refactoring and reimplementing the infrastructure with other technologies not used in the original version.

In this sense,
the original version is implemented in a more traditional (and centralized) approach,
with a web application written in Flask serving the API and sending job requests to an Amazon SQS queue.
Workers connect to the queue and grab jobs for execution,
and upload results to Amazon S3.

```{r wortArch, eval=TRUE, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, cache=TRUE, out.width="500px", fig.width=4, fig.cap="wort architecture"}
knitr::include_graphics('figure/arch.png')
```

\todoin{
- Figure 4.1 needs a caption.
}

<!-- drop the "a little centralization goes a long way" paper here -->
The coordination of the system is still centralized,
but this new infrastructure allows workers to be executed in any system that has Docker or Singularity support,
which includes most academic clusters and cloud providers,
as well as local workstations and personal laptops.
Since signature calculation is I/O bound,
CPU resources consumption is proportional to how fast the network connection can provide data,
and the CPU is idle waiting for data most of the time.

<!--
(Use "asymmetric" for the way data behaves: lots goes in, little goes out, compute is not actually that significant)
-->

Since signature calculation jobs are independent of each other and don't have to communicate with each other,
the single point of failure of this approach is the centralized coordination.
At the same time,
developing a system is easier if there is no coordination required between most parts of the system,
and given the exploratory nature of this experimental system having an initial implementation that can be used to clarify use cases and figure out the requirements and interfaces needed is very valuable.

Over time the implementation can become less dependant on the original platform used (AWS),
and move to more decentralized approaches.
<!-- It's easier to test and refactor than coming up with the perfect design upfront -->
Because the original version is already working,
refactoring the internal implementation while keeping all tests working allow improvements to the system without breaking the public API and clients using `wort`.


## Results

\todoin{
- CTB
}

- 1M+ signatures
- 2 TB compressed

## Discussion

\todoin{
- CTB
}

<!--
We set out to solve a problem...

We designed a system to take advantage of custom features of the task:
asymmetric, etc.

This system was centralized in the following way, but otherwise relied on independent workers.

The system worked pretty well within its limitations. Processed XX YY ZZ etc etc.

Challenges of centralization. Challenges of single database to
access. Are there better solutions that rely on the central people to
do things differently, e.g. allow docker containers running on their
hardware? (Kind of what the cloud allows, sure. But you can make the
point that no, not really, you're always limited by the centralized
nature of the design.) So in chp4 we explore a different more
decentralized approach.

CTB: Attacks on this system by bad faith workers? Access restrictions on queue.

Maybe reference "linda tuple space" in the paper somewhere?

CTBQ: What points do you want to make here?
-->

## Conclusion

\todoin{
- CTB
}

<!--
Distributed workers can be an efficient and effective solution but only insofar as the centralized database can scale to hand data to them fast enough.

TODO:
* pull text down from above into discussion as you think it makes sense
* smoothen out additional text from CTB discussion, with extra points
* move on to chp5
-->
