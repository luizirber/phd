# Decentralizing indices for public genomic data

\chaptermark {decentralized}

## Introduction

\todoin{
Extend discussion from centralized resources from previous chapter.

- Failure modes for public genomic resources: de-funding, too much data, most
  data infrequently accessed, bandwidth limitations

- Current solutions: move to cloud infrastructure, but charge from user for data
  transfer out of cloud infra (SRA and AWS/GCP)
}


### Content addressable storage

One example of content-addressable storage is git.
Every commit in git has a hash attached to it,
calculated based on the content plus some extra metadata
(author and parent commit, for example),
creating a git object.
<!-- mention other types of git objects? -->
The hash is used to index and access the git objects in the repository history,
and they are stored usually stored in the `.git/storage` directory.
The commit history is a directed acyclic graph (DAG) of these git objects.
Since the parent commit hash is also included in the calculation this
creates a structure where changing (or tampering) with a commit changes all the
hashes depending on this commit,
in turn making the DAG into a Merkle DAG.
<!-- TODO: cite merkle dag/structures -->

While git uses content-addresses for storing the data,
these git objects are not shareable with other repositories automatically.
Finding and synchronizing repositories is an explicit operation,
so when a new `clone` command is executed the user needs to provide the location of the original repository,
and all the objects will be pulled from this one repository,
even if there are many other copies spread around the network
(or even other copies already exist locally in the machine).

Every git repository is self-contained
(can operate only with a local copy of the data)
and git provides operations to synchronize between repositories,
but there is no networking layer for sharing automatically the data with other repositories:
pushing and pulling changes are operations using a single remote,
ant they need to be explicitly configured beforehand.

git was originally created for the Linux kernel development,
where change proposals are usually communicated through mailing lists,
but services that streamlined the process for other development models appeared,
with GitHub being the dominant player.
GitHub is a centralized service for hosting repositories,
and the majority of users don't synchronize changes with each other directly,
opting to push and pull changes from GitHub instead.

### Distributed Hash Tables (DHT)

DHTs originated from research in peer-to-peer (P2P) systems,
<!-- TODO: cite dump -->
with the goal of allowing queries for which node had a specific piece of data.
It exposes a simple key-value store,
but each node of the network is also responsible for storing a small chunk of
the store,
with data available in multiple nodes for additional resiliency.
In order to maintain an operational DHT,
protocols for distributing the data between nodes,
and dealing with network partitions (leading to node disconnection) are
required.

In the early 2000s P2P networks started using DHTs in large scale for distributing content,
with BitTorrent as a representative example.
<!-- TODO: cite? -->
A torrent is a file containing metadata for any content being shared in the network,
and includes information about size, blocks (pieces of the data) and checksums
to verify that the data was transmitted without errors.
Initial implementations of the BitTorrent protocol used servers ("trackers") for storing
which peers had which parts of the torrent content,
but content transfer didn't involve these servers:
they were used only as coordinators.
If a server went offline,
peers couldn't discover any new peers,
and so the network was vulnerable to anything that affected the server.
With DHTs,
instead of storing peer information in a server,
the torrent file was added to the DHT,
and peers could announce (again at the DHT) which blocks of the torrent they have.

### The Interplanetary File System (IPFS)

One drawback of using the torrent files as the unit of data is that creating
another torrent file with the same content means that the two torrent files can't
see blocks from each other (even if they exactly the same).
Torrents bring the distributed network routing for peer discovery and content addressing at the torrent file level,
but they lack content-addressable storage at the block level.

On the other hand,
git has content-addressable storage,
and one could create a DHT that indexes git objects addresses,
mapping it to what nodes in the network have it available.
Instead of querying a single remote node,
this would allow git to fetch remote objects from many sources,
and any user that cloned the same repository is a potential provider for the data.

The Interplanetary File System (IPFS)
<!-- TODO cite benet 2014 -->
builds on these two ideas.
IPFS addresses are hashes of the content organized as a Merkle DAG like git,
but using a DHT for peer and content discovery,
like Bittorrent.

## Methods

<!-- previous TODO:
- The SBT index is amenable to content addressable approaches.
  Leaf nodes are effectively immutable (since original datasets don't change),
  and adding new leaf nodes to an existing tree doesn't change the other leaf
  nodes. Some internal nodes are changed, but most also stay the same.

- Allow operations to continue working if internal node data is not available
  (with a performance hit, but doesn't fail)

- Leaf-only SBT, basically the original MH collection with a tree structure,
  but without internal node data. Allow reconstructing the full SBT.
-->

### The SBT index is amenable to content addressable approaches

The SBT index introduced in chapter 2 has structural features that make it a
good fit for content-addressable approaches.

Leaf nodes in an SBT are signatures for a specific dataset,
and are effectively immutable for public genomic databases,
since any changes to a dataset in these databases tend to generate a new version or identifier.
Signatures can also change if other parameters for calculating their sketches are used,
but having a default set of "good enough" parameters like the ones defined in `wort`
<!-- TODO: link to previous chapter? -->
allows general queries that can later be refined once a candidate subset of the database is found.

<!-- TODO figure from recomb 2017 paper showing insertion changes for current method -->

Insertion of new signatures to an SBT trigger changes in the path from new leaf
location to root of the tree,
but all other nodes are left unchanged.
Even if new methods for signature placement in the tree are developed in the future,
in the worst case at most half of the total nodes (the internal nodes) would change,
but the other half (the leaves) would stay the same.

### Flexible and resilient indices with leaf-only SBTs

Since SBTs follow a similar principle to Merkle DAGs,
where the content of an internal node depends on the content of the subtree defined by using the internal node as the root,
this also means that given only the leaves of a tree and their placement it is possible to reconstruct all the internal nodes (and so the full SBT).
This opens the possibility of distributing SBT indices as leaf-only SBTs:
a description of the tree structure with the leaves placement,
and where to download the content for each leaf.
This information is enough to reconstruct the full index,
and also prioritizes storage and distribution of a minimal amount of data,
since the internal nodes content is redundant information (for allowing faster queries)
and are potentially mutable (on new signature insertions to the index),

While internal nodes are bad candidates for long-term archival,
they are still good candidates for short-term caching for performance,
either locally or loaded from the network.
A trade-off is making internal nodes available in the network,
but not failing local operations with the index if they fail to load:
either recalculate the internal node (more expensive)
or proceed with operations where this is acceptable.
For the latter case,
a search where an internal node is missing can proceed by queueing the children of the node for search,
which is less efficient than a regular search (which could potentially stop at that node and avoid checking the subtree),
but is a better user experience than failing completely.
In the worst case where only the leaves are available,
this search turns into a linear scan over a list.

## Results

### Hosting an SBT index on IPFS

SBTs in \emph{sourmash} use a `Storage` instance to save and load data,
with the default storage being a hidden directory with all the content for internal nodes and leaves and,
in the future (version 4),
Zip files.
\emph{sourmash} support for IPFS is implement as an instance of the `Storage` interface that communicates with an IPFS node running at the same machine or on a remote location.
Since it follows the same interface,
any operations on other storages is also supported with the IPFS backend.

Saving data is implementing by sending an `add` request to the IPFS node and pinning the data.
A pin is an operation that instructs an IPFS node to avoid garbage-collecting the data,
making a good fit for long-term availability.
Once the data is saved,
a new IPFS multihash is returned,
which serves as a location for addressing this data in the network from now on.
Loading data involves passing an IPFS multihash to the storage,
and returns a byte buffer ready for consumption.

### sourmash operations using a decentralized index

sourmash version 3.3.0 was used for the experiments.
All experiments are running a `sourmash gather` command on an SBT index with 5914 signatures.
These signatures are a subset of the prepared Genbank database that sourmash provides,
while the query signature is an iHMP metagenome.
<!-- TODO: more info here, obvs) -->

The three machines used for the experiments reflect different levels of performance:
- `takver` is a workstation with fast CPU (AMD Ryzen 9 3900X 12-Core running at 3.80 GHz),
  memory (64 GB DDR4 2133 MHz)
  and storage (2 TB NVMe Samsung SSD 970 EVO Plus),
  connected to a Gigabit local network with access to the UC Davis network.
- `datasilo` is a compute stick,
  a system with limited computational resources aimed at the low cost market.
  It has an Intel Atom x5-Z8330 CPU running at 1.44 GHz,
  2 GB DDR3 1600 MHz of memory,
  a 32 GB internal eMMC storage,
  and it is connected to the same Gigabit local network as `takver`.
- `rosewater` is a 2015 laptop with resources typically found on a regular user notebook:
  an Intel Core i7-5600U CPU running at 2.60GHz,
  16GB DDR3 1600MHz of memory,
  and 1 TB mSATA Samsung 860 SSD for storage.
  It is not on the same network as the other machines,
  and it connects to the internet with a 20 Mbps DSL connection.

#### SBT index with an IPFS storage

Experiment 1) uses a Zipped SBT index,
with a storage using a Zip file for holding the tree structure and all the nodes compressed data.
This will be the default format in sourmash 4,
and is more convenient than the previous hidden dir storage.
The goal of this experiment is to define a baseline to compare both each individual machine performance and the decentralizing approaches described in previous sections.

Experiments 2) and 3) use the same SBT index as 1), but with an IPFS backend as storage.
2) and 3) are the same experiment,
with the only difference being that 3) is executed right after 2),
and so all the data is already available in the local IPFS node and no network transfer is needed.
The goal is to show the performance impact of a cold start (experiment 2),
and what is the overhead that IPFS imposes when used in conditions more similar to experiment 1),
when all the data is available.

Table: (\#tab:sbt-ipfs) Performance of SBT operations with IPFS storage. Units in seconds, with fold-difference to the baseline in parenthesis.

 experiment              takver      datasilo        rosewater
-------------           --------  --------------  ------------------
1) local (zip)              9        43 (4.7x)        14 (1.5x)
2) ipfs                    12       115 (9.5x)       415 (34.5x)
3) ipfs again              12        64 (5.5x)        23 (1.9x)

Experiment 1) is a measure of raw processing power,
and as expected `takver` is the fastest one.
`datasilo` suffers from the low cost components in the system,
taking 4.7 times longer to run.

Experiment 2) shows the impact of loading the data from the network.
For `takver` the impact is smaller,
since it is also the data seeder for the other machines,
and ends up showing the performance overhead of using IPFS
(since `sourmash` communicates with an external IPFS process to load/save the data).
The IPFS storage is 33% slower than the Zip storage (9 to 12 seconds).
For `datasilo`,
which is in the same Gigabit network as `takver`,
using IPFS is 2.6 times slower (43 to 115 seconds).
Data loading in `sourmash` storages is sequential,
so even with a very fast network the throughput is low.
This effect is even clearer with `rosewater`,
which has a slower connection with higher latency and is almost 30 times slower (14 to 415 seconds) than loading from a Zip file.

Experiment 3) show the impact of IPFS overhead if the data is already available in the local IPFS node.
For `takver` it is the same as experiment 2),
but for the other machines the results are closer to what happened with `takver`,
with `datasilo` being 48% slower and `rosewater` being 64% slower than the Zip storage.
Despite `rosewater` having the worst relative performance
(64% versus 33% and 48% for the other machines),
it is important to point out that all results are in the order of seconds,
and since `datasilo` has lower performance hardware it takes longer in absolute numbers,
being over a minute for this experiment.
<!-- TODO: A better experiment would be taking `rosewater` to the lab,
     and connect to the same network. Alas, we are in the middle of a pandemic,
     and I want to avoid going there if possible =] -->

#### Leaf-only SBTs

The goal of leaf-only SBTs is to increase resilience and reduce storage space,
trading it for performance costs.
Experiments 4), 5) and 6) changes the SBT index to a leaf-only SBT,
but otherwise they follow the same conditions as experiments 1-3.
<!-- The goal is to verify the impact of reconstructing the internal nodes and also loading all the leaves from storage
(since all the leaves are required for reconstruction).
-->

Table: (\#tab:leaf-ipfs) Performance of a leaf-only SBT operations with IPFS storage. Units in seconds, with fold-difference to the baseline in parenthesis.

<!--
 experiment              takver      datasilo        rosewater
-------------           --------  --------------  ------------------
1) local (zip)              9        43 (4.7x)        14 (1.5x)
2) ipfs                    12       115 (9.5x)       415 (34.5x)
3) ipfs again              12        64 (5.5x)        23 (1.9x)
-->

 experiment              takver      datasilo        rosewater
-------------           --------  --------------  ------------------
4) leaf-only (zip)         20        92 (4.6x)        35 (1.7x)
5) leaf-only ipfs          31       307 (14.6x)     1267 (40.8x)
6) leaf-only ipfs again    31       170 (5.4x)        63 (2x)

The relative performance difference between machines are in the same order of magnitude when comparing experiments 4-6 to their counterparts in the previous section.
We can see more clearly the performance impact of reconstructing the internal nodes in experiment 4),
where all systems take twice as long to run when compared to experiment 1).

Experiment 5) show a drawback of the leaf-only approach,
combined with the sequential nature of sourmash storages:
since all the leaves are required for reconstruction,
it will potentially have to download more data than a query to an SBT fully loaded into an IPFS storage,
since it can't prune subtrees from the search space (and so avoid downloading them).
`rosewater` is particularly impacted,
with a runtime of more than 20 minutes.


<!--
- issues with current results:
  takver, datasilo and rosewater are wildly different machines, too many confounding factors?
  At the same time, it's good to have an underpowered machine like datasilo,
  and a regular laptop like rosewater because they show clearly what are the CPU/mem or network trade-offs.

a small experiment:

- load a current SBT into IPFS
    SBT: 5914 signatures
      sourmash index -k 51 --traverse-directory index.sbt.zip sigs/
      sourmash index -k 51 --traverse-directory index.sbt.json sigs/
      sourmash storage convert -b ipfs index.sbt.json

    leaf only:
      sourmash index -k 51 --sparseness 1 --traverse-directory index_leaf.sbt.zip sigs/
      sourmash index -k 51 --sparseness 1 --traverse-directory index_leaf.sbt.json sigs/
      sourmash storage convert -b ipfs index_leaf.sbt.json

- do a local query with the new SBT and compare performance
    takver: AMD Ryzen 9 3900X 12-Core @ 3.80GHz, 64GB DDR4 2133 MHz, 2TB Samsung SSD 970 EVO Plus, 1Gbps network
      local: 9s
      ipfs: 12s

      leaf-only: 20s
      leaf-only ipfs: 31s

- do a query with a computer in the same network and the new SBT
- run two queries in a row, showing impact of loading data in the first one,
  and how the second time it is queried is similar to a local query

    datasilo: Intel(R) Atom(TM) x5-Z8330  CPU @ 1.44GHz, 2GB DDR3 1600 MHz, 32GB eMMC, 1 Gbps network
              https://ark.intel.com/content/www/us/en/ark/products/91065/intel-compute-stick-stk1aw32sc.html
      local: 43s
      ipfs: 1m55s
      second time: 1m04s

      leaf-only: 1m32s
      leaf-only ipfs: 5m07s
      second time: 2m50s

- do a query with a computer in another network
    rosewater: Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz, 16GB DDR3 1600MHz, 1TB Samsung SSD 860, 2.2 Mbps network
      local: 14s
      ipfs: 6m55s
      second time: 23s
      time to download zipped DB: 1m53s

      leaf-only: 35s
      leaf-only ipfs: 21m7s  <- not having any internal node leads to having to load all leaves, one by one...
      second time: 1m3s  <- all leaves already available, overhead is recalculating internal nodes
      time to download zipped DB: 51s

-->


## Discussion

\todoin{
- CTB
}

<!--
If there is a system for sharing the node contents,
this means that users once users download an SBT index they can become new seeders for future users.
In the worst case,
where only the initial seeder is available,
it becomes a centralized system and the whole network depends on the resources and bandwidth that the seeder provides,
but this is typically the situation where most bioinformatics web services are anyway.


IPFS was chosen because it provides implementations in multiple programming languages,
and has a large user and developer base that can drive development of useful features in the long term.
IPFS is a good candidate for the features required to do this,
but any system with similar features can also be used.
-->


### Future work

<!-- initial TODO
 - Updating and syncing separate indices. Mergeable replicated data types

 - Integrating into wort. wort as minimal shim over decentralized infra.
-->

In the current version only the content of each node (internal or leaf) is stored in a decentralized manner,
but index construction and updating is not synchronized.
There is no protocol for merging changes if multiple users start from the same index but update it with their own data.
This is an active area of research,
but recent developments involving Conflict-Free Replicated Data Types (CRDTs)
<!-- TODO: cite -->
and especially Mergeable Replicated Data Types (MRDTs) point in promising directions,
with the latter defining semantics for merging tree-like structures that are applicable to SBTs.

While these methods can provide the technical means for fully decentralized indices,
they don't solve organizational problems that also arise,
or even if the system design should aim for a global index that is updated with every single new signature that
is added to an initial index.
For public databases there is a ground truth that provides what signatures should be present in an index,
but private collections of signatures could be organized as a group of users
sharing a single index and updating each other local copies when any user add
new signatures.

`wort` initially focused in calculating signatures for public genomic databases,
but since it already has the immutable parts of SBTs (the signatures) available
on IPFS it is a good candidate for being extended and also provide leaf-only
SBT indices for these databases.

<!-- TODO this might be better in chapter 4? -->
While the `wort` service is currently implemented as a central server that coordinates new submissions and requests for computing new signatures,
the main benefit it brings is a clear API that doesn't have to change if the implementation start using other technologies.
As an example,
the first prototype uploaded the data only to AWS S3,
and the IPFS upload was added later.
Moreover,
there is also a command-line interface (CLI) that currently communicates directly with the central API for downloading data,
but following the same principle of the public interface of the CLI not changing
if the implementation changes it can serve as a minimal shim over more decentralized approaches.

## Conclusion

\todoin{
- CTB
}
