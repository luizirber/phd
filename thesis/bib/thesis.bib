
@inproceedings{heule_hyperloglog_2013,
	title = {{HyperLogLog} in practice: Algorithmic engineering of a state of the art cardinality estimation algorithm},
	shorttitle = {{HyperLogLog} in practice},
	pages = {683--692},
	booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
	publisher = {{ACM}},
	author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
	date = {2013},
}

@inproceedings{alon_space_1996,
	title = {The space complexity of approximating the frequency moments},
	url = {http://dl.acm.org/citation.cfm?id=237823},
	pages = {20--29},
	booktitle = {Proceedings of the twenty-eighth annual {ACM} symposium on Theory of computing},
	publisher = {{ACM}},
	author = {Alon, Noga and Matias, Yossi and Szegedy, Mario},
	urldate = {2015-09-02},
	date = {1996},
}

@article{giroire_directions_nodate,
	title = {Directions to use probabilistic algorithms for cardinality for {DNA} analysis},
	abstract = {Probabilistic algorithms for cardinality (see for example [1]) allow to estimate the number of distinct words of very large multisets. Best of them are very fast (only few tens of {CPU} operations per element) c and use constant memory (standard error of √M attained using M units of memory) to be compared with the linear memory used by exact algorithms. Hence they allow to do multiple experiments in few minutes with few {KiloBytes} on files of several {GigaBytes} that would be unfeasible with exact counting algorithms. Typically they are used for applications in the area of databases (see [2]) or networking (see [3] or [4]). Such algorithms are used here to analyze base correlation in human genome. The correlation is measured by the number of distinct subwords of fixed size k (10 bases for example) in a {DNA} piece of size N. The idea is that a sequence with few distinct subwords is more corrolated than a sequence of same size with more distinct subwords. Three different angles of study are introduced:- Are all possible words (4 k subwords of size k) present in the genome or, on the contrary, are a lot of patterns forbidden?- Is the genome homogeneus or are some areas more corrolated than others? In the late case, is it possible to recognize or have location hints, in a fast and easy way, for regions of different natures such as repetitions, coding or not coding regions?- What is the arrival rate of distinct subwords in the genome when considered as a sequence read},
	pages = {2006},
	journaltitle = {Journées Ouvertes Biologie Informatique Mathématiques},
	author = {Giroire, Frédéric},
}

@article{flajolet_hyperloglog:_2008,
	title = {{HyperLogLog}: the analysis of a near-optimal cardinality estimation algorithm},
	url = {http://www.dmtcs.org/dmtcs-ojs/index.php/proceedings/article/viewArticle/914},
	shorttitle = {{HyperLogLog}},
	number = {1},
	journaltitle = {{DMTCS} Proceedings},
	author = {Flajolet, Philippe and Fusy, Éric and Gandouet, Olivier and Meunier, Frédéric},
	urldate = {2015-09-29},
	date = {2008},
}

@article{holley_bloom_2016,
	title = {Bloom Filter Trie: an alignment-free and reference-free data structure for pan-genome storage},
	volume = {11},
	issn = {1748-7188},
	url = {http://dx.doi.org/10.1186/s13015-016-0066-8},
	doi = {10.1186/s13015-016-0066-8},
	shorttitle = {Bloom Filter Trie},
	abstract = {High throughput sequencing technologies have become fast and cheap in the past years. As a result, large-scale projects started to sequence tens to several thousands of genomes per species, producing a high number of sequences sampled from each genome. Such a highly redundant collection of very similar sequences is called a pan-genome. It can be transformed into a set of sequences “colored” by the genomes to which they belong. A colored de Bruijn graph (C-{DBG}) extracts from the sequences all colored k-mers, strings of length k, and stores them in vertices.},
	pages = {3},
	journaltitle = {Algorithms for Molecular Biology},
	shortjournal = {Algorithms for Molecular Biology},
	author = {Holley, Guillaume and Wittler, Roland and Stoye, Jens},
	urldate = {2016-04-25},
	date = {2016},
	keywords = {Bloom filter, Colored de bruijn graph, Index, Pan-genome, Population genomics, Similar genomes, Succinct data structure, Trie, compression},
}

@article{mohamadi_ntcard:_nodate,
	title = {{ntCard}: a streaming algorithm for cardinality estimation in genomics data},
	url = {https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btw832/2832780/ntCard-a-streaming-algorithm-for-cardinality},
	doi = {10.1093/bioinformatics/btw832},
	shorttitle = {{ntCard}},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Mohamadi, Hamid and Khan, Hamza and Birol, Inanc},
	urldate = {2017-04-25},
}

@article{melsted_kmerstream:_2014,
	title = {{KmerStream}: streaming algorithms for k -mer abundance estimation},
	volume = {30},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/30/24/3541/2422237/KmerStream-streaming-algorithms-for-k-mer},
	doi = {10.1093/bioinformatics/btu713},
	shorttitle = {{KmerStream}},
	pages = {3541--3547},
	number = {24},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Melsted, Páll and Halldórsson, Bjarni V.},
	urldate = {2017-04-25},
	date = {2014-12-15},
}

@article{benet_ipfs_2014,
	title = {{IPFS} - Content Addressed, Versioned, P2P File System},
	url = {http://arxiv.org/abs/1407.3561},
	abstract = {The {InterPlanetary} File System ({IPFS}) is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. In some ways, {IPFS} is similar to the Web, but {IPFS} could be seen as a single {BitTorrent} swarm, exchanging objects within one Git repository. In other words, {IPFS} provides a high throughput content-addressed block storage model, with content-addressed hyper links. This forms a generalized Merkle {DAG}, a data structure upon which one can build versioned file systems, blockchains, and even a Permanent Web. {IPFS} combines a distributed hashtable, an incentivized block exchange, and a self-certifying namespace. {IPFS} has no single point of failure, and nodes do not need to trust each other.},
	journaltitle = {{arXiv}:1407.3561 [cs]},
	author = {Benet, Juan},
	urldate = {2017-04-26},
	date = {2014-07-14},
	eprinttype = {arxiv},
	eprint = {1407.3561},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
}

@article{ondov_mash:_2016,
	title = {Mash: fast genome and metagenome distance estimation using {MinHash}},
	volume = {17},
	issn = {1474-760X},
	url = {http://dx.doi.org/10.1186/s13059-016-0997-x},
	doi = {10.1186/s13059-016-0997-x},
	shorttitle = {Mash},
	abstract = {Mash extends the {MinHash} dimensionality-reduction technique to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 {NCBI} {RefSeq} genomes in 33 {CPU} h; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a {BSD} license (                  https://github.com/marbl/mash                                  ).},
	pages = {132},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Ondov, Brian D. and Treangen, Todd J. and Melsted, Páll and Mallonee, Adam B. and Bergman, Nicholas H. and Koren, Sergey and Phillippy, Adam M.},
	urldate = {2017-04-26},
	date = {2016},
	keywords = {metagenomics, Comparative genomics, Genomic distance, Alignment, Sequencing, Nanopore},
}

@article{solomon_fast_2016,
	title = {Fast search of thousands of short-read sequencing experiments},
	volume = {34},
	rights = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1087-0156},
	url = {https://www.nature.com/nbt/journal/v34/n3/full/nbt.3442.html},
	doi = {10.1038/nbt.3442},
	abstract = {The amount of sequence information in public repositories is growing at a rapid rate. Although these data are likely to contain clinically important information that has not yet been uncovered, our ability to effectively mine these repositories is limited. Here we introduce Sequence Bloom Trees ({SBTs}), a method for querying thousands of short-read sequencing experiments by sequence, 162 times faster than existing approaches. The approach searches large data archives for all experiments that involve a given sequence. We use {SBTs} to search 2,652 human blood, breast and brain {RNA}-seq experiments for all 214,293 known transcripts in under 4 days using less than 239 {MB} of {RAM} and a single {CPU}. Searching sequence archives at this scale and in this time frame is currently not possible using existing tools.},
	pages = {300--302},
	number = {3},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotech},
	author = {Solomon, Brad and Kingsford, Carl},
	urldate = {2017-04-26},
	date = {2016-03},
	langid = {english},
	keywords = {data mining, Genome informatics, Transcriptomics},
}

@article{driscoll_making_1989,
	title = {Making data structures persistent},
	volume = {38},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/0022000089900342},
	doi = {10.1016/0022-0000(89)90034-2},
	abstract = {This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O(1) space bounds for insertion and deletion.},
	pages = {86--124},
	number = {1},
	journaltitle = {Journal of Computer and System Sciences},
	shortjournal = {Journal of Computer and System Sciences},
	author = {Driscoll, James R. and Sarnak, Neil and Sleator, Daniel D. and Tarjan, Robert E.},
	urldate = {2017-04-26},
	date = {1989-02-01},
}

@article{gb_editorial_team_closure_2011,
	title = {Closure of the {NCBI} {SRA} and implications for the long-term future of genomics data storage},
	volume = {12},
	issn = {1474-760X},
	url = {http://dx.doi.org/10.1186/gb-2011-12-3-402},
	doi = {10.1186/gb-2011-12-3-402},
	pages = {402},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {{GB Editorial Team}},
	urldate = {2017-04-26},
	date = {2011},
}

@article{kyrpides_microbiome_2016,
	title = {Microbiome Data Science: Understanding Our Microbial Planet},
	volume = {24},
	issn = {0966-842X, 1878-4380},
	url = {http://www.cell.com/trends/microbiology/abstract/S0966-842X(16)00048-2},
	doi = {10.1016/j.tim.2016.02.011},
	shorttitle = {Microbiome Data Science},
	pages = {425--427},
	number = {6},
	journaltitle = {Trends in Microbiology},
	shortjournal = {Trends in Microbiology},
	author = {Kyrpides, Nikos C. and Eloe-Fadrosh, Emiley A. and Ivanova, Natalia N.},
	urldate = {2017-04-26},
	date = {2016-06-01},
	pmid = {27197692},
	keywords = {metagenomics, Microbiome, data science, data integration, data standards, data interoperability},
}

@article{popic_gattaca:_2017,
	title = {{GATTACA}: Lightweight Metagenomic Binning With Compact Indexing Of Kmer Counts And {MinHash}-based Panel Selection},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/content/early/2017/04/26/130997},
	doi = {10.1101/130997},
	shorttitle = {{GATTACA}},
	abstract = {We introduce {GATTACA}, a framework for rapid and accurate binning of metagenomic contigs from a single or multiple metagenomic samples into clusters associated with individual species. The clusters are computed using co-abundance profiles within a set of reference metagnomes; unlike previous methods, {GATTACA} estimates these profiles from k-mer counts stored in a highly compact index. On multiple synthetic and real benchmark datasets, {GATTACA} produces clusters that correspond to distinct bacterial species with an accuracy that matches earlier methods, while being up to 20x faster when the reference panel index can be computed offline and 6x faster for online co-abundance estimation. Leveraging the {MinHash} technique to quickly compare metagenomic samples, {GATTACA} also provides an efficient way to identify publicly-available metagenomic data that can be incorporated into the set of reference metagenomes to further improve binning accuracy. Thus, enabling easy indexing and reuse of publicly-available metagenomic datasets, {GATTACA} makes accurate metagenomic analyses accessible to a much wider range of researchers.},
	pages = {130997},
	journaltitle = {{bioRxiv}},
	author = {Popic, Victoria and Kuleshov, Volodymyr and Snyder, Michael and Batzoglou, Serafim},
	urldate = {2017-04-26},
	date = {2017-04-26},
	langid = {english},
}

@inproceedings{broder_resemblance_1997,
	title = {On the resemblance and containment of documents},
	url = {http://ieeexplore.ieee.org/abstract/document/666900/},
	pages = {21--29},
	booktitle = {Compression and Complexity of Sequences 1997. Proceedings},
	publisher = {{IEEE}},
	author = {Broder, Andrei Z.},
	urldate = {2017-04-26},
	date = {1997},
}

@article{christiani_set_2016,
	title = {Set Similarity Search Beyond {MinHash}},
	url = {http://arxiv.org/abs/1612.07710},
	abstract = {We consider the problem of approximate set similarity search under Braun-Blanquet similarity \$B({\textbackslash}mathbf\{x\}, {\textbackslash}mathbf\{y\}) = {\textbar}{\textbackslash}mathbf\{x\} {\textbackslash}cap {\textbackslash}mathbf\{y\}{\textbar} / {\textbackslash}max({\textbar}{\textbackslash}mathbf\{x\}{\textbar}, {\textbar}{\textbackslash}mathbf\{y\}{\textbar})\$. The \$(b\_2, b\_2)\$-approximate Braun-Blanquet similarity search problem is to preprocess a collection of sets \$P\$ such that, given a query set \${\textbackslash}mathbf\{q\}\$, if there exists \${\textbackslash}mathbf\{x\} {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}) {\textbackslash}geq b\_1\$, then we can efficiently return \${\textbackslash}mathbf\{x\}' {\textbackslash}in P\$ with \$B({\textbackslash}mathbf\{q\}, {\textbackslash}mathbf\{x\}') {\textgreater} b\_2\$. We present a simple data structure that solves this problem with space usage \$O(n{\textasciicircum}\{1+{\textbackslash}rho\}{\textbackslash}log n + {\textbackslash}sum\_\{{\textbackslash}mathbf\{x\} {\textbackslash}in P\}{\textbar}{\textbackslash}mathbf\{x\}{\textbar})\$ and query time \$O({\textbar}{\textbackslash}mathbf\{q\}{\textbar}n{\textasciicircum}\{{\textbackslash}rho\} {\textbackslash}log n)\$ where \$n = {\textbar}P{\textbar}\$ and \${\textbackslash}rho = {\textbackslash}log(1/b\_1)/{\textbackslash}log(1/b\_2)\$. Making use of existing lower bounds for locality-sensitive hashing by O'Donnell et al. ({TOCT} 2014) we show that this value of \${\textbackslash}rho\$ is tight across the parameter space, i.e., for every choice of constants \$0 {\textless} b\_2 {\textless} b\_1 {\textless} 1\$. In the case where all sets have the same size our solution strictly improves upon the value of \${\textbackslash}rho\$ that can be obtained through the use of state-of-the-art data-independent techniques in the Indyk-Motwani locality-sensitive hashing framework ({STOC} 1998) such as Broder's {MinHash} ({CCS} 1997) for Jaccard similarity and Andoni et al.'s cross-polytope {LSH} ({NIPS} 2015) for cosine similarity. Surprisingly, even though our solution is data-independent, for a large part of the parameter space we outperform the currently best data-dependent method by Andoni and Razenshteyn ({STOC} 2015).},
	journaltitle = {{arXiv}:1612.07710 [cs]},
	author = {Christiani, Tobias and Pagh, Rasmus},
	urldate = {2017-04-26},
	date = {2016-12-22},
	eprinttype = {arxiv},
	eprint = {1612.07710},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{brown_sourmash:_2016,
	title = {sourmash: a library for {MinHash} sketching of {DNA}},
	volume = {1},
	url = {http://joss.theoj.org/papers/10.21105/joss.00027},
	doi = {10.21105/joss.00027},
	shorttitle = {sourmash},
	number = {5},
	journaltitle = {The Journal of Open Source Software},
	author = {Brown, C. Titus and Irber, Luiz},
	urldate = {2017-04-26},
	date = {2016-09-14}
}

@article{yu_hyperminhash:_2017,
	title = {{HyperMinHash}: Jaccard index sketching in {LogLog} space},
	url = {http://arxiv.org/abs/1710.08436},
	shorttitle = {{HyperMinHash}},
	abstract = {In this extended abstract, we describe and analyse a streaming probabilistic sketch, {HYPERMINHASH}, to estimate the Jaccard index (or Jaccard similarity coefficient) over two sets \$A\$ and \$B\$. {HyperMinHash} can be thought of as a compression of standard {MinHash} by building off of a {HyperLogLog} count-distinct sketch. Given Jaccard index \${\textbackslash}delta\$, using \$k\$ buckets of size \$O({\textbackslash}log(l) + {\textbackslash}log{\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ (in practice, typically 2 bytes) per set, {HyperMinHash} streams over \$A\$ and \$B\$ and generates an estimate of the Jaccard index \${\textbackslash}delta\$ with error \$O(1/l + {\textbackslash}sqrt\{k/{\textbackslash}delta\})\$. This improves on the best previously known sketch, {MinHash}, which requires the same number of storage units (buckets), but using \$O({\textbackslash}log({\textbar}A {\textbackslash}cup B{\textbar}))\$ bit per bucket. For instance, our new algorithm allows estimating Jaccard indices of 0.01 for set cardinalities on the order of \$10{\textasciicircum}\{19\}\$ with relative error of around 5{\textbackslash}\% using 64KiB of memory; the previous state-of-the-art {MinHash} can only estimate Jaccard indices for cardinalities of \$10{\textasciicircum}\{10\}\$ with the same memory consumption. Alternately, one can think of {HyperMinHash} as an augmentation of b-bit {MinHash} that enables streaming updates, unions, and cardinality estimation (and thus intersection cardinality by way of Jaccard), while using \${\textbackslash}log{\textbackslash}log\$ extra bits.},
	journaltitle = {{arXiv}:1710.08436 [cs]},
	author = {Yu, Y. William and Weber, Griffin},
	urldate = {2017-10-25},
	date = {2017-10-23},
	eprinttype = {arxiv},
	eprint = {1710.08436},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
}

@article{tsitsiklis_power_2011,
	title = {On the power of (even a little) centralization in distributed processing},
	volume = {39},
	pages = {121--132},
	number = {1},
	journaltitle = {{ACM} {SIGMETRICS} Performance Evaluation Review},
	author = {Tsitsiklis, John N. and Xu, Kuang},
	date = {2011},
}

@inproceedings{sun_allsome_2017,
	title = {{AllSome} Sequence Bloom Trees},
	isbn = {978-3-319-56969-7 978-3-319-56970-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-56970-3_17},
	doi = {10.1007/978-3-319-56970-3_17},
	series = {Lecture Notes in Computer Science},
	abstract = {The ubiquity of next generation sequencing has transformed the size and nature of many databases, pushing the boundaries of current indexing and searching methods. One particular example is a database of 2,652 human {RNA}-seq experiments uploaded to the Sequence Read Archive. Recently, Solomon and Kingsford proposed the Sequence Bloom Tree data structure and demonstrated how it can be used to accurately identify {SRA} samples that have a transcript of interest potentially expressed. In this paper, we propose an improvement called the {AllSome} Sequence Bloom Tree. Results show that our new data structure significantly improves performance, reducing the tree construction time by 52.7\% and query time by 39–85\%, with a price of up to 3x memory consumption during queries. Notably, it can query a batch of 198,074 queries in under 8 h (compared to around two days previously) and a whole set of {\textbackslash}(k{\textbackslash})-mers from a sequencing experiment (about 27 mil {\textbackslash}(k{\textbackslash})-mers) in under 11 min.},
	eventtitle = {International Conference on Research in Computational Molecular Biology},
	pages = {272--286},
	booktitle = {Research in Computational Molecular Biology},
	publisher = {Springer, Cham},
	author = {Sun, Chen and Harris, Robert S. and Chikhi, Rayan and Medvedev, Paul},
	urldate = {2017-10-27},
	date = {2017-05-03},
	langid = {english},
}

@article{tennessen_prodege:_2016,
	title = {{ProDeGe}: a computational protocol for fully automated decontamination of genomes},
	volume = {10},
	rights = {© 2015 Nature Publishing Group},
	issn = {1751-7362},
	url = {https://www.nature.com/ismej/journal/v10/n1/abs/ismej2015100a.html},
	doi = {10.1038/ismej.2015.100},
	shorttitle = {{ProDeGe}},
	abstract = {Single amplified genomes and genomes assembled from metagenomes have enabled the exploration of uncultured microorganisms at an unprecedented scale. However, both these types of products are plagued by contamination. Since these genomes are now being generated in a high-throughput manner and sequences from them are propagating into public databases to drive novel scientific discoveries, rigorous quality controls and decontamination protocols are urgently needed. Here, we present {ProDeGe} (Protocol for fully automated Decontamination of Genomes), the first computational protocol for fully automated decontamination of draft genomes. {ProDeGe} classifies sequences into two classes—clean and contaminant—using a combination of homology and feature-based methodologies. On average, 84\% of sequence from the non-target organism is removed from the data set (specificity) and 84\% of the sequence from the target organism is retained (sensitivity). The procedure operates successfully at a rate of {\textasciitilde}0.30 {CPU} core hours per megabase of sequence and can be applied to any type of genome sequence.},
	pages = {269--272},
	number = {1},
	journaltitle = {The {ISME} Journal},
	shortjournal = {{ISME} J},
	author = {Tennessen, Kristin and Andersen, Evan and Clingenpeel, Scott and Rinke, Christian and Lundberg, Derek S. and Han, James and Dangl, Jeff L. and Ivanova, Natalia and Woyke, Tanja and Kyrpides, Nikos and Pati, Amrita},
	urldate = {2017-10-27},
	date = {2016-01},
	langid = {english},
}

@article{lux_acdc_2016,
	title = {acdc – Automated Contamination Detection and Confidence estimation for single-cell genome data},
	volume = {17},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-016-1397-7},
	doi = {10.1186/s12859-016-1397-7},
	abstract = {A major obstacle in single-cell sequencing is sample contamination with foreign {DNA}. To guarantee clean genome assemblies and to prevent the introduction of contamination into public databases, considerable quality control efforts are put into post-sequencing analysis. Contamination screening generally relies on reference-based methods such as database alignment or marker gene search, which limits the set of detectable contaminants to organisms with closely related reference species. As genomic coverage in the tree of life is highly fragmented, there is an urgent need for a reference-free methodology for contaminant identification in sequence data.},
	pages = {543},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Lux, Markus and Krüger, Jan and Rinke, Christian and Maus, Irena and Schlüter, Andreas and Woyke, Tanja and Sczyrba, Alexander and Hammer, Barbara},
	urldate = {2017-10-27},
	date = {2016-12-20},
	keywords = {Binning, Clustering, Contamination detection, Machine learning, Quality control, Single-cell sequencing},
}

@article{baquero_pure_2017,
	title = {Pure Operation-Based Replicated Data Types},
	url = {http://arxiv.org/abs/1710.04469},
	abstract = {Distributed systems designed to serve clients across the world often make use of geo-replication to attain low latency and high availability. Conflict-free Replicated Data Types ({CRDTs}) allow the design of predictable multi-master replication and support eventual consistency of replicas that are allowed to transiently diverge. {CRDTs} come in two flavors: state-based, where a state is changed locally and shipped and merged into other replicas; operation-based, where operations are issued locally and reliably causal broadcast to all other replicas. However, the standard definition of op-based {CRDTs} is very encompassing, allowing even sending the full-state, and thus imposing storage and dissemination overheads as well as blurring the distinction from state-based {CRDTs}. We introduce pure op-based {CRDTs}, that can only send operations to other replicas, drawing a clear distinction from state-based ones. Data types with commutative operations can be trivially implemented as pure op-based {CRDTs} using standard reliable causal delivery; whereas data types having non-commutative operations are implemented using a {PO}-Log, a partially ordered log of operations, and making use of an extended {API}, i.e., a Tagged Causal Stable Broadcast ({TCSB}), that provides extra causality information upon delivery and later informs when delivered messages become causally stable, allowing further {PO}-Log compaction. The framework is illustrated by a catalog of pure op-based specifications for classic {CRDTs}, including counters, multi-value registers, add-wins and remove-wins sets.},
	journaltitle = {{arXiv}:1710.04469 [cs]},
	author = {Baquero, Carlos and Almeida, Paulo Sergio and Shoker, Ali},
	urldate = {2017-11-01},
	date = {2017-10-12},
	eprinttype = {arxiv},
	eprint = {1710.04469},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
}

@inproceedings{solomon_improved_2017,
	title = {Improved Search of Large Transcriptomic Sequencing Databases Using Split Sequence Bloom Trees},
	pages = {257--271},
	booktitle = {International Conference on Research in Computational Molecular Biology},
	publisher = {Springer},
	author = {Solomon, Brad and Kingsford, Carl},
	date = {2017},
}

@article{crainiceanu_bloofi:_2015,
	title = {Bloofi: Multidimensional Bloom Filters},
	volume = {54},
	issn = {03064379},
	url = {http://arxiv.org/abs/1501.01941},
	doi = {10.1016/j.is.2015.01.002},
	shorttitle = {Bloofi},
	abstract = {Bloom filters are probabilistic data structures commonly used for approximate membership problems in many areas of Computer Science (networking, distributed systems, databases, etc.). With the increase in data size and distribution of data, problems arise where a large number of Bloom filters are available, and all them need to be searched for potential matches. As an example, in a federated cloud environment, each cloud provider could encode the information using Bloom filters and share the Bloom filters with a central coordinator. The problem of interest is not only whether a given element is in any of the sets represented by the Bloom filters, but which of the existing sets contain the given element. This problem cannot be solved by just constructing a Bloom filter on the union of all the sets. Instead, we effectively have a multidimensional Bloom filter problem: given an element, we wish to receive a list of candidate sets where the element might be. To solve this problem, we consider 3 alternatives. Firstly, we can naively check many Bloom filters. Secondly, we propose to organize the Bloom filters in a hierarchical index structure akin to a B+ tree, that we call Bloofi. Finally, we propose another data structure that packs the Bloom filters in such a way as to exploit bit-level parallelism, which we call Flat-Bloofi. Our theoretical and experimental results show that Bloofi and Flat-Bloofi provide scalable and efficient solutions alternatives to search through a large number of Bloom filters.},
	pages = {311--324},
	journaltitle = {Information Systems},
	author = {Crainiceanu, Adina and Lemire, Daniel},
	urldate = {2017-12-27},
	date = {2015-12},
	eprinttype = {arxiv},
	eprint = {1501.01941},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases},
}

@software{noauthor_bitfunnel:_2017,
	title = {{BitFunnel}: A signature-based search engine},
	rights = {{MIT}},
	url = {https://github.com/BitFunnel/BitFunnel},
	shorttitle = {{BitFunnel}},
	publisher = {{BitFunnel}},
	urldate = {2017-12-27},
	date = {2017-12-27},
	note = {original-date: 2016-04-01T22:44:40Z},
	keywords = {search, search-engine, search-in-text},
}

@article{bradley_real-time_2017,
	title = {Real-time search of all bacterial and viral genomic data},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/15/234955},
	doi = {10.1101/234955},
	abstract = {Genome sequencing of pathogens is now ubiquitous in microbiology, and the sequence archives are effectively no longer searchable for arbitrary sequences. Furthermore, the exponential increase of these archives is likely to be further spurred by automated diagnostics. To unlock their use for scientific research and real-time surveillance we have combined knowledge about bacterial genetic variation with ideas used in web-search, to build a {DNA} search engine for microbial data that can grow incrementally. We indexed the complete global corpus of bacterial and viral whole genome sequence data (447,833 genomes), using four orders of magnitude less storage than previous methods. The method allows future scaling to millions of genomes. This renders the global archive accessible to sequence search, which we demonstrate with three applications: ultra-fast search for resistance genes {MCR}1-3, analysis of host-range for 2827 plasmids, and quantification of the rise of antibiotic resistance prevalence in the sequence archives.},
	pages = {234955},
	journaltitle = {{bioRxiv}},
	author = {Bradley, Phelim and Bakker, Henk den and Rocha, Eduardo and {McVean}, Gil and Iqbal, Zamin},
	urldate = {2018-01-08},
	date = {2017-12-15},
	langid = {english},
}

@article{muggli_succinct_2017,
	title = {Succinct De Bruijn Graph Construction for Massive Populations Through Space-Efficient Merging},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/12/06/229641},
	doi = {10.1101/229641},
	abstract = {Recently, there has been significant amount of effort in developing space-efficient and succinct data structures for storing and building the traditional de Bruijn graph and its variants, including the colored de Bruijn graph. However, a problem not yet considered is developing a means to merge succinct representations of the de Bruijn graph---a challenge is necessary for constructing the de Bruijn graph on very-large datasets. We create {VARIMERGE}, for building the colored de Bruijn graph on a very-large dataset through partitioning the data into smaller subsets, building the colored de Bruijn graph using a {FM}-index based representation, and merging these representations in an iterative format. This last step is an algorithmic challenge for which we present an algorithm in this paper. Lastly, we demonstrate the utility of {VARIMERGE} by demonstrating: a four-fold reduction in working space when constructing an 8,000 color dataset, and the construction of population graph two orders of magnitude larger than previous reported methods.},
	pages = {229641},
	journaltitle = {{bioRxiv}},
	author = {Muggli, Martin D. and Boucher, Christina},
	urldate = {2018-01-08},
	date = {2017-12-06},
	langid = {english},
}

@article{langmead_cloud_2018,
	title = {Cloud computing for genomic data analysis and collaboration},
	rights = {2018 Nature Publishing Group},
	issn = {1471-0064},
	url = {https://www.nature.com/articles/nrg.2017.113},
	doi = {10.1038/nrg.2017.113},
	abstract = {Next-generation sequencing has made major strides in the past decade. Studies based on large sequencing data sets are growing in number, and public archives for raw sequencing data have been doubling in size every 18 months. Leveraging these data requires researchers to use large-scale computational resources. Cloud computing, a model whereby users rent computers and storage from large data centres, is a solution that is gaining traction in genomics research. Here, we describe how cloud computing is used in genomics for research and large-scale collaborations, and argue that its elasticity, reproducibility and privacy features make it ideally suited for the large-scale reanalysis of publicly available archived data, including privacy-protected data.},
	journaltitle = {Nature Reviews Genetics},
	author = {Langmead, Ben and Nellore, Abhinav},
	urldate = {2018-02-09},
	date = {2018-01-30},
	langid = {english},
}

@article{sczyrba_critical_2017,
	title = {Critical Assessment of Metagenome Interpretation—a benchmark of metagenomics software},
	volume = {14},
	rights = {2017 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4458},
	doi = {10.1038/nmeth.4458},
	abstract = {Methods for assembly, taxonomic profiling and binning are key to interpreting metagenome data, but a lack of consensus about benchmarking complicates performance assessment. The Critical Assessment of Metagenome Interpretation ({CAMI}) challenge has engaged the global developer community to benchmark their programs on highly complex and realistic data sets, generated from ∼700 newly sequenced microorganisms and ∼600 novel viruses and plasmids and representing common experimental setups. Assembly and genome binning programs performed well for species represented by individual genomes but were substantially affected by the presence of related strains. Taxonomic profiling and binning programs were proficient at high taxonomic ranks, with a notable performance decrease below family level. Parameter settings markedly affected performance, underscoring their importance for program reproducibility. The {CAMI} results highlight current challenges but also provide a roadmap for software selection to answer specific research questions.},
	pages = {1063--1071},
	number = {11},
	journaltitle = {Nature Methods},
	author = {Sczyrba, Alexander and Hofmann, Peter and Belmann, Peter and Koslicki, David and Janssen, Stefan and Dröge, Johannes and Gregor, Ivan and Majda, Stephan and Fiedler, Jessika and Dahms, Eik and Bremges, Andreas and Fritz, Adrian and Garrido-Oter, Ruben and Jørgensen, Tue Sparholt and Shapiro, Nicole and Blood, Philip D. and Gurevich, Alexey and Bai, Yang and Turaev, Dmitrij and {DeMaere}, Matthew Z. and Chikhi, Rayan and Nagarajan, Niranjan and Quince, Christopher and Meyer, Fernando and Balvočiūtė, Monika and Hansen, Lars Hestbjerg and Sørensen, Søren J. and Chia, Burton K. H. and Denis, Bertrand and Froula, Jeff L. and Wang, Zhong and Egan, Robert and Don Kang, Dongwan and Cook, Jeffrey J. and Deltel, Charles and Beckstette, Michael and Lemaitre, Claire and Peterlongo, Pierre and Rizk, Guillaume and Lavenier, Dominique and Wu, Yu-Wei and Singer, Steven W. and Jain, Chirag and Strous, Marc and Klingenberg, Heiner and Meinicke, Peter and Barton, Michael D. and Lingner, Thomas and Lin, Hsin-Hung and Liao, Yu-Chieh and Silva, Genivaldo Gueiros Z. and Cuevas, Daniel A. and Edwards, Robert A. and Saha, Surya and Piro, Vitor C. and Renard, Bernhard Y. and Pop, Mihai and Klenk, Hans-Peter and Göker, Markus and Kyrpides, Nikos C. and Woyke, Tanja and Vorholt, Julia A. and Schulze-Lefert, Paul and Rubin, Edward M. and Darling, Aaron E. and Rattei, Thomas and {McHardy}, Alice C.},
	urldate = {2018-02-16},
	date = {2017-11},
	langid = {english},
}

@article{mcintyre_comprehensive_2017,
	title = {Comprehensive benchmarking and ensemble approaches for metagenomic classifiers},
	volume = {18},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-017-1299-7},
	doi = {10.1186/s13059-017-1299-7},
	abstract = {One of the main challenges in metagenomics is the identification of microorganisms in clinical and environmental samples. While an extensive and heterogeneous set of computational tools is available to classify microorganisms using whole-genome shotgun sequencing data, comprehensive comparisons of these methods are limited.},
	pages = {182},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {{McIntyre}, Alexa B. R. and Ounit, Rachid and Afshinnekoo, Ebrahim and Prill, Robert J. and Hénaff, Elizabeth and Alexander, Noah and Minot, Samuel S. and Danko, David and Foox, Jonathan and Ahsanuddin, Sofia and Tighe, Scott and Hasan, Nur A. and Subramanian, Poorani and Moffat, Kelly and Levy, Shawn and Lonardi, Stefano and Greenfield, Nick and Colwell, Rita R. and Rosen, Gail L. and Mason, Christopher E.},
	urldate = {2018-02-16},
	date = {2017-09-21},
	keywords = {Metagenomics, Classification, Comparison, Ensemble methods, Meta-classification, Pathogen detection, Shotgun sequencing, Taxonomy},
}

@inproceedings{indyk_approximate_1998,
	location = {New York, {NY}, {USA}},
	title = {Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality},
	isbn = {978-0-89791-962-3},
	url = {http://doi.acm.org/10.1145/276698.276876},
	doi = {10.1145/276698.276876},
	series = {{STOC} '98},
	shorttitle = {Approximate Nearest Neighbors},
	pages = {604--613},
	booktitle = {Proceedings of the Thirtieth Annual {ACM} Symposium on Theory of Computing},
	publisher = {{ACM}},
	author = {Indyk, Piotr and Motwani, Rajeev},
	urldate = {2018-03-06},
	date = {1998},
}

@article{flajolet_birthday_1992,
	title = {Birthday Paradox, Coupon Collectors, Caching Algorithms and Self-Organizing Search},
	volume = {39},
	doi = {10.1016/0166-218X(92)90177-C},
	abstract = {This paper introduces a unified framework for the analysis of a class of random allocation processes thdt include: (i) the birthday paradox; (ii) the coupon collector problem* (iii) least-tecently-used ({LRU}) caching in memory management systems under the independent reference model; (iv) the move-to-front heuristic of self-organizing search. All analyses are relative to general nonuniform f robability distributions. Our approach to these problems comprises two stages. First, the probabilistic phenozna ct’ interest are described by means of regular languages extended by addition of the shu!‘Re product. Next, systematic translation mechanisms are used to derive integral representations for expectations and probability distributions.},
	pages = {207--229},
	journaltitle = {Discrete Applied Mathematics},
	author = {Flajolet, Philippe and Gardy, Danièle and Thimonier, Loÿs},
	date = {1992},
}

@article{downey_computational_1999,
	title = {Computational tractability: The view from mars},
	volume = {69},
	shorttitle = {Computational tractability},
	pages = {73--97},
	journaltitle = {Bulletin of the {EATCS}},
	author = {Downey, Rodney G. and Fellows, Michael R. and Stege, Ulrike},
	date = {1999},
}

@report{sparka_p2kmv:_2018,
	title = {P2KMV: A Privacy-preserving Counting Sketch for Efficient and Accurate Set Intersection Cardinality Estimations},
	url = {http://eprint.iacr.org/2018/234},
	shorttitle = {P2KMV},
	abstract = {In this paper, we propose P2KMV, a novel privacy-preserving counting sketch, based on the k minimum values algorithm. With P2KMV, we offer a versatile privacy-enhanced technology for obtaining statistics, following the principle of data minimization, and aiming for the sweet spot between privacy, accuracy, and computational efficiency. As our main contribution, we develop methods to perform set operations, which facilitate cardinality estimates under strong privacy requirements. Most notably, we propose an efficient, privacy-preserving algorithm to estimate the set intersection cardinality. P2KMV provides plausible deniability for all data items contained in the sketch. We discuss the algorithm's privacy guarantees as well as the accuracy of the obtained estimates. An experimental evaluation confirms our analytical expectations and provides insights regarding parameter choices.},
	number = {234},
	author = {Sparka, Hagen and Tschorsch, Florian and Scheuermann, Björn},
	urldate = {2018-04-02},
	date = {2018},
	keywords = {applications, privacy},
}

@article{ju_tahcoroll:_2017,
	title = {{TahcoRoll}: An Efficient Approach for Signature Profiling in Genomic Data through Variable-Length k-mers},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/06/229708},
	doi = {10.1101/229708},
	shorttitle = {{TahcoRoll}},
	abstract = {K-mer profiling has been one of the trending approaches to analyze read data generated by high-throughput sequencing technologies. The tasks of k-mer profiling include, but are not limited to, counting the frequencies and determining the occurrences of short sequences in a dataset. The notion of k-mer has been extensively used to build de Bruijn graphs in genome or transcriptome assembly, which requires examining all possible k-mers presented in the dataset. Recently, an alternative way of profiling has been proposed, which constructs a set of representative k-mers as genomic markers and profiles their occurrences in the sequencing data. This technique has been applied in both transcript quantification through {RNA}-Seq and taxonomic classification of metagenomic reads. Most of these applications use a set of fixed-size k-mers since the majority of existing k-mer counters are inadequate to process genomic sequences with variable-length k-mers. However, choosing the appropriate k is challenging, as it varies for different applications. As a pioneer work to profile a set of variable-length k-mers, we propose {TahcoRoll} in order to enhance the Aho-Corasick algorithm. More specifically, we use one bit to represent each nucleotide, and integrate the rolling hash technique to construct an efficient in-memory data structure for this task. Using both synthetic and real datasets, results show that {TahcoRoll} outperforms existing approaches in either or both time and memory efficiency without using any disk space. In addition, compared to the most efficient state-of-the-art k-mer counters, such as {KMC} and {MSBWT}, {TahcoRoll} is the only approach that can process long read data from both {PacBio} and Oxford Nanopore on a commodity desktop computer. The source code of {TahcoRoll} is implemented in C++14, and available at https://github.com/chelseaju/{TahcoRoll}.git.},
	pages = {229708},
	journaltitle = {{bioRxiv}},
	author = {Ju, Chelsea Jui-Ting and Jiang, Jyun-Yu and Li, Ruirui and Li, Zeyu and Wang, Wei},
	urldate = {2018-04-02},
	date = {2017-12-06},
	langid = {english},
}

@article{ertl_new_2017,
	title = {New cardinality estimation algorithms for {HyperLogLog} sketches},
	url = {http://arxiv.org/abs/1702.01284},
	abstract = {This paper presents new methods to estimate the cardinalities of data sets recorded by {HyperLogLog} sketches. A theoretically motivated extension to the original estimator is presented that eliminates the bias for small and large cardinalities. Based on the maximum likelihood principle a second unbiased method is derived together with a robust and efficient numerical algorithm to calculate the estimate. The maximum likelihood approach can also be applied to more than a single {HyperLogLog} sketch. In particular, it is shown that it gives more precise cardinality estimates for union, intersection, or relative complements of two sets that are both represented by {HyperLogLog} sketches compared to the conventional technique using the inclusion-exclusion principle. All the new methods are demonstrated and verified by extensive simulations.},
	journaltitle = {{arXiv}:1702.01284 [cs]},
	author = {Ertl, Otmar},
	urldate = {2018-04-02},
	date = {2017-02-04},
	eprinttype = {arxiv},
	eprint = {1702.01284},
	keywords = {Computer Science - Data Structures and Algorithms, 68W15, 68W25, 62-07, E.1, H.2.8, I.1.2},
}

@article{deorowicz_kmer-db:_2018,
	title = {Kmer-db: instant evolutionary distance estimation},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2018/02/12/263590},
	doi = {10.1101/263590},
	shorttitle = {Kmer-db},
	abstract = {Summary: Kmer-db is a new tool for estimating evolutionary relationship on the basis of k-mers extracted from genomes or sequencing reads. Thanks to an efficient data structure and parallel implementation, our software estimates distances between 40,715 pathogens in less than 4 minutes (on a modern workstation), 44 times faster than Mash, its main competitor. Availability and Implementation: https://github.com/refresh-bio/kmer-db},
	pages = {263590},
	journaltitle = {{bioRxiv}},
	author = {Deorowicz, Sebastian and Gudys, Adam and Dlugosz, Maciej and Kokot, Marek and Danek, Agnieszka},
	urldate = {2018-04-02},
	date = {2018-02-12},
	langid = {english},
}

@article{zhu_lsh_2016,
	title = {{LSH} ensemble: internet-scale domain search},
	volume = {9},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=2994509.2994534},
	doi = {10.14778/2994509.2994534},
	shorttitle = {{LSH} ensemble},
	abstract = {We study the problem of domain search where a domain is a set of distinct values from an unspeciﬁed universe. We use Jaccard set containment score, deﬁned as {\textbar}Q ∩ X{\textbar}/{\textbar}Q{\textbar}, as the measure of relevance of a domain X to a query domain Q. Our choice of Jaccard set containment over Jaccard similarity as a measure of relevance makes our work particularly suitable for searching Open Data and data on the web, as Jaccard similarity is known to have poor performance over sets with large diﬀerences in their domain sizes. We demonstrate that the domains found in several real-life Open Data and web data repositories show a power-law distribution over their domain sizes.},
	pages = {1185--1196},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	author = {Zhu, Erkang and Nargesian, Fatemeh and Pu, Ken Q. and Miller, Renée J.},
	urldate = {2018-04-02},
	date = {2016-08-01},
	langid = {english},
}

@article{wang_hashing_2014,
	title = {Hashing for Similarity Search: A Survey},
	url = {http://arxiv.org/abs/1408.2927},
	shorttitle = {Hashing for Similarity Search},
	abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
	journaltitle = {{arXiv}:1408.2927 [cs]},
	author = {Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
	urldate = {2018-04-02},
	date = {2014-08-13},
	eprinttype = {arxiv},
	eprint = {1408.2927},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Computer Vision and Pattern Recognition},
}

@article{shrivastava_optimal_2017,
	title = {Optimal Densification for Fast and Accurate Minwise Hashing},
	url = {http://arxiv.org/abs/1703.04664},
	abstract = {Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification{\textasciitilde}{\textbackslash}cite\{Proc:{OneHashLSH}\_ICML14,Proc:Shrivastava\_UAI14\} have shown that it is possible to compute \$k\$ minwise hashes, of a vector with \$d\$ nonzeros, in mere \$(d + k)\$ computations, a significant improvement over the classical \$O(dk)\$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.},
	journaltitle = {{arXiv}:1703.04664 [cs]},
	author = {Shrivastava, Anshumali},
	urldate = {2018-04-02},
	date = {2017-03-14},
	eprinttype = {arxiv},
	eprint = {1703.04664},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning},
}

@article{jiang_comparison_2012,
	title = {Comparison of metagenomic samples using sequence signatures},
	volume = {13},
	issn = {1471-2164},
	url = {https://doi.org/10.1186/1471-2164-13-730},
	doi = {10.1186/1471-2164-13-730},
	abstract = {Sequence signatures, as defined by the frequencies of k-tuples (or k-mers, k-grams), have been used extensively to compare genomic sequences of individual organisms, to identify cis-regulatory modules, and to study the evolution of regulatory sequences. Recently many next-generation sequencing ({NGS}) read data sets of metagenomic samples from a variety of different environments have been generated. The assembly of these reads can be difficult and analysis methods based on mapping reads to genes or pathways are also restricted by the availability and completeness of existing databases. Sequence-signature-based methods, however, do not need the complete genomes or existing databases and thus, can potentially be very useful for the comparison of metagenomic samples using {NGS} read data. Still, the applications of sequence signature methods for the comparison of metagenomic samples have not been well studied.},
	pages = {730},
	journaltitle = {{BMC} Genomics},
	shortjournal = {{BMC} Genomics},
	author = {Jiang, Bai and Song, Kai and Ren, Jie and Deng, Minghua and Sun, Fengzhu and Zhang, Xuegong},
	urldate = {2018-04-02},
	date = {2012-12-27},
	keywords = {Dissimilarity Measure, Galapagos Island, Metagenomic Dataset, Metagenomic Sample, Sequencing Depth},
}

@article{aflitos_cnidaria:_2015,
	title = {Cnidaria: fast, reference-free clustering of raw and assembled genome and transcriptome {NGS} data},
	volume = {16},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4630969/},
	doi = {10.1186/s12859-015-0806-7},
	shorttitle = {Cnidaria},
	abstract = {Background
Identification of biological specimens is a requirement for a range of applications. Reference-free methods analyse unprocessed sequencing data without relying on prior knowledge, but generally do not scale to arbitrarily large genomes and arbitrarily large phylogenetic distances.

Results
We present Cnidaria, a practical tool for clustering genomic and transcriptomic data with no limitation on genome size or phylogenetic distances. We successfully simultaneously clustered 169 genomic and transcriptomic datasets from 4 kingdoms, achieving 100 \% identification accuracy at supra-species level and 78 \% accuracy at the species level.

Conclusion
{CNIDARIA} allows for fast, resource-efficient comparison and identification of both raw and assembled genome and transcriptome data. This can help answer both fundamental (e.g. in phylogeny, ecological diversity analysis) and practical questions (e.g. sequencing quality control, primer design).

Electronic supplementary material
The online version of this article (doi:10.1186/s12859-015-0806-7) contains supplementary material, which is available to authorized users.},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Aflitos, Saulo Alves and Severing, Edouard and Sanchez-Perez, Gabino and Peters, Sander and de Jong, Hans and de Ridder, Dick},
	urldate = {2018-04-02},
	date = {2015-11-02},
	pmid = {26525298},
	pmcid = {PMC4630969},
}

@article{mustafa_dynamic_2018,
	title = {Dynamic compression schemes for graph coloring},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/03/17/239806},
	doi = {10.1101/239806},
	abstract = {Technological advancements in high-throughput {DNA} sequencing have led to an exponential growth of sequencing data being produced and stored as a byproduct of biomedical research. Despite its public availability, a majority of this data remains hard to query to the research community due to a lack of efficient data representation and indexing solutions. One of the available techniques to represent read data is a condensed form as an assembly graph. Such a representation contains all sequence information but does not store contextual information and metadata. We present two new approaches for a compressed representation of a graph coloring: a lossless compression scheme based on a novel application of wavelet tries as well as a highly accurate lossy compression based on a set of Bloom filters. Both strategies retain a coloring with dynamically changing graph topology. We present construction and merge procedures for both methods and evaluate their performance on a wide range of different datasets. By dropping the requirement of a fully lossless compression and using the topological information of the underlying graph, we can reduce memory requirements by up to three orders of magnitude. Representing individual colors as independently stored modules, our approaches are fully dynamic and can be efficiently parallelized. These properties allow for an easy upscaling to the problem sizes common to the biomedical domain. We provide prototype implementations in C++, summaries of our experiments as well as links to all datasets publicly at https://github.com/ratschlab/graph\_annotation.},
	pages = {239806},
	journaltitle = {{bioRxiv}},
	author = {Mustafa, Harun and Schilken, Ingo and Karasikov, Mikhail and Eickhoff, Carsten and Ratsch, Gunnar and Kahles, Andre},
	urldate = {2018-04-02},
	date = {2018-03-17},
	langid = {english},
}

@article{zappia_clustering_2018,
	title = {Clustering trees: a visualisation for evaluating clusterings at multiple resolutions},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/05/22/274035},
	doi = {10.1101/274035},
	shorttitle = {Clustering trees},
	abstract = {Clustering techniques are widely used in the analysis of large data sets to group together samples with similar properties. For example, clustering is often used in the field of single-cell {RNA}- sequencing in order to identify different cell types present in a tissue sample. There are many algorithms for performing clustering and the results can vary substantially. In particular, the number of groups present in a data set is often unknown and the number of clusters identified by an algorithm can change based on the parameters used. To explore and examine the impact of varying clustering resolution we present clustering trees. This visualisation shows the relationships between clusters at multiple resolutions allowing researchers to see how samples move as the number of clusters increases. In addition, meta-information can be overlaid on the tree to inform the choice of resolution and guide in identification of clusters. We illustrate the features of clustering trees using a series of simulations as well as two real examples, the classical iris dataset and a complex single-cell {RNA}-sequencing dataset. Clustering trees can be produced using the clustree R package available from {CRAN} (https://{CRAN}.R-project.org/package=clustree) and developed on {GitHub} (https://github.com/lazappi/clustree).},
	pages = {274035},
	journaltitle = {{bioRxiv}},
	author = {Zappia, Luke and Oshlack, Alicia},
	urldate = {2018-06-18},
	date = {2018-05-22},
	langid = {english},
	keywords = {sbt},
}

@online{bovee_finch:_2018,
	title = {Finch: a tool adding dynamic abundance filtering to genomic {MinHashing}},
	url = {http://joss.theoj.org},
	shorttitle = {Finch},
	abstract = {The Journal of Open Source Software, a {\textless}strong{\textgreater}developer friendly{\textless}/strong{\textgreater} journal for research software packages.},
	titleaddon = {The Journal of Open Source Software},
	author = {Bovee, Roderick and Greenfield, Nick},
	urldate = {2018-06-26},
	date = {2018-02-01},
	langid = {english},
	doi = {10.21105/joss.00505},
}

@article{kucherov_algorithms_2018,
	title = {Algorithms for biosequence search: past, present and future},
	url = {http://arxiv.org/abs/1808.01038},
	shorttitle = {Algorithms for biosequence search},
	abstract = {The paper surveys the evolution of main algorithmic ideas used to compare and search biological sequences, including current trends and future prospects.},
	journaltitle = {{arXiv}:1808.01038 [q-bio]},
	author = {Kucherov, Gregory},
	urldate = {2018-08-07},
	date = {2018-08-02},
	eprinttype = {arxiv},
	eprint = {1808.01038},
	keywords = {Quantitative Biology - Genomics},
}

@inproceedings{yang_histosketch:_2018,
	title = {{HistoSketch}: Fast Similarity-Preserving Sketching of Streaming Histograms with Concept Drift},
	isbn = {978-1-5386-3835-4},
	url = {doi.ieeecomputersociety.org/10.1109/ICDM.2017.64},
	doi = {10.1109/ICDM.2017.64},
	shorttitle = {{HistoSketch}},
	abstract = {Histogram-based similarity has been widely adopted in many machine learning tasks. However, measuring histogram similarity is a challenging task for streaming data, where the elements of a histogram are observed in a streaming manner. First, the ever-growing cardinality of histogram elements makes any similarity computation inefficient. Second, the concept-drift issue in the data streams also impairs the accurate assessment of the similarity. In this paper, we propose to overcome the above challenges with {HistoSketch}, a fast similarity-preserving sketching method for streaming histograms with concept drift. Specifically, {HistoSketch} is designed to incrementally maintain a set of compact and fixed-size sketches of streaming histograms to approximate similarity between the histograms, with the special consideration of gradually forgetting the outdated histogram elements. We evaluate {HistoSketch} on multiple classification tasks using both synthetic and real-world datasets. The results show that our method is able to efficiently approximate similarity for streaming histograms and quickly adapt to concept drift. Compared to full streaming histograms gradually forgetting the outdated histogram elements, {HistoSketch} is able to dramatically reduce the classification time (with a 7500x speedup) with only a modest loss in accuracy (about 3.5\%).},
	pages = {545--554},
	booktitle = {2017 {IEEE} International Conference on Data Mining ({ICDM})},
	author = {Yang, D. and Li, B. and Rettig, L. and Cudre-Mauroux, P.},
	urldate = {2018-08-08},
	date = {2018-11},
	keywords = {learning (artificial intelligence), pattern classification},
}

@article{rowe_streaming_2018,
	title = {Streaming histogram sketching for rapid microbiome analytics},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/09/04/408070},
	doi = {10.1101/408070},
	abstract = {Motivation: The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research; allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time. To address this need, we propose a new method for the compact representation of microbiome sequencing data using similarity-preserving sketches of streaming k-mer spectra. These sketches allow for dissimilarity estimation, rapid microbiome catalogue searching, and classification of microbiome samples in near real-time. Results: We apply streaming histogram sketching to microbiome samples as a form of dimensionality reduction, creating a compressed "histosketch" that can be used to efficiently represent microbiome k-mer spectra. Using public microbiome datasets, we show that histosketches can be clustered by sample type using pairwise Jaccard similarity estimation, consequently allowing for rapid microbiome similarity searches via a locality sensitive hashing indexing scheme. Furthermore, we show that histosketches can be used to train machine learning classifiers to accurately label microbiome samples. Specifically, using a collection of 108 novel microbiome samples from a cohort of premature neonates, we trained and tested a Random Forest Classifier that could accurately predict whether the neonate had received antibiotic treatment (95\% accuracy, precision 97\%) and could subsequently be used to classify microbiome data streams in less than 12 seconds. We provide our implementation, Histosketching Using Little K-mers ({HULK}), which can histosketch a typical 2GB microbiome in 50 seconds on a standard laptop using 4 cores, with the sketch occupying 3000 bytes of disk space. Availability: Our implementation ({HULK}) is written in Go and is available at: https://github.com/will-rowe/hulk ({MIT} License).},
	pages = {408070},
	journaltitle = {{bioRxiv}},
	author = {Rowe, Will {PM} and Carrieri, Anna Paola and Alcon-Giner, Cristina and Caim, Shabhonam and Shaw, Alex and Sim, Kathleen and Kroll, J. Simon and Hall, Lindsay and Pyzer-Knapp, Edward O. and Winn, Martyn D.},
	urldate = {2018-09-05},
	date = {2018-09-04},
	langid = {english},
}

@article{wood_kraken:_2014,
	title = {Kraken: ultrafast metagenomic sequence classification using exact alignments},
	volume = {15},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/gb-2014-15-3-r46},
	doi = {10.1186/gb-2014-15-3-r46},
	shorttitle = {Kraken},
	abstract = {Kraken is an ultrafast and highly accurate program for assigning taxonomic labels to metagenomic {DNA} sequences. Previous programs designed for this task have been relatively slow and computationally expensive, forcing researchers to use faster abundance estimation programs, which only classify small subsets of metagenomic data. Using exact alignment of k-mers, Kraken achieves classification accuracy comparable to the fastest {BLAST} program. In its fastest mode, Kraken classifies 100 base pair reads at a rate of over 4.1 million reads per minute, 909 times faster than Megablast and 11 times faster than the abundance estimation program {MetaPhlAn}. Kraken is available at http://ccb.jhu.edu/software/kraken/.},
	pages = {R46},
	number = {3},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Wood, Derrick E. and Salzberg, Steven L.},
	urldate = {2018-09-22},
	date = {2014-03-03},
}

@article{freelon_computational_2018,
	title = {Computational research in the post-{API} age},
	url = {https://osf.io/preprints/socarxiv/56f4q/},
	doi = {10.31235/osf.io/56f4q},
	abstract = {Briefly describes the implications of the emerging post-{API} age for social science research methods.},
	journaltitle = {{SocArXiv}},
	author = {Freelon, Deen},
	urldate = {2018-09-28},
	date = {2018-08-20},
}

@online{noauthor_k-mer_nodate,
	title = {k-mer counting, part I: Introduction {\textbar} {BioInfoLogics}},
	url = {https://bioinfologics.github.io/post/2018/09/17/k-mer-counting-part-i-introduction/},
	urldate = {2018-09-28},
}

@article{chu_improving_2018,
	title = {Improving on hash-based probabilistic sequence classification using multiple spaced seeds and multi-index Bloom filters},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/10/05/434795},
	doi = {10.1101/434795},
	abstract = {Alignment-free classification of sequences against collections of sequences has enabled high-throughput processing of sequencing data in many bioinformatics analysis pipelines. Originally hash-table based, much work has been done to improve and reduce the memory requirement of indexing of k-mer sequences with probabilistic indexing strategies. These efforts have led to lower memory highly efficient indexes, but often lack sensitivity in the face of sequencing errors or polymorphism because they are k-mer based. To address this, we designed a new memory efficient data structure that can tolerate mismatches using multiple spaced seeds, called a multi-index Bloom Filter. Implemented as part of {BioBloom} Tools, we demonstrate our algorithm in two applications, read binning for targeted assembly and taxonomic read assignment. Our tool shows a higher sensitivity and specificity for read-binning than {BWA} {MEM} at an order of magnitude less time. For taxonomic classification, we show higher sensitivity than {CLARK}-S at an order of magnitude less time while using half the memory.},
	pages = {434795},
	journaltitle = {{bioRxiv}},
	author = {Chu, Justin and Mohamadi, Hamid and Erhan, Emre and Tse, Jeffery and Chiu, Readman and Yeo, Sarah and Birol, Inanc},
	urldate = {2018-10-05},
	date = {2018-10-05},
	langid = {english},
}

@article{lemire_roaring_2018,
	title = {Roaring Bitmaps: Implementation of an Optimized Software Library},
	volume = {48},
	issn = {00380644},
	url = {http://arxiv.org/abs/1709.07821},
	doi = {10.1002/spe.2560},
	shorttitle = {Roaring Bitmaps},
	abstract = {Compressed bitmap indexes are used in systems such as Git or Oracle to accelerate queries. They represent sets and often support operations such as unions, intersections, differences, and symmetric differences. Several important systems such as Elasticsearch, Apache Spark, Netflix's Atlas, {LinkedIn}'s Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez, Microsoft Visual Studio Team Services and Apache Kylin rely on a specific type of compressed bitmap index called Roaring. We present an optimized software library written in C implementing Roaring bitmaps: {CRoaring}. It benefits from several algorithms designed for the single-instruction-multiple-data ({SIMD}) instructions available on commodity processors. In particular, we present vectorized algorithms to compute the intersection, union, difference and symmetric difference between arrays. We benchmark the library against a wide range of competitive alternatives, identifying weaknesses and strengths in our software. Our work is available under a liberal open-source license.},
	pages = {867--895},
	number = {4},
	journaltitle = {Software: Practice and Experience},
	author = {Lemire, Daniel and Kaser, Owen and Kurz, Nathan and Deri, Luca and O'Hara, Chris and Saint-Jacques, François and Ssi-Yan-Kai, Gregory},
	urldate = {2018-10-07},
	date = {2018-04},
	eprinttype = {arxiv},
	eprint = {1709.07821},
	keywords = {Computer Science - Databases},
}

@online{noauthor_dream-yara:_nodate,
	title = {{DREAM}-Yara: an exact read mapper for very large databases with short update time {\textbar} Bioinformatics {\textbar} Oxford Academic},
	url = {https://academic.oup.com/bioinformatics/article/34/17/i766/5093228},
	urldate = {2018-10-10},
}

@article{piro_ganon:_2018,
	title = {ganon: continuously up-to-date with database growth for precise short read classification in metagenomics},
	url = {http://biorxiv.org/lookup/doi/10.1101/406017},
	doi = {10.1101/406017},
	shorttitle = {ganon},
	abstract = {The exponential growth of assembled genome sequences greatly benefits metagenomic studies, providing a broader catalog of reference organisms in a variety of studies. However, due to the massive amount of sequences, tools are struggling to manage such data and their frequent updates. Processing and indexing currently available repositories is no longer possible on standard infrastructures and can take days and hundreds of {GB} even on large servers. Few methods have acknowledged such issues thus far and even though many can theoretically handle large amounts of references, time-memory requirements are prohibitive for a real application. As a result, many tools still rely on static outdated datasets and clearly underperform under the currently available wealth of genome sequences. The content and taxonomic distribution of such databases is also a crucial factor, introducing bias when not properly managed. Motivated by those limitations we created ganon, a k-mer based short read classification tool which uses Interleaved Bloom Filters in conjunction with a taxonomic clustering and a k-mer counting-filtering scheme. Ganon provides an efficient method for indexing references and keeping them updated, requiring under 25 minutes to index the complete (i.e. high quality) archaeal-bacterial genomes and under 3 hours for all archaeal-bacterial genomes (30GB and 380GB of raw data, respectively). Ganon can keep such indices up-to-date in a fraction of the time necessary to create them allowing researchers to permanently work on the most recent references. Using data from the {CAMI} challenge, ganon shows strongly improved precision (in some cases more than double) while having equal or better sensitivity compared to state-of-the-art tools. Ganon's memory footprint is less than half of similar tools using the same reference set. Using a larger set of references with a broader diversity, ganon classified 25\% more reads achieving 97\% precision at species level. Ganon supports taxonomy and assembly level classification as well as multiple indices for a hierarchical classification. The software is open-source and available at: https://gitlab.com/rki\_bioinformatics/ganon},
	author = {Piro, Vitor C and Dadi, Temesgen Hailemariam and Seiler, Enrico and Reinert, Knut and Renard, Bernhard Y},
	urldate = {2018-10-10},
	date = {2018-08-31},
	langid = {english},
}

@online{noauthor_using_nodate,
	title = {Using Multidimensional Bloom filters to Search {RNAseq} Libraries - (i)},
	url = {http://homolog.us/blogs/blog/2016/12/13/multidimensional-bloom-i/},
	urldate = {2018-10-10},
}

@article{zhao_bindash_nodate,
	title = {{BinDash}, software for fast genome distance estimation on a typical personal laptop},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty651/5058094},
	doi = {10.1093/bioinformatics/bty651},
	abstract = {{AbstractMotivation}.  The number of genomes (including meta-genomes) is increasing at an accelerating pace. In the near future, we may need to estimate pairwise},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Zhao, {XiaoFei}},
	urldate = {2018-10-10},
	langid = {english},
}

@article{sivadasan_kmerlight:_2016,
	title = {Kmerlight: fast and accurate k-mer abundance estimation},
	url = {http://arxiv.org/abs/1609.05626},
	shorttitle = {Kmerlight},
	abstract = {k-mers (nucleotide strings of length k) form the basis of several algorithms in computational genomics. In particular, k-mer abundance information in sequence data is useful in read error correction, parameter estimation for genome assembly, digital normalization etc. We give a streaming algorithm Kmerlight for computing the k-mer abundance histogram from sequence data. Our algorithm is fast and uses very small memory footprint. We provide analytical bounds on the error guarantees of our algorithm. Kmerlight can efficiently process genome scale and metagenome scale data using standard desktop machines. Few applications of abundance histograms computed by Kmerlight are also shown. We use abundance histogram for de novo estimation of repetitiveness in the genome based on a simple probabilistic model that we propose. We also show estimation of k-mer error rate in the sampling using abundance histogram. Our algorithm can also be used for abundance estimation in a general streaming setting. The Kmerlight tool is written in C++ and is available for download and use from https://github.com/nsivad/kmerlight.},
	journaltitle = {{arXiv}:1609.05626 [cs]},
	author = {Sivadasan, Naveen and Srinivasan, Rajgopal and Goyal, Kshama},
	urldate = {2018-10-15},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.05626},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@online{noauthor_hulk_nodate,
	title = {Hulk and histosketch - Daniel's blog},
	url = {https://standage.github.io/a-brief-review-of-hulk-and-histosketch.html},
	urldate = {2018-10-19},
}

@article{nasko_refseq_2018,
	title = {{RefSeq} database growth influences the accuracy of k-mer-based lowest common ancestor species identification},
	volume = {19},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-018-1554-6},
	doi = {10.1186/s13059-018-1554-6},
	abstract = {In order to determine the role of the database in taxonomic sequence classification, we examine the influence of the database over time on k-mer-based lowest common ancestor taxonomic classification. We present three major findings: the number of new species added to the {NCBI} {RefSeq} database greatly outpaces the number of new genera; as a result, more reads are classified with newer database versions, but fewer are classified at the species level; and Bayesian-based re-estimation mitigates this effect but struggles with novel genomes. These results suggest a need for new classification approaches specially adapted for large databases.},
	pages = {165},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Nasko, Daniel J. and Koren, Sergey and Phillippy, Adam M. and Treangen, Todd J.},
	urldate = {2018-10-30},
	date = {2018-10-30},
}

@article{kirk_functional_2018,
	title = {Functional classification of long non-coding {RNAs} by k -mer content},
	volume = {50},
	rights = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-018-0207-8},
	doi = {10.1038/s41588-018-0207-8},
	abstract = {{SEEKR} is a method that deconstructs linear sequence relationships between {lncRNAs} and evaluates similarity on the basis of abundance of short motifs called k-mers. {LncRNAs} of related function often have similar k-mer profiles despite lacking linear homology.},
	pages = {1474},
	number = {10},
	journaltitle = {Nature Genetics},
	author = {Kirk, Jessime M. and Kim, Susan O. and Inoue, Kaoru and Smola, Matthew J. and Lee, David M. and Schertzer, Megan D. and Wooten, Joshua S. and Baker, Allison R. and Sprague, Daniel and Collins, David W. and Horning, Christopher R. and Wang, Shuo and Chen, Qidi and Weeks, Kevin M. and Mucha, Peter J. and Calabrese, J. Mauro},
	urldate = {2018-10-31},
	date = {2018-10},
}

@article{bar-joseph_fast_2001,
	title = {Fast optimal leaf ordering for hierarchical clustering},
	volume = {17},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/17/suppl_1/S22/261423},
	doi = {10.1093/bioinformatics/17.suppl_1.S22},
	abstract = {Abstract.  We present the first practical algorithm for the optimal
 linear leaf ordering of trees that are generated by hierarchical
 clustering. Hierarchical},
	pages = {S22--S29},
	issue = {suppl\_1},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Bar-Joseph, Ziv and Gifford, David K. and Jaakkola, Tommi S.},
	urldate = {2018-10-31},
	date = {2001-06-01},
	langid = {english},
}

@article{mullner_modern_2011,
	title = {Modern hierarchical, agglomerative clustering algorithms},
	url = {http://arxiv.org/abs/1109.2378},
	abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
	journaltitle = {{arXiv}:1109.2378 [cs, stat]},
	author = {Müllner, Daniel},
	urldate = {2018-10-31},
	date = {2011-09-12},
	eprinttype = {arxiv},
	eprint = {1109.2378},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning, 62H30, I.5.3},
}

@article{popic_hybrid_2017,
	title = {A hybrid cloud read aligner based on {MinHash} and kmer voting that preserves privacy},
	volume = {8},
	issn = {2041-1723},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440850/},
	doi = {10.1038/ncomms15311},
	abstract = {Low-cost clouds can alleviate the compute and storage burden of the genome sequencing data explosion. However, moving personal genome data analysis to the cloud can raise serious privacy concerns. Here, we devise a method named Balaur, a privacy preserving read mapper for hybrid clouds based on locality sensitive hashing and kmer voting. Balaur can securely outsource a substantial fraction of the computation to the public cloud, while being highly competitive in accuracy and speed with non-private state-of-the-art read aligners on short read data. We also show that the method is significantly faster than the state of the art in long read mapping. Therefore, Balaur can enable institutions handling massive genomic data sets to shift part of their analysis to the cloud without sacrificing accuracy or exposing sensitive information to an untrusted third party., Outsourcing computation for genomic data processing offers the ability to allocate massive computing power and storage on demand. Here, Popic and Batzoglou develop a hybrid cloud aligner for sequence read mapping that preserves privacy with competitive accuracy and speed.},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Popic, Victoria and Batzoglou, Serafim},
	urldate = {2018-11-01},
	date = {2017-05-16},
	pmid = {28508884},
	pmcid = {PMC5440850},
}

@inproceedings{leiserson_work-efficient_2010,
	location = {Thira, Santorini, Greece},
	title = {A work-efficient parallel breadth-first search algorithm (or how to cope with the nondeterminism of reducers)},
	isbn = {978-1-4503-0079-7},
	url = {http://portal.acm.org/citation.cfm?doid=1810479.1810534},
	doi = {10.1145/1810479.1810534},
	abstract = {We have developed a multithreaded implementation of breadth-ﬁrst search ({BFS}) of a sparse graph using the Cilk++ extensions to C++. Our {PBFS} program on a single processor runs as quickly as a standard C++ breadth-ﬁrst search implementation. {PBFS} achieves high work-efﬁciency by using a novel implementation of a multiset data structure, called a “bag,” in place of the {FIFO} queue usually employed in serial breadth-ﬁrst search algorithms. For a variety of benchmark input graphs whose diameters are signiﬁcantly smaller than the number of vertices — a condition met by many real-world graphs — {PBFS} demonstrates good speedup with the number of processing cores.},
	eventtitle = {the 22nd {ACM} symposium},
	pages = {303},
	booktitle = {Proceedings of the 22nd {ACM} symposium on Parallelism in algorithms and architectures - {SPAA} '10},
	publisher = {{ACM} Press},
	author = {Leiserson, Charles E. and Schardl, Tao B.},
	urldate = {2018-11-01},
	date = {2010},
	langid = {english},
}

@article{pandey_mantis:_2018,
	title = {Mantis: A Fast, Small, and Exact Large-Scale Sequence-Search Index},
	volume = {7},
	issn = {2405-4712},
	url = {https://www.cell.com/cell-systems/abstract/S2405-4712(18)30239-4},
	doi = {10.1016/j.cels.2018.05.021},
	shorttitle = {Mantis},
	pages = {201--207.e4},
	number = {2},
	journaltitle = {Cell Systems},
	shortjournal = {cels},
	author = {Pandey, Prashant and Almodaresi, Fatemeh and Bender, Michael A. and Ferdman, Michael and Johnson, Rob and Patro, Rob},
	urldate = {2018-11-02},
	date = {2018-08-22},
	pmid = {29936185},
	keywords = {de Bruijn graph, Bloom filter, color equivalence classes, counting quotient filter, experiment discovery, Mantis, {RNA} sequencing, sequence Bloom tree, sequence search},
}

@online{noauthor_[1803.01969]_nodate,
	title = {[1803.01969] Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries},
	url = {https://arxiv.org/abs/1803.01969},
	urldate = {2018-11-04},
}

@article{tai_sketching_2017,
	title = {Sketching Linear Classifiers over Data Streams},
	url = {http://arxiv.org/abs/1711.02305},
	abstract = {We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the model. This enables memory-limited execution of several statistical analyses over streams, including online feature selection, streaming data explanation, relative deltoid detection, and streaming estimation of pointwise mutual information. Unlike related sketches that capture the most frequently-occurring features (or items) in a data stream, the Weight-Median Sketch captures the features that are most discriminative of one stream (or class) compared to another. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We provide a theoretical analysis that establishes recovery guarantees for batch and online learning, and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods, including count-based sketches and feature hashing.},
	journaltitle = {{arXiv}:1711.02305 [cs, stat]},
	author = {Tai, Kai Sheng and Sharan, Vatsal and Bailis, Peter and Valiant, Gregory},
	urldate = {2018-11-04},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1711.02305},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{harris_improved_2018,
	title = {Improved Representation of Sequence Bloom Trees},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/12/19/501452},
	doi = {10.1101/501452},
	abstract = {Algorithmic solutions to index and search biological databases are a fundamental part of bioinformatics, providing underlying components to many end-user tools. Inexpensive next generation sequencing has filled publicly available databases such as the Sequence Read Archive beyond the capacity of traditional indexing methods. Recently, the Sequence Bloom Tree ({SBT}) and its derivatives were proposed as a way to efficiently index such data for queries about transcript presence. We build on the {SBT} framework to construct the {HowDe}-{SBT} data structure, which uses a novel partitioning of information to reduce the construction and query time as well as the size of the index. We evaluate {HowDe}-{SBT} by both proving theoretical bounds on its performance and using real {RNA}-seq data. Compared to previous {SBT} methods, {HowDe}-{SBT} can construct the index in less than half the time, and with less than 32\% of the space, and can answer small-batch queries nine times faster.},
	pages = {501452},
	journaltitle = {{bioRxiv}},
	author = {Harris, Robert S. and Medvedev, Paul},
	urldate = {2018-12-20},
	date = {2018-12-19},
	langid = {english},
}

@article{baker_dashing:_2018,
	title = {Dashing: Fast and Accurate Genomic Distances with {HyperLogLog}},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/12/20/501726},
	doi = {10.1101/501726},
	shorttitle = {Dashing},
	abstract = {Dashing is a fast and accurate software tool for estimating similarities of genomes or sequencing datasets. It uses the {HyperLogLog} sketch together with cardinality estimation methods that specialize in set unions and intersections. Dashing sketches genomes more rapidly than previous {MinHash}-based methods while providing greater accuracy across a wide range of input sizes and sketch sizes. It can sketch and calculate pairwise distances for over 87K genomes in under 6 minutes. Dashing is open source and available at https://github.com/dnbaker/dashing.},
	pages = {501726},
	journaltitle = {{bioRxiv}},
	author = {Baker, Daniel N. and Langmead, Benjamin},
	urldate = {2018-12-20},
	date = {2018-12-20},
	langid = {english},
}

@article{benson_genbank_2017,
	title = {{GenBank}},
	volume = {45},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/45/D1/D37/2605704},
	doi = {10.1093/nar/gkw1070},
	abstract = {Abstract.  {GenBank}® (www.ncbi.nlm.nih.gov/genbank/) is a comprehensive database that contains publicly available nucleotide sequences for 370 000 formally descr},
	pages = {D37--D42},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Benson, Dennis A. and Cavanaugh, Mark and Clark, Karen and Karsch-Mizrachi, Ilene and Lipman, David J. and Ostell, James and Sayers, Eric W.},
	urldate = {2018-12-26},
	date = {2017-01-04},
	langid = {english},
}

@online{noauthor_rusty_nodate,
	title = {A Rusty Advent of Code - Casey Primozic},
	url = {https://cprimozic.net/blog/a-rusty-aoc/},
	urldate = {2019-01-04},
}

@article{jeffryes_rapid_2018,
	title = {Rapid identification of novel protein families using similarity searches},
	volume = {7},
	issn = {2046-1402},
	url = {https://f1000research.com/articles/7-1975/v1},
	doi = {10.12688/f1000research.17315.1},
	pages = {1975},
	journaltitle = {F1000Research},
	author = {Jeffryes, Matt and Bateman, Alex},
	urldate = {2019-01-05},
	date = {2018-12-24},
	langid = {english},
}

@online{noauthor_sanity_2019,
	title = {Sanity check: Code to concurrently process items from iterator in rayon},
	url = {https://users.rust-lang.org/t/sanity-check-code-to-concurrently-process-items-from-iterator-in-rayon/24197/9},
	shorttitle = {Sanity check},
	abstract = {Wow, that’s awesome and does exactly what I want. Thanks!  And it was all right there in the docs, even with channels as example use case 😦 . I hadn’t come across the try pattern for iterators yet and so those methods didn’t stand out to me. And after not being able to do what I wanted with the default map I got side tracked trying to work with rayon at a lower level.  For anyone following along here’s a much shorter way to do what I was trying to do in original post:  pub fn process\_it...},
	titleaddon = {The Rust Programming Language Forum},
	urldate = {2019-01-17},
	date = {2019-01-15},
	langid = {english},
}

@article{leskovec_mining_nodate,
	title = {Mining of Massive Datasets},
	pages = {513},
	author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeﬀrey D},
	langid = {english},
}

@article{marcais_locality_2019,
	title = {Locality sensitive hashing for the edit distance},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/534446v1},
	doi = {10.1101/534446},
	abstract = {{\textless}p{\textgreater}Motivation: Sequence alignment is a central operation in bioinformatics pipeline and, despite many improvements, remains a computationally challenging problem. Locality Sensitive Hashing ({LSH}) is one method used to estimate the likelihood of two sequences to have a proper alignment. Using an {LSH}, it is possible to separate, with high probability and relatively low computation, the pairs of sequences that do not have an alignment from those that may have an alignment. Therefore, an {LSH} reduces in the overall computational requirement while not introducing many false negatives (i.e., omitting to report a valid alignment). However, current {LSH} methods treat sequences as a bag of k-mers and do not take into account the relative ordering of k-mers in sequences. And due to the lack of a practical {LSH} method for edit distance, in practice, {LSH} methods for Jaccard similarity or Hamming distance are used as a proxy. Results: We present an {LSH} method, called Order Min Hash ({OMH}), for the edit distance. This method is a refinement of the {minHash} {LSH} used to approximate the Jaccard similarity, in that {OMH} is not only sensitive to the k-mer contents of the sequences but also to the relative order of the k-mers in the sequences. We present theoretical guarantees of the {OMH} as a gapped {LSH}.{\textless}/p{\textgreater}},
	pages = {534446},
	journaltitle = {{bioRxiv}},
	author = {Marcais, Guillaume and {DeBlasio}, Dan and Pandey, Prashant and Kingsford, Carl},
	urldate = {2019-01-30},
	date = {2019-01-29},
	langid = {english},
}

@article{bradley_ultrafast_2019,
	title = {Ultrafast search of all deposited bacterial and viral genomic data},
	volume = {37},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-018-0010-1/},
	doi = {10.1038/s41587-018-0010-1},
	abstract = {The global set of bacterial and viral sequences can be rapidly searched using a data structure inspired by web-search algorithms.},
	pages = {152},
	number = {2},
	journaltitle = {Nature Biotechnology},
	author = {Bradley, Phelim and Bakker, Henk C. den and Rocha, Eduardo P. C. and {McVean}, Gil and Iqbal, Zamin},
	urldate = {2019-02-14},
	date = {2019-02},
}

@article{ertl_new_2017-1,
	title = {New cardinality estimation algorithms for {HyperLogLog} sketches},
	url = {http://arxiv.org/abs/1702.01284},
	abstract = {This paper presents new methods to estimate the cardinalities of data sets recorded by {HyperLogLog} sketches. A theoretically motivated extension to the original estimator is presented that eliminates the bias for small and large cardinalities. Based on the maximum likelihood principle a second unbiased method is derived together with a robust and efficient numerical algorithm to calculate the estimate. The maximum likelihood approach can also be applied to more than a single {HyperLogLog} sketch. In particular, it is shown that it gives more precise cardinality estimates for union, intersection, or relative complements of two sets that are both represented by {HyperLogLog} sketches compared to the conventional technique using the inclusion-exclusion principle. All the new methods are demonstrated and verified by extensive simulations.},
	journaltitle = {{arXiv}:1702.01284 [cs]},
	author = {Ertl, Otmar},
	urldate = {2019-02-28},
	date = {2017-02-04},
	eprinttype = {arxiv},
	eprint = {1702.01284},
	keywords = {Computer Science - Data Structures and Algorithms, 68W15, 68W25, 62-07, E.1, H.2.8, I.1.2},
}

@inproceedings{ertl_bagminhash_2018,
	location = {New York, {NY}, {USA}},
	title = {{BagMinHash} - Minwise Hashing Algorithm for Weighted Sets},
	isbn = {978-1-4503-5552-0},
	url = {http://doi.acm.org/10.1145/3219819.3220089},
	doi = {10.1145/3219819.3220089},
	series = {{KDD} '18},
	abstract = {Minwise hashing has become a standard tool to calculate signatures which allow direct estimation of Jaccard similarities. While very efficient algorithms already exist for the unweighted case, the calculation of signatures for weighted sets is still a time consuming task. {BagMinHash} is a new algorithm that can be orders of magnitude faster than current state of the art without any particular restrictions or assumptions on weights or data dimensionality. Applied to the special case of unweighted sets, it represents the first efficient algorithm producing independent signature components. A series of tests finally verifies the new algorithm and also reveals limitations of other approaches published in the recent past.},
	pages = {1368--1377},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Ertl, Otmar},
	urldate = {2019-03-04},
	date = {2018},
	note = {event-place: London, United Kingdom},
	keywords = {consistent weighted sampling, jaccard similarity, locality-sensitive hashing, sketching algorithms, weighted minwise hashing},
}

@article{dunning_computing_2019,
	title = {Computing Extremely Accurate Quantiles Using t-Digests},
	url = {http://arxiv.org/abs/1902.04023},
	abstract = {We present on-line algorithms for computing approximations of rank-based statistics that give high accuracy, particularly near the tails of a distribution, with very small sketches. Notably, the method allows a quantile \$q\$ to be computed with an accuracy relative to \${\textbackslash}max(q, 1-q)\$ rather than absolute accuracy as with most other methods. This new algorithm is robust with respect to skewed distributions or ordered datasets and allows separately computed summaries to be combined with no loss in accuracy. An open-source Java implementation of this algorithm is available from the author. Independent implementations in Go and Python are also available.},
	journaltitle = {{arXiv}:1902.04023 [cs, stat]},
	author = {Dunning, Ted and Ertl, Otmar},
	urldate = {2019-03-04},
	date = {2019-02-11},
	eprinttype = {arxiv},
	eprint = {1902.04023},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Computation},
}

@article{marcais_asymptotically_2018,
	title = {Asymptotically optimal minimizers schemes},
	volume = {34},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/34/13/i13/5045769},
	doi = {10.1093/bioinformatics/bty258},
	abstract = {{AbstractMotivation}.  The minimizers technique is a method to sample k-mers that is used in many bioinformatics software to reduce computation, memory usage and},
	pages = {i13--i22},
	number = {13},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Marçais, Guillaume and {DeBlasio}, Dan and Kingsford, Carl},
	urldate = {2019-03-04},
	date = {2018-07-01},
	langid = {english},
}

@article{orenstein_designing_2017,
	title = {Designing small universal k-mer hitting sets for improved analysis of high-throughput sequencing},
	volume = {13},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005777},
	doi = {10.1371/journal.pcbi.1005777},
	abstract = {With the rapidly increasing volume of deep sequencing data, more efficient algorithms and data structures are needed. Minimizers are a central recent paradigm that has improved various sequence analysis tasks, including hashing for faster read overlap detection, sparse suffix arrays for creating smaller indexes, and Bloom filters for speeding up sequence search. Here, we propose an alternative paradigm that can lead to substantial further improvement in these and other tasks. For integers k and L {\textgreater} k, we say that a set of k-mers is a universal hitting set ({UHS}) if every possible L-long sequence must contain a k-mer from the set. We develop a heuristic called {DOCKS} to find a compact {UHS}, which works in two phases: The first phase is solved optimally, and for the second we propose several efficient heuristics, trading set size for speed and memory. The use of heuristics is motivated by showing the {NP}-hardness of a closely related problem. We show that {DOCKS} works well in practice and produces {UHSs} that are very close to a theoretical lower bound. We present results for various values of k and L and by applying them to real genomes show that {UHSs} indeed improve over minimizers. In particular, {DOCKS} uses less than 30\% of the 10-mers needed to span the human genome compared to minimizers. The software and computed {UHSs} are freely available at github.com/Shamir-Lab/{DOCKS}/ and acgt.cs.tau.ac.il/docks/, respectively.},
	pages = {e1005777},
	number = {10},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Orenstein, Yaron and Pellow, David and Marçais, Guillaume and Shamir, Ron and Kingsford, Carl},
	urldate = {2019-03-04},
	date = {2017-10-02},
	langid = {english},
	keywords = {Sequence analysis, Algorithms, Human genomics, {DNA} sequencing, Bacterial genomics, Directed acyclic graphs, Invertebrate genomics, Polynomials},
}

@article{loeffler_analysis_2019,
	title = {Analysis of multiple fungal sequence repositories highlights shortcomings in microbial databases},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/497867v2},
	doi = {10.1101/497867},
	abstract = {{\textless}p{\textgreater}Reference genomes are essential for metagenomics studies, which require comparing short metagenomic reads with available reference genomes to identify organisms within a sample. We analyzed the current state of fungal reference databases to assess their usability as reference databases for metagenomic studies. The overlap of genera and species in the databases analyzed was alarmingly small. In other words, using only a single reference database for analysis of metagenomic samples possibly results in the failure to identify some organisms in the sample. Communication between database developers needs to be established to create a set of standards for the way reference databases are organized and distributed.{\textless}/p{\textgreater}},
	pages = {497867},
	journaltitle = {{bioRxiv}},
	author = {Loeffler, Caitlin and Karlsberg, Aaron and Eskin, Eleazar and Koslicki, David and Mangul, Serghei},
	urldate = {2019-03-12},
	date = {2019-02-27},
	langid = {english},
}

@online{moulton_minhashing_2018,
	title = {{MinHashing}},
	url = {https://moultano.wordpress.com/2018/11/08/minhashing-3kbzhsxyg4467-6/},
	abstract = {Clustering in linear time with a key-value store and a few lines of code.},
	titleaddon = {Ryan Moulton's Articles},
	author = {Moulton, Ryan},
	urldate = {2019-03-12},
	date = {2018-11-08},
	langid = {english},
}

@software{chen_simulating_2018,
	title = {Simulating the performance of various streaming algorithms. \#experimentalmathematics: echen/streaming-simulations},
	url = {https://github.com/echen/streaming-simulations},
	shorttitle = {Simulating the performance of various streaming algorithms. \#experimentalmathematics},
	author = {Chen, Edwin},
	urldate = {2019-03-12},
	date = {2018-11-20},
	note = {original-date: 2012-12-23T22:59:49Z}
}

@online{noauthor_introduction_nodate,
	title = {Introduction to Locality-Sensitive Hashing},
	url = {http://unboxresearch.com/articles/lsh_post1.html},
	urldate = {2019-03-12},
}

@online{noauthor_choosing_nodate,
	title = {Choosing parameters — drep 2.0.0 documentation},
	url = {https://drep.readthedocs.io/en/latest/choosing_parameters.html},
	urldate = {2019-03-12},
}

@software{nikitaivkin_simple_2019,
	title = {Simple Hierarchical Count Sketch in Python . Contribute to nikitaivkin/csh development by creating an account on {GitHub}},
	url = {https://github.com/nikitaivkin/csh},
	author = {nikitaivkin},
	urldate = {2019-03-13},
	date = {2019-03-11},
	note = {original-date: 2018-11-24T22:56:21Z}
}

@article{fernandez_lazo:_nodate,
	title = {Lazo: A Cardinality-Based Method for Coupled Estimation of Jaccard Similarity and Containment},
	abstract = {Data analysts often need to ﬁnd datasets that are similar (i.e., have high overlap) or that are subsets of one another (i.e., one contains the other). Exactly computing such relationships is expensive because it entails an all-pairs comparison between all values in all datasets, an O(n2) operation. Fortunately, it is possible to obtain approximate solutions much faster, using locality sensitive hashing ({LSH}). Unfortunately, {LSH} does not lend itself naturally to compute containment, and only returns results with a similarity beyond a pre-deﬁned threshold; we want to know the speciﬁc similarity and containment score.},
	pages = {12},
	author = {Fernandez, Raul Castro and Min, Jisoo and Nava, Demitri and Madden, Samuel},
	langid = {english},
}

@article{chikhi_data_2019,
	title = {Data structures to represent sets of k-long {DNA} sequences},
	url = {http://arxiv.org/abs/1903.12312},
	abstract = {The analysis of biological sequencing data has been one of the biggest applications of string algorithms. The approaches used in many such applications are based on the analysis of kmers, which are short ﬁxed-length strings present in a dataset. While these approaches are rather diverse, storing and querying k-mer sets has emerged as a shared underlying component. Sets of k-mers have unique features and applications that, over the last ten years, have resulted in many specialized approaches for their representation. In this survey, we give a uniﬁed presentation and comparison of the data structures that have been proposed to store and query k-mer sets. We hope this survey will not only serve as a resource for researchers in the ﬁeld but also make the area more accessible to outsiders.},
	journaltitle = {{arXiv}:1903.12312 [cs, q-bio]},
	author = {Chikhi, Rayan and Holub, Jan and Medvedev, Paul},
	urldate = {2019-04-01},
	date = {2019-03-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.12312},
	keywords = {Computer Science - Data Structures and Algorithms, Quantitative Biology - Genomics},
}

@article{ahle_subsets_2019,
	title = {Subsets and Supermajorities: Unifying Hashing-based Set Similarity Search},
	url = {http://arxiv.org/abs/1904.04045},
	shorttitle = {Subsets and Supermajorities},
	abstract = {We consider the problem of designing Locality Sensitive Filters ({LSF}) for set overlaps, also known as maximum inner product search on binary data. We give a simple data structure that generalizes and outperforms previous algorithms such as {MinHash} [J. Discrete Algorithms 1998], {SimHash} [{STOC} 2002], Spherical {LSF} [{SODA} 2017] and Chosen Path [{STOC} 2017]; and we show matching lower bounds using hypercontractive inequalities for a wide range of parameters and space/time trade-offs. This answers the main open question in Christiani and Pagh [{STOC} 2017] on unifying the landscape of Locality Sensitive (non-data-dependent) set similarity search.},
	journaltitle = {{arXiv}:1904.04045 [cs]},
	author = {Ahle, Thomas Dybdahl},
	urldate = {2019-04-12},
	date = {2019-04-08},
	eprinttype = {arxiv},
	eprint = {1904.04045},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, 68W01, Computer Science - Computational Geometry},
}

@article{marcais_sketching_2019,
	title = {Sketching and Sublinear Data Structures in Genomics},
	volume = {2},
	url = {https://doi.org/10.1146/annurev-biodatasci-072018-021156},
	doi = {10.1146/annurev-biodatasci-072018-021156},
	abstract = {Large-scale genomics demands computational methods that scale sublinearly with the growth of data. We review several data structures and sketching techniques that have been used in genomic analysis methods. Specifically, we focus on four key ideas that take different approaches to achieve sublinear space usage and processing time: compressed full text indices, approximate membership query data structures, locality-sensitive hashing, and minimizers schemes. We describe these techniques at a high level and give several representative applications of each. Expected final online publication date for the Annual Review of Biomedical Data Science Volume 2 is July 22, 2019. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	pages = {null},
	number = {1},
	journaltitle = {Annual Review of Biomedical Data Science},
	author = {Marçais, Guillaume and Solomon, Brad and Patro, Rob and Kingsford, Carl},
	urldate = {2019-04-15},
	date = {2019},
}

@article{kucherov_evolution_nodate,
	title = {Evolution of biosequence search algorithms: a brief survey},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btz272/5474902},
	doi = {10.1093/bioinformatics/btz272},
	shorttitle = {Evolution of biosequence search algorithms},
	abstract = {{AbstractMotivation}.  Although modern high-throughput biomolecular technologies produce various types of data, biosequence data remains at the core of bioinforma},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Kucherov, Gregory},
	urldate = {2019-04-23},
	langid = {english},
}

@article{zhu_josie:_nodate,
	title = {{JOSIE}: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes},
	abstract = {We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. The problem can be formulated as an overlap set similarity search problem by considering columns as sets and matching values as intersection between sets. Although set similarity search is well-studied in the field of approximate string search (e.g., fuzzy keyword search), the solutions are designed for and evaluated over sets of relatively small size (average set size rarely much over 100 and maximum set size in the low thousands) with modest dictionary sizes (the total number of distinct values in all sets is only a few million). We observe that modern data lakes typically have massive set sizes (with maximum set sizes that may be tens of millions) and dictionaries that include hundreds of millions of distinct values. Our new algorithm, {JOSIE} ({JOining} Search using Intersection Estimation) minimizes the cost of set reads and inverted index probes used in finding the top-k sets. We show that {JOSIE} completely out performs the state-of-the-art overlap set similarity search techniques on data lakes. More surprising, we also consider state-of-the-art approximate algorithm and show that our new exact search algorithm performs almost as well, and even in some cases better, on real data lakes.},
	pages = {18},
	author = {Zhu, Erkang and Deng, Dong and Nargesian, Fatemeh and Miller, Renée J},
	langid = {english},
}

@thesis{ramesh_deep_2019,
	title = {Deep Learning for Taxonomy Prediction},
	rights = {This item is protected by copyright and/or related rights. Some uses of this item may be deemed fair and permitted by law even without permission from the rights holder(s), or the rights holder(s) may have licensed the work for use under certain conditions. For other uses you need to obtain permission from the rights holder(s).},
	url = {https://vtechworks.lib.vt.edu/handle/10919/89752},
	abstract = {The last decade has seen great advances in Next-Generation Sequencing technologies, and, as a result, there has been a rise in the number of genomes sequenced each year. In 2017, there were as many as 10,000 new organisms sequenced and added into the {RefSeq} Database. Taxonomy prediction is a science involving the hierarchical classification of {DNA} fragments up to the rank species. In this research, we introduce Predicting Linked Organisms, Plinko, for short. Plinko is a fully-functioning, state-of-the-art predictive system that accurately captures {DNA} - Taxonomy relationships where other state-of-the-art algorithms falter. Plinko leverages multi-view convolutional neural networks and the pre-defined taxonomy tree structure to improve multi-level taxonomy prediction. In the Plinko strategy, each network takes advantage of different word usage patterns corresponding to different levels of evolutionary divergence. Plinko has the advantages of relatively low storage, {GPGPU} parallel training and inference, making the solution portable, and scalable with anticipated genome database growth. To the best of our knowledge, Plinko is the first to use multi-view convolutional neural networks as the core algorithm in a compositional,alignment-free approach to taxonomy prediction.},
	institution = {Virginia Tech},
	type = {Thesis},
	author = {Ramesh, Shreyas},
	urldate = {2019-06-11},
	date = {2019-06-04},
	langid = {english},
}

@article{bingmann_cobs:_2019,
	title = {{COBS}: a Compact Bit-Sliced Signature Index},
	url = {http://arxiv.org/abs/1905.09624},
	shorttitle = {{COBS}},
	abstract = {We present {COBS}, a compact bit-sliced signature index, which is a cross-over between an inverted index and Bloom ﬁlters. Our target application is to index k-mers of {DNA} samples or q-grams from text documents and process approximate pattern matching queries on the corpus with a userchosen coverage threshold. Query results may contain a number of false positives which decreases exponentially with the query length and the false positive rate of the index determined at construction time. We compare {COBS} to seven other index software packages on 100 000 microbial {DNA} samples. {COBS}’ compact but simple data structure outperforms the other indexes in construction time and query performance with Mantis by Pandey et al. on second place. However, diﬀerent from Mantis and other previous work, {COBS} does not need the complete index in {RAM} and is thus designed to scale to larger document sets.},
	journaltitle = {{arXiv}:1905.09624 [cs]},
	author = {Bingmann, Timo and Bradley, Phelim and Gauger, Florian and Iqbal, Zamin},
	urldate = {2019-06-11},
	date = {2019-05-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.09624},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Databases, H.3.1, H.3.3},
}

@article{marti_not_2019,
	title = {Not just {BLAST} nt: {WGS} database joins the party},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/653592v1},
	doi = {10.1101/653592},
	shorttitle = {Not just {BLAST} nt},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Since its introduction in 1990 and with over 50k citations, the {NCBI} {BLAST} family has been an essential tool of \textit{in silico} molecular biology. The {BLAST} nt database, based on the traditional divisions of {GenBank}, has been the default and most comprehensive database for nucleotide {BLAST} searches and for taxonomic classification software in metagenomics. Here we argue that this is no longer the case. Currently, the {NCBI} {WGS} database contains one billion reads (almost five times more than {GenBank}), and with 4.4 trillion nucleotides, {WGS} has about 14 times more nucleotides than {GenBank}. This ratio is growing with time. We advocate a change in the database paradigm in taxonomic classification by systematically combining the nt and {WGS} databases in order to boost taxonomic classifiers sensitivity. We present here a case in which, by adding {WGS} data, we obtained over five times more classified reads and with a higher confidence score. To facilitate the adoption of this approach, we provide the {draftGenomes} script.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Author summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}Culture-independent methods are revolutionizing biology. The {NIH}/{NCBI} Basic Local Alignment Search Tool ({BLAST}) is one of the most widely used methods in computational biology. The {BLAST} nt database has become a de facto standard for taxonomic classifiers in metagenomics. We believe that it is time for a change in the database paradigm for such a classification. We advocate the systematic combination of the {BLAST} nt database with genomes of the massive {NCBI} Whole-Genome Shotgun ({WGS}) database. We make {draftGenomes} available, a script that eases the adoption of this approach. Current developments and technologies make it feasible now. Our recent results in several metagenomic projects indicate that this strategy boosts the sensitivity in taxonomic classifications.{\textless}/p{\textgreater}},
	pages = {653592},
	journaltitle = {{bioRxiv}},
	author = {Martí, Jose Manuel and Garay, Carlos P.},
	urldate = {2019-08-22},
	date = {2019-06-04},
	langid = {english},
}

@software{bingmann_experiment_2019,
	title = {Experiment Scripts for {COBS}, {BIGSI}, Mantis, and various {SBTs}: bingmann/cobs-experiments},
	url = {https://github.com/bingmann/cobs-experiments},
	shorttitle = {Experiment Scripts for {COBS}, {BIGSI}, Mantis, and various {SBTs}},
	author = {Bingmann, Timo},
	urldate = {2019-08-22},
	date = {2019-06-12},
	note = {original-date: 2019-04-16T11:11:10Z}
}

@software{piro_benchmarking_2019,
	title = {benchmarking scripts for ganon. Contribute to pirovc/ganon\_benchmark development by creating an account on {GitHub}},
	rights = {{MIT}},
	url = {https://github.com/pirovc/ganon_benchmark},
	author = {Piro, Vitor C.},
	urldate = {2019-08-22},
	date = {2019-08-22},
	note = {original-date: 2019-07-10T14:35:14Z}
}

@software{mangul_my_2019,
	title = {my public presentations. Contribute to smangul1/my.presentations development by creating an account on {GitHub}},
	url = {https://github.com/smangul1/my.presentations},
	author = {Mangul, Serghei},
	urldate = {2019-08-22},
	date = {2019-07-13},
	note = {original-date: 2019-04-11T17:50:31Z}
}

@article{ye_benchmarking_2019,
	title = {Benchmarking Metagenomics Tools for Taxonomic Classification},
	volume = {178},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(19)30775-5},
	doi = {10.1016/j.cell.2019.07.010},
	abstract = {{\textless}p{\textgreater}Metagenomic sequencing is revolutionizing the detection and characterization of microbial species, and a wide variety of software tools are available to perform taxonomic classification of these data. The fast pace of development of these tools and the complexity of metagenomic data make it important that researchers are able to benchmark their performance. Here, we review current approaches for metagenomic analysis and evaluate the performance of 20 metagenomic classifiers using simulated and experimental datasets. We describe the key metrics used to assess performance, offer a framework for the comparison of additional classifiers, and discuss the future of metagenomic data analysis.{\textless}/p{\textgreater}},
	pages = {779--794},
	number = {4},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Ye, Simon H. and Siddle, Katherine J. and Park, Daniel J. and Sabeti, Pardis C.},
	urldate = {2019-08-22},
	date = {2019-08-08},
	pmid = {31398336},
}

@online{jacobs_im_2019,
	title = {I'm working on a new google doc cataloging public \#microbiome/\#metagenomics data.  Anyone know of a systematic review of all available dbs in this space? Or a public document?  both 16/{ITS} and shotgun data And-yes I'll share the doc once ready-and no {SRA} doesn't have it all},
	url = {https://twitter.com/bioinformer/status/1159871345087242240},
	titleaddon = {@bioinformer},
	type = {Tweet},
	author = {Jacobs, Jonathan},
	urldate = {2019-08-22},
	date = {2019-08-09},
	langid = {english},
}

@article{wood_improved_2019,
	title = {Improved metagenomic analysis with Kraken 2},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/762302v1},
	doi = {10.1101/762302},
	abstract = {{\textless}p{\textgreater}Although Kraken9s k-mer-based approach provides fast taxonomic classification of metagenomic sequence data, its large memory requirements can be limiting for some applications. Kraken 2 improves upon Kraken 1 by reducing memory usage by 85\%, allowing greater amounts of reference genomic data to be used, while maintaining high accuracy and increasing speed five-fold. Kraken 2 also introduces a translated search mode, providing increased sensitivity in viral metagenomics analysis.{\textless}/p{\textgreater}},
	pages = {762302},
	journaltitle = {{bioRxiv}},
	author = {Wood, Derrick E. and Lu, Jennifer and Langmead, Ben},
	urldate = {2019-09-24},
	date = {2019-09-07},
	langid = {english},
}

@article{ye_benchmarking_2019-1,
	title = {Benchmarking Metagenomics Tools for Taxonomic Classification},
	volume = {178},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(19)30775-5},
	doi = {10.1016/j.cell.2019.07.010},
	pages = {779--794},
	number = {4},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Ye, Simon H. and Siddle, Katherine J. and Park, Daniel J. and Sabeti, Pardis C.},
	urldate = {2019-09-24},
	date = {2019-08-08},
	pmid = {31398336},
}

@article{chikhi_recent_2019,
	title = {Recent advances in data structures for storing sets of k-mer sets},
	pages = {25},
	author = {Chikhi, Rayan},
	date = {2019},
	langid = {english},
}

@article{rowe_streaming_2019,
	title = {Streaming histogram sketching for rapid microbiome analytics},
	volume = {7},
	issn = {2049-2618},
	url = {https://doi.org/10.1186/s40168-019-0653-2},
	doi = {10.1186/s40168-019-0653-2},
	abstract = {The growth in publically available microbiome data in recent years has yielded an invaluable resource for genomic research, allowing for the design of new studies, augmentation of novel datasets and reanalysis of published works. This vast amount of microbiome data, as well as the widespread proliferation of microbiome research and the looming era of clinical metagenomics, means there is an urgent need to develop analytics that can process huge amounts of data in a short amount of time.},
	pages = {40},
	number = {1},
	journaltitle = {Microbiome},
	shortjournal = {Microbiome},
	author = {Rowe, Will {PM} and Carrieri, Anna Paola and Alcon-Giner, Cristina and Caim, Shabhonam and Shaw, Alex and Sim, Kathleen and Kroll, J. Simon and Hall, Lindsay J. and Pyzer-Knapp, Edward O. and Winn, Martyn D.},
	urldate = {2019-10-11},
	date = {2019-03-16},
}

@article{benoit_multiple_2016,
	title = {Multiple comparative metagenomics using multiset k-mer counting},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-94},
	doi = {10.7717/peerj-cs.94},
	abstract = {Background Large scale metagenomic projects aim to extract biodiversity knowledge between different environmental conditions. Current methods for comparing microbial communities face important limitations. Those based on taxonomical or functional assignation rely on a small subset of the sequences that can be associated to known organisms. On the other hand, de novo methods, that compare the whole sets of sequences, either do not scale up on ambitious metagenomic projects or do not provide precise and exhaustive results. Methods These limitations motivated the development of a new de novo metagenomic comparative method, called Simka. This method computes a large collection of standard ecological distances by replacing species counts by k-mer counts. Simka scales-up today’s metagenomic projects thanks to a new parallel k-mer counting strategy on multiple datasets. Results Experiments on public Human Microbiome Project datasets demonstrate that Simka captures the essential underlying biological structure. Simka was able to compute in a few hours both qualitative and quantitative ecological distances on hundreds of metagenomic samples (690 samples, 32 billions of reads). We also demonstrate that analyzing metagenomes at the k-mer level is highly correlated with extremely precise de novo comparison techniques which rely on all-versus-all sequences alignment strategy or which are based on taxonomic profiling.},
	pages = {e94},
	journaltitle = {{PeerJ} Computer Science},
	shortjournal = {{PeerJ} Comput. Sci.},
	author = {Benoit, Gaëtan and Peterlongo, Pierre and Mariadassou, Mahendra and Drezen, Erwan and Schbath, Sophie and Lavenier, Dominique and Lemaitre, Claire},
	urldate = {2019-10-11},
	date = {2016-11-14},
	langid = {english},
}

@inproceedings{leng_online_2015,
	location = {Boston, {MA}, {USA}},
	title = {Online sketching hashing},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298865/},
	doi = {10.1109/CVPR.2015.7298865},
	abstract = {Recently, hashing based approximate nearest neighbor ({ANN}) search has attracted much attention. Extensive new algorithms have been developed and successfully applied to different applications. However, two critical problems are rarely mentioned. First, in real-world applications, the data often comes in a streaming fashion but most of existing hashing methods are batch based models. Second, when the dataset becomes huge, it is almost impossible to load all the data into memory to train hashing models. In this paper, we propose a novel approach to handle these two problems simultaneously based on the idea of data sketching. A sketch of one dataset preserves its major characters but with signiﬁcantly smaller size. With a small size sketch, our method can learn hash functions in an online fashion, while needs rather low computational complexity and storage space. Extensive experiments on two large scale benchmarks and one synthetic dataset demonstrate the efﬁcacy of the proposed method.},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2503--2511},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Leng, Cong and Wu, Jiaxiang and Cheng, Jian and Bai, Xiao and Lu, Hanqing},
	urldate = {2019-10-11},
	date = {2015-06},
	langid = {english},
}

@software{czxxjtu_czxxjtu/hash-learning.github.io_2019,
	title = {czxxjtu/Hash-Learning.github.io},
	url = {https://github.com/czxxjtu/Hash-Learning.github.io},
	abstract = {Contribute to czxxjtu/Hash-Learning.github.io development by creating an account on {GitHub}.},
	author = {czxxjtu},
	urldate = {2019-10-11},
	date = {2019-10-05},
	note = {original-date: 2015-12-22T11:33:05Z}
}

@inproceedings{ji_min-max_2013,
	title = {Min-Max Hash for Jaccard Similarity},
	doi = {10.1109/ICDM.2013.119},
	abstract = {Min-wise hash is a widely-used hashing method for scalable similarity search in terms of Jaccard similarity, while in practice it is necessary to compute many such hash functions for certain precision, leading to expensive computational cost. In this paper, we introduce an effective method, i.e. the min-max hash method, which significantly reduces the hashing time by half, yet it has a provably slightly smaller variance in estimating pair wise Jaccard similarity. In addition, the estimator of min-max hash only contains pair wise equality checking, thus it is especially suitable for approximate nearest neighbor search. Since min-max hash is equally simple as min-wise hash, many extensions based on min-wise hash can be easily adapted to min-max hash, and we show how to combine it with b-bit minwise hash. Experiments show that with the same length of hash code, min-max hash reduces the hashing time to half as much as that of min-wise hash, while achieving smaller mean squared error ({MSE}) in estimating pair wise Jaccard similarity, and better best approximate ratio ({BAR}) in approximate nearest neighbor search.},
	eventtitle = {2013 {IEEE} 13th International Conference on Data Mining},
	pages = {301--309},
	booktitle = {2013 {IEEE} 13th International Conference on Data Mining},
	author = {Ji, J. and Li, J. and Yan, S. and Tian, Q. and Zhang, B.},
	date = {2013-12},
	keywords = {Approximation algorithms, approximate nearest neighbor search, Approximation methods, b-bit minwise hash, best approximate ratio, Computational efficiency, Computer science, Educational institutions, hash code, hashing method, hashing time reduction, Jaccard similarity, Laboratories, mean square error methods, mean squared error, min-max hash, min-max hash estimator, min-max hash method, min-wise hash, minimax techniques, Nearest neighbor searches, pairwise equality checking, pairwise Jaccard similarity estimation, pattern clustering, scalable similarity search, search problems},
}

@software{ye_yesimon/metax_bakeoff_2019_2019,
	title = {yesimon/metax\_bakeoff\_2019},
	rights = {{MIT}},
	url = {https://github.com/yesimon/metax_bakeoff_2019},
	abstract = {Contribute to yesimon/metax\_bakeoff\_2019 development by creating an account on {GitHub}.},
	author = {Ye, Simon},
	urldate = {2019-10-11},
	date = {2019-09-23},
	note = {original-date: 2019-07-26T05:02:38Z}
}

@misc{wood_kraken_2019,
	title = {Kraken 2 manuscript experiment data},
	url = {https://zenodo.org/record/3365797},
	abstract = {This dataset contains the data used in the strain-exclusion experiments in the Kraken 2 manuscript.},
	publisher = {Zenodo},
	author = {Wood, Derrick},
	urldate = {2019-10-11},
	date = {2019-09-07},
	doi = {10.5281/zenodo.3365797},
	note = {type: dataset},
}

@online{noauthor_derrickwood/kraken2-experiment-code_nodate,
	title = {{DerrickWood}/kraken2-experiment-code},
	url = {https://github.com/DerrickWood/kraken2-experiment-code},
	abstract = {Code backing the results of the Kraken 2 paper. Contribute to {DerrickWood}/kraken2-experiment-code development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2019-10-11},
	langid = {english},
}

@article{wood_improved_2019-1,
	title = {Improved metagenomic analysis with Kraken 2},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/762302v1},
	doi = {10.1101/762302},
	abstract = {{\textless}p{\textgreater}Although Kraken9s k-mer-based approach provides fast taxonomic classification of metagenomic sequence data, its large memory requirements can be limiting for some applications. Kraken 2 improves upon Kraken 1 by reducing memory usage by 85\%, allowing greater amounts of reference genomic data to be used, while maintaining high accuracy and increasing speed five-fold. Kraken 2 also introduces a translated search mode, providing increased sensitivity in viral metagenomics analysis.{\textless}/p{\textgreater}},
	pages = {762302},
	journaltitle = {{bioRxiv}},
	author = {Wood, Derrick E. and Lu, Jennifer and Langmead, Ben},
	urldate = {2019-10-11},
	date = {2019-09-07},
	langid = {english},
}

@online{noauthor_cuckoo_nodate,
	title = {Cuckoo Hashing {\textbar} Programming.Guide},
	url = {https://programming.guide/cuckoo-hashing.html},
	urldate = {2019-10-11},
}

@online{noauthor_focus_nodate,
	title = {Focus on: the Kullback-Leibler divergence},
	url = {http://blog.thegrandlocus.com/2019/06/focus-on-the-kullback-leibler-divergence},
	shorttitle = {Focus on},
	abstract = {Focus on: the Kullback-Leibler divergence {\textbar} Filed under
        Kullback-Leibler divergence, series: focus on.},
	urldate = {2019-10-11},
}

@software{noauthor_hacarus/spm-image_2019,
	title = {hacarus/spm-image},
	url = {https://github.com/hacarus/spm-image},
	abstract = {Sparse modeling and Compressive sensing in Python. Contribute to hacarus/spm-image development by creating an account on {GitHub}.},
	publisher = {Hacarus Inc.},
	urldate = {2019-10-11},
	date = {2019-09-21},
	note = {original-date: 2018-03-13T05:24:28Z},
	keywords = {python, machine-learning-library, sparse-modeling}
}

@software{steinig_esteinig/sketchy_2019,
	title = {esteinig/sketchy},
	rights = {{MIT}},
	url = {https://github.com/esteinig/sketchy},
	abstract = {Real-time lineage hashing and genotyping of bacterial pathogens from uncorrected nanopore reads :closed\_umbrella:},
	author = {Steinig, Eike},
	urldate = {2019-10-11},
	date = {2019-10-08},
	note = {original-date: 2019-03-03T01:01:45Z}
}

@inproceedings{ioffe_improved_2010,
	location = {Sydney, Australia},
	title = {Improved Consistent Sampling, Weighted Minhash and L1 Sketching},
	isbn = {978-1-4244-9131-5},
	url = {http://ieeexplore.ieee.org/document/5693978/},
	doi = {10.1109/ICDM.2010.80},
	abstract = {We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efﬁcient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufﬁciently distinct. We show how to choose the optimal number of bits per hash for sketching, and demonstrate experimental results which agree with the theoretical analysis.},
	eventtitle = {2010 {IEEE} 10th International Conference on Data Mining ({ICDM})},
	pages = {246--255},
	booktitle = {2010 {IEEE} International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Ioffe, Sergey},
	urldate = {2019-10-31},
	date = {2010-12},
	langid = {english},
}

@inproceedings{ioffe_improved_2010-1,
	location = {Sydney, Australia},
	title = {Improved Consistent Sampling, Weighted Minhash and L1 Sketching},
	isbn = {978-1-4244-9131-5},
	url = {http://ieeexplore.ieee.org/document/5693978/},
	doi = {10.1109/ICDM.2010.80},
	abstract = {We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efﬁcient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufﬁciently distinct. We show how to choose the optimal number of bits per hash for sketching, and demonstrate experimental results which agree with the theoretical analysis.},
	eventtitle = {2010 {IEEE} 10th International Conference on Data Mining ({ICDM})},
	pages = {246--255},
	booktitle = {2010 {IEEE} International Conference on Data Mining},
	publisher = {{IEEE}},
	author = {Ioffe, Sergey},
	urldate = {2019-11-07},
	date = {2010-12},
	langid = {english},
}

@article{langmead_genomic_2019,
	title = {Genomic sketching with {HyperLogLog}},
	volume = {8},
	url = {https://f1000research.com/slides/8-1866},
	doi = {10.7490/f1000research.1117605.1},
	abstract = {Read this work by Langmead {BT}, at F1000Research.},
	journaltitle = {F1000Research},
	author = {Langmead, Benjamin T. and Baker, Daniel},
	urldate = {2019-11-08},
	date = {2019-11-07},
}

@article{lapierre_metalign_2020,
	title = {Metalign: Efficient alignment-based metagenomic profiling via containment min hash},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.01.17.910521v1},
	doi = {10.1101/2020.01.17.910521},
	shorttitle = {Metalign},
	abstract = {{\textless}p{\textgreater}Whole-genome shotgun sequencing enables the analysis of microbial communities in unprecedented detail, with major implications in medicine and ecology. Predicting the presence and relative abundances of microbes in a sample, known as "metagenomic profiling", is a critical first step in microbiome analysis. Existing profiling methods have been shown to suffer from poor false positive or false negative rates, while alignment-based approaches are often considered accurate but computationally infeasible. Here we present a novel method, Metalign, that addresses these concerns by performing efficient alignment-based metagenomic profiling. We use a containment min hash approach to reduce the reference database size dramatically before alignment and a method to estimate organism relative abundances in the sample by resolving reads aligned to multiple genomes. We show that Metalign achieves significantly improved results over existing methods on simulated datasets from a large benchmarking study, {CAMI}, and performs well on in vitro mock community data and environmental data from the Tara Oceans project. Metalign is freely available at https://github.com/nlapier2/Metalign, along with the results and plots used in this paper, and a docker image is also available at https://hub.docker.com/repository/docker/nlapier2/metalign.{\textless}/p{\textgreater}},
	pages = {2020.01.17.910521},
	journaltitle = {{bioRxiv}},
	author = {{LaPierre}, Nathan and Alser, Mohammed and Eskin, Eleazar and Koslicki, David and Mangul, Serghei},
	urldate = {2020-01-29},
	date = {2020-01-18},
	langid = {english},
}

@online{noauthor_hadrieng2019_classifiers_benchmark_nodate,
	title = {{HadrienG}/2019\_classifiers\_benchmark},
	url = {https://github.com/HadrienG/2019_classifiers_benchmark},
	abstract = {Benchmarking of metagenomic classifiers. Contribute to {HadrienG}/2019\_classifiers\_benchmark development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2020-02-17},
	langid = {english},
}

@video{noauthor_david_nodate,
	title = {David Koslicki, Multi-resolution metagenomic classification and database reduction strategies.},
	url = {https://www.youtube.com/watch?v=_WK-zZ2YAQw},
	abstract = {Multi-resolution metagenomic classification and database reduction strategies. David Koslicki, Associate Professor, Penn State},
	urldate = {2020-02-18},
	langid = {english},
}

@online{noauthor_empirical_nodate,
	title = {Empirical Evaluation Guidelines},
	url = {http://www.sigplan.org/Resources/EmpiricalEvaluation/},
	urldate = {2020-04-02},
}

@online{noauthor_microbiomedasim_nodate,
	title = {{microbiomeDASim}: Simulating longitudinal... {\textbar} F1000Research},
	url = {https://f1000research.com/articles/8-1769/v2},
	urldate = {2020-06-27},
}

@software{braghetto_marcelbraghetto-simple-triangle_2020,
	title = {{MarcelBraghetto}/a-simple-triangle},
	rights = {{MIT} License         ,                 {MIT} License},
	url = {https://github.com/MarcelBraghetto/a-simple-triangle},
	abstract = {A Simple Triangle},
	author = {Braghetto, Marcel},
	urldate = {2020-07-03},
	date = {2020-07-03},
	note = {original-date: 2019-10-20T07:38:12Z}
}

@article{yin_rabbitmash_nodate,
	title = {{RabbitMash}: Accelerating hash-based genome analysis on modern multi-core architectures},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaa754/5897409},
	doi = {10.1093/bioinformatics/btaa754},
	shorttitle = {{RabbitMash}},
	abstract = {{AbstractMotivation}.  Mash is a popular hash-based genome analysis toolkit with applications to important downstream analyses tasks such as clustering and assemb},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Yin, Zekun and Xu, Xiaoming and Zhang, Jinxiao and Wei, Yanjie and Schmidt, Bertil and Liu, Weiguo},
	urldate = {2020-08-27},
	langid = {english},
}
@article{roberts_reducing_2004,
	title = {Reducing storage requirements for biological sequence comparison},
	volume = {20},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/20/18/3363},
	doi = {10.1093/bioinformatics/bth408},
	abstract = {Motivation: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the ‘seed-and-extend’ approach, in which occurrences of short subsequences called ‘seeds’ are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in {RAM} on a single computer. This significantly slows the matching process.
Results: We present a simple and elegant method in which only a small fraction of seeds, called ‘minimizers’, needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
	pages = {3363--3369},
	number = {18},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R. and Mount, Stephen M. and Yorke, James A.},
	urldate = {2015-11-25},
	date = {2004-12-12},
	langid = {english},
	pmid = {15256412},
}

@article{tsitsiklis_power_2011,
	title = {On the power of (even a little) centralization in distributed processing},
	volume = {39},
	pages = {121--132},
	number = {1},
	journaltitle = {{ACM} {SIGMETRICS} Performance Evaluation Review},
	author = {Tsitsiklis, John N. and Xu, Kuang},
	date = {2011},
}

@article{sczyrba_critical_2017,
	title = {Critical Assessment of Metagenome Interpretation—a benchmark of metagenomics software},
	volume = {14},
	rights = {2017 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4458},
	doi = {10.1038/nmeth.4458},
	abstract = {Methods for assembly, taxonomic profiling and binning are key to interpreting metagenome data, but a lack of consensus about benchmarking complicates performance assessment. The Critical Assessment of Metagenome Interpretation ({CAMI}) challenge has engaged the global developer community to benchmark their programs on highly complex and realistic data sets, generated from ∼700 newly sequenced microorganisms and ∼600 novel viruses and plasmids and representing common experimental setups. Assembly and genome binning programs performed well for species represented by individual genomes but were substantially affected by the presence of related strains. Taxonomic profiling and binning programs were proficient at high taxonomic ranks, with a notable performance decrease below family level. Parameter settings markedly affected performance, underscoring their importance for program reproducibility. The {CAMI} results highlight current challenges but also provide a roadmap for software selection to answer specific research questions.},
	pages = {1063--1071},
	number = {11},
	journaltitle = {Nature Methods},
	author = {Sczyrba, Alexander and Hofmann, Peter and Belmann, Peter and Koslicki, David and Janssen, Stefan and Dröge, Johannes and Gregor, Ivan and Majda, Stephan and Fiedler, Jessika and Dahms, Eik and Bremges, Andreas and Fritz, Adrian and Garrido-Oter, Ruben and Jørgensen, Tue Sparholt and Shapiro, Nicole and Blood, Philip D. and Gurevich, Alexey and Bai, Yang and Turaev, Dmitrij and {DeMaere}, Matthew Z. and Chikhi, Rayan and Nagarajan, Niranjan and Quince, Christopher and Meyer, Fernando and Balvočiūtė, Monika and Hansen, Lars Hestbjerg and Sørensen, Søren J. and Chia, Burton K. H. and Denis, Bertrand and Froula, Jeff L. and Wang, Zhong and Egan, Robert and Don Kang, Dongwan and Cook, Jeffrey J. and Deltel, Charles and Beckstette, Michael and Lemaitre, Claire and Peterlongo, Pierre and Rizk, Guillaume and Lavenier, Dominique and Wu, Yu-Wei and Singer, Steven W. and Jain, Chirag and Strous, Marc and Klingenberg, Heiner and Meinicke, Peter and Barton, Michael D. and Lingner, Thomas and Lin, Hsin-Hung and Liao, Yu-Chieh and Silva, Genivaldo Gueiros Z. and Cuevas, Daniel A. and Edwards, Robert A. and Saha, Surya and Piro, Vitor C. and Renard, Bernhard Y. and Pop, Mihai and Klenk, Hans-Peter and Göker, Markus and Kyrpides, Nikos C. and Woyke, Tanja and Vorholt, Julia A. and Schulze-Lefert, Paul and Rubin, Edward M. and Darling, Aaron E. and Rattei, Thomas and {McHardy}, Alice C.},
	urldate = {2018-02-16},
	date = {2017-11},
	langid = {english},
}

@thesis{rackauckas_simulation_2018,
	title = {Simulation and Control of Biological Stochasticity},
	url = {https://escholarship.org/uc/item/4k41h9gz},
	abstract = {Stochastic models of biochemical interactions elucidate essential properties of the network which are not accessible to deterministic modeling. In this thesis it is described how a network motif, the proportional-reversibility interaction with active intermediate states, gives rise to the ability for the variance of biochemical signals to be controlled without changing the mean, a property designated as mean-independent noise control ({MINC}). This noise control is demonstrated to be essential for macro-scale biological processes via spatial models of the zebrafish hindbrain boundary sharpening. Additionally, the ability to deduce noise origin from the aggregate noise properties is shown. However, these large-scale stochastic models of developmental processes required significant advances in the methodology and tooling for solving stochastic differential equations. Two improvements to stochastic integration methods, an efficient method for time stepping adaptivity on high order stochastic Runge-Kutta methods termed Rejection Sampling with Memory ({RSwM}) and optimal-stability stochastic Runge-Kutta methods, are combined to give over 1000 times speedups on biological models over previously used methodologies. In addition, a new software for solving differential equations in the Julia programming language is detailed. Its unique features for handling complex biological models, along with its high performance (routinely benchmarking as faster than classic C++ and Fortran integrators of similar implementations) and new methods, give rise to an accessible tool for simulation of large-scale stochastic biological models.},
	institution = {{UC} Irvine},
	type = {phdthesis},
	author = {Rackauckas, Christopher Vincent},
	urldate = {2019-05-25},
	date = {2018},
	langid = {english},
}

@article{jain_dissertation_nodate,
	title = {A Dissertation Presented to The Academic Faculty},
	pages = {157},
	journaltitle = {{ALGORITHMS} {AND} {APPLICATIONS}},
	author = {Jain, Chirag},
	langid = {english},
}

@article{howard_distributed_nodate,
	title = {Distributed consensus revised},
	abstract = {We depend upon distributed systems in every aspect of life. Distributed consensus, the ability to reach agreement in the face of failures and asynchrony, is a fundamental and powerful primitive for constructing reliable distributed systems from unreliable components.},
	pages = {151},
	author = {Howard, Heidi},
	langid = {english},
}

@software{srivastava_thesis_2019,
	title = {thesis for {RPE} and defense @ Stony Brook. Contribute to k3yavi/thesis development by creating an account on {GitHub}},
	url = {https://github.com/k3yavi/thesis},
	author = {Srivastava, Avi},
	urldate = {2019-06-11},
	date = {2019-05-08},
	note = {original-date: 2016-07-11T23:15:44Z}
}

@article{aletti_exact_2019,
	title = {Exact confidence interval for generalized Flajolet-Martin algorithms},
	url = {http://arxiv.org/abs/1909.11564},
	abstract = {This paper develop a deep mathematical-statistical approach to analyze a class of Flajolet-Martin algorithms ({FMa}), and provide a exact analytical confidence interval for the number \$F\_0\$ of distinct elements in a stream, based on Chernoff bounds. The class of {FMa} has reached a significant popularity in bigdata stream learning, and the attention of the literature has mainly been based on algorithmic aspects, basically complexity optimality, while the statistical analysis of these class of algorithms has been often faced heuristically. The analysis provided here shows a deep connections with special mathematical functions and with extreme value theory. The latter connection may help in explaining heuristic considerations, while the first opens many numerical issues, faced at the end of the present paper. Finally, {MonteCarlo} simulations are provided to support our analytical choice in this context.},
	journaltitle = {{arXiv}:1909.11564 [cs, math, stat]},
	author = {Aletti, Giacomo},
	urldate = {2019-10-11},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11564},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning, Mathematics - Statistics Theory, Computer Science - Machine Learning, Statistics - Computation},
}

@software{bowe_alexbowe/phd-thesis_2019,
	title = {alexbowe/phd-thesis},
	url = {https://github.com/alexbowe/phd-thesis},
	abstract = {{PhD} Thesis: Succinct de Bruijn Graphs. Contribute to alexbowe/phd-thesis development by creating an account on {GitHub}.},
	author = {Bowe, Alex},
	urldate = {2019-10-11},
	date = {2019-08-27},
	note = {original-date: 2018-08-27T19:07:03Z}
}

@online{phillippy_reason_2019,
	title = {Reason we didn't use a mod sketch for Mash Screen is because it kills your ability to detect small things like viruses, and that was our motivating application},
	url = {https://twitter.com/aphillippy/status/1164273479799074817},
	titleaddon = {@aphillippy},
	type = {Tweet},
	author = {Phillippy, Adam},
	urldate = {2019-10-11},
	date = {2019-08-21},
	langid = {english},
}

@software{neches_ryneches/dissertation_2017,
	title = {ryneches/dissertation},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/ryneches/dissertation},
	abstract = {My Ph.D. dissertation. Contribute to ryneches/dissertation development by creating an account on {GitHub}.},
	author = {Neches, Russell},
	urldate = {2019-11-01},
	date = {2017-02-05},
	note = {original-date: 2016-12-30T04:40:51Z}
}

@article{ondov_mash_2019,
	title = {Mash Screen: high-throughput sequence containment estimation for genome discovery},
	volume = {20},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-019-1841-x},
	doi = {10.1186/s13059-019-1841-x},
	shorttitle = {Mash Screen},
	abstract = {The {MinHash} algorithm has proven effective for rapidly estimating the resemblance of two genomes or metagenomes. However, this method cannot reliably estimate the containment of a genome within a metagenome. Here, we describe an online algorithm capable of measuring the containment of genomes and proteomes within either assembled or unassembled sequencing read sets. We describe several use cases, including contamination screening and retrospective analysis of metagenomes for novel genome discovery. Using this tool, we provide containment estimates for every {NCBI} {RefSeq} genome within every {SRA} metagenome and demonstrate the identification of a novel polyomavirus species from a public metagenome.},
	pages = {232},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Ondov, Brian D. and Starrett, Gabriel J. and Sappington, Anna and Kostic, Aleksandra and Koren, Sergey and Buck, Christopher B. and Phillippy, Adam M.},
	urldate = {2019-12-06},
	date = {2019-11-05},
}

@online{eghbal_reimagining_2019,
	title = {Reimagining the {PhD}},
	url = {https://nadiaeghbal.com/phd},
	abstract = {I recently decided to wrap up my time at Protocol Labs, and along with it, my time in open source research.},
	titleaddon = {Nadia Eghbal},
	author = {Eghbal, Nadia},
	urldate = {2019-12-06},
	date = {2019-11-15},
}

@article{marchet_data_2019,
	title = {Data structures based on k-mers for querying large collections of sequencing datasets},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/866756v1},
	doi = {10.1101/866756},
	abstract = {{\textless}p{\textgreater}High-throughput sequencing datasets are usually deposited in public repositories, e.g. the European Nucleotide Archive, to ensure reproducibility. As the amount of data has reached petabyte scale, repositories do not allow to perform online sequence searches; yet such a feature would be highly useful to investigators. Towards this goal, in the last few years several computational approaches have been introduced to index and query large collections of datasets. Here we propose an accessible survey of these approaches, which are generally based on representing datasets as sets of k-mers. We review their properties, introduce a classification, and present their general intuition. We summarize their performance and highlight their current strengths and limitations.{\textless}/p{\textgreater}},
	pages = {866756},
	journaltitle = {{bioRxiv}},
	author = {Marchet, Camille and Boucher, Christina and Puglisi, Simon J. and Medvedev, Paul and Salson, Mikael and Chikhi, Rayan},
	urldate = {2019-12-07},
	date = {2019-12-06},
	langid = {english},
}

@software{srivastava_k3yavithesis_2019,
	title = {k3yavi/thesis},
	url = {https://github.com/k3yavi/thesis},
	abstract = {thesis for {RPE} and defense @ Stony Brook. Contribute to k3yavi/thesis development by creating an account on {GitHub}.},
	author = {Srivastava, Avi},
	urldate = {2020-01-06},
	date = {2019-12-02},
	note = {original-date: 2016-07-11T23:15:44Z},
	keywords = {dissertation, prelim, rpe, stony-brook, thesis}
}

@software{marijon_natirthesis_2019,
	title = {natir/thesis},
	url = {https://github.com/natir/thesis},
	abstract = {Contribute to natir/thesis development by creating an account on {GitHub}.},
	author = {Marijon, Pierre},
	urldate = {2020-01-06},
	date = {2019-11-28},
	note = {original-date: 2019-04-15T18:26:53Z}
}

@online{noauthor_local-first_nodate,
	title = {Local-first software: You own your data, in spite of the cloud},
	url = {https://www.inkandswitch.com/local-first.html},
	shorttitle = {Local-first software},
	abstract = {A new generation of collaborative software that allows users to retain ownership of their data.},
	urldate = {2020-01-29},
	langid = {american},
}

@software{noauthor_redecentralizealternative-internet_2020,
	title = {redecentralize/alternative-internet},
	url = {https://github.com/redecentralize/alternative-internet},
	abstract = {A collection of interesting new networks and tech aiming at decentralisation (in some form).},
	publisher = {Redecentralize.org},
	urldate = {2020-01-29},
	date = {2020-01-29},
	note = {original-date: 2013-07-06T15:38:51Z},
	keywords = {decentralization, decentralized-applications}
}

@online{noauthor_easily_nodate,
	title = {Easily Add Offline-First to Any Application},
	url = {https://replicache.dev/},
	urldate = {2020-01-29},
}

@thesis{pereira_whirlpool_2019,
	title = {Whirlpool: A microservice style scalable continuous topical web crawler},
	url = {http://dspace.calstate.edu/handle/10211.3/214919},
	shorttitle = {Whirlpool},
	abstract = {Historically, web crawlers/bots/spiders have been well known for indexing, ranking websites on the internet. This thesis augments the crawling activity but approaches the problem through the lens of a data engineer. Whirlpool as a continuous, topical web crawling tool is also a data ingestion pipeline implemented from bottom-up using {RabbitMQ} which is a high performance messaging buffer to organize the data flow within its network. It is based on a open, standard blueprint design of mercator. This paper discusses the high and low level design of this complex program covering auxiliary data structures, object-oriented design, addressing scalability concerns, and deployment on {AWS}. The project name Whirlpool is used as an analogy referring to the naturally occurring phenomenon where opposing water currents in sea cause water to spin round and round drawing various objects into it.},
	institution = {California State University Channel Islands},
	type = {Thesis},
	author = {Pereira, Rihan Stephen},
	urldate = {2020-02-06},
	date = {2019-12},
	langid = {american},
}

@article{chen_imgm_2019,
	title = {{IMG}/M v.5.0: an integrated data management and comparative analysis system for microbial genomes and microbiomes},
	volume = {47},
	issn = {1362-4962},
	doi = {10.1093/nar/gky901},
	shorttitle = {{IMG}/M v.5.0},
	abstract = {The Integrated Microbial Genomes \& Microbiomes system v.5.0 ({IMG}/M: https://img.jgi.doe.gov/m/) contains annotated datasets categorized into: archaea, bacteria, eukarya, plasmids, viruses, genome fragments, metagenomes, cell enrichments, single particle sorts, and metatranscriptomes. Source datasets include those generated by the {DOE}'s Joint Genome Institute ({JGI}), submitted by external scientists, or collected from public sequence data archives such as {NCBI}. All submissions are typically processed through the {IMG} annotation pipeline and then loaded into the {IMG} data warehouse. {IMG}'s web user interface provides a variety of analytical and visualization tools for comparative analysis of isolate genomes and metagenomes in {IMG}. {IMG}/M allows open access to all public genomes in the {IMG} data warehouse, while its expert review ({ER}) system ({IMG}/{MER}: https://img.jgi.doe.gov/mer/) allows registered users to access their private genomes and to store their private datasets in workspace for sharing and for further analysis. {IMG}/M data content has grown by 60\% since the last report published in the 2017 {NAR} Database Issue. {IMG}/M v.5.0 has a new and more powerful genome search feature, new statistical tools, and supports metagenome binning.},
	pages = {D666--D677},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res.},
	author = {Chen, I.-Min A. and Chu, Ken and Palaniappan, Krishna and Pillay, Manoj and Ratner, Anna and Huang, Jinghua and Huntemann, Marcel and Varghese, Neha and White, James R. and Seshadri, Rekha and Smirnova, Tatyana and Kirton, Edward and Jungbluth, Sean P. and Woyke, Tanja and Eloe-Fadrosh, Emiley A. and Ivanova, Natalia N. and Kyrpides, Nikos C.},
	date = {2019-01-08},
	pmid = {30289528},
	pmcid = {PMC6323987},
}

@article{leinonen_sequence_2011,
	title = {The Sequence Read Archive},
	volume = {39},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/39/suppl_1/D19/2505848},
	doi = {10.1093/nar/gkq1019},
	abstract = {Abstract.   The combination of significantly lower cost and increased speed of sequencing has resulted in an explosive growth of data submitted into the primary},
	pages = {D19--D21},
	issue = {suppl\_1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Leinonen, Rasko and Sugawara, Hideaki and Shumway, Martin},
	urldate = {2020-02-14},
	date = {2011-01-01},
	langid = {english},
}

@article{kodama_sequence_2012,
	title = {The sequence read archive: explosive growth of sequencing data},
	volume = {40},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/40/D1/D54/2903448},
	doi = {10.1093/nar/gkr854},
	shorttitle = {The sequence read archive},
	abstract = {Abstract.  New generation sequencing platforms are producing data with significantly higher throughput and lower cost. A portion of this capacity is devoted to},
	pages = {D54--D56},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Kodama, Yuichi and Shumway, Martin and Leinonen, Rasko},
	urldate = {2020-02-14},
	date = {2012-01-01},
	langid = {english},
}

@article{leinonen_european_2011,
	title = {The European Nucleotide Archive},
	volume = {39},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/39/suppl_1/D28/2508963},
	doi = {10.1093/nar/gkq967},
	abstract = {Abstract.   The European Nucleotide Archive ({ENA}; http://www.ebi.ac.uk/ena ) is Europe’s primary nucleotide-sequence repository. The {ENA} consists of three main},
	pages = {D28--D31},
	issue = {suppl\_1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Leinonen, Rasko and Akhtar, Ruth and Birney, Ewan and Bower, Lawrence and Cerdeno-Tárraga, Ana and Cheng, Ying and Cleland, Iain and Faruque, Nadeem and Goodgame, Neil and Gibson, Richard and Hoad, Gemma and Jang, Mikyung and Pakseresht, Nima and Plaister, Sheila and Radhakrishnan, Rajesh and Reddy, Kethi and Sobhany, Siamak and Ten Hoopen, Petra and Vaughan, Robert and Zalunin, Vadim and Cochrane, Guy},
	urldate = {2020-02-14},
	date = {2011-01-01},
	langid = {english},
}

@article{kaminuma_ddbj_2010,
	title = {{DDBJ} launches a new archive database with analytical tools for next-generation sequence data},
	volume = {38},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/38/suppl_1/D33/3112217},
	doi = {10.1093/nar/gkp847},
	abstract = {{ABSTRACT}.  The {DNA} Data Bank of Japan ({DDBJ}) (http://www.ddbj.nig.ac.jp) has collected and released 1 701 110 entries/1 116 138 614 bases between July 2008 and},
	pages = {D33--D38},
	issue = {suppl\_1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Kaminuma, Eli and Mashima, Jun and Kodama, Yuichi and Gojobori, Takashi and Ogasawara, Osamu and Okubo, Kousaku and Takagi, Toshihisa and Nakamura, Yasukazu},
	urldate = {2020-02-14},
	date = {2010-01-01},
	langid = {english},
}

@article{wilks_snaptron_2018,
	title = {Snaptron: querying splicing patterns across tens of thousands of {RNA}-seq samples},
	volume = {34},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/34/1/114/4101942},
	doi = {10.1093/bioinformatics/btx547},
	shorttitle = {Snaptron},
	abstract = {{AbstractMotivation}.  As more and larger genomics studies appear, there is a growing need for comprehensive and queryable cross-study summaries. These enable res},
	pages = {114--116},
	number = {1},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Wilks, Christopher and Gaddipati, Phani and Nellore, Abhinav and Langmead, Ben},
	urldate = {2020-02-14},
	date = {2018-01-01},
	langid = {english},
}

@article{piro_dudes_2016,
	title = {{DUDes}: a top-down taxonomic profiler for metagenomics},
	volume = {32},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/32/15/2272/1743640},
	doi = {10.1093/bioinformatics/btw150},
	shorttitle = {{DUDes}},
	abstract = {Abstract.  Motivation: Species identification and quantification are common tasks in metagenomics and pathogen detection studies. The most recent techniques are},
	pages = {2272--2280},
	number = {15},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Piro, Vitor C. and Lindner, Martin S. and Renard, Bernhard Y.},
	urldate = {2020-03-03},
	date = {2016-08-01},
	langid = {english},
}

@article{milanese_microbial_2019,
	title = {Microbial abundance, activity and population genomic profiling with {mOTUs}2},
	volume = {10},
	rights = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-08844-4},
	doi = {10.1038/s41467-019-08844-4},
	abstract = {Metagenomic analysis based on universal phylogenetic marker gene ({MG})-based operational taxonomic units ({mOTUs}) is a useful strategy, especially for microbial species without reference genomes. Here, the authors develop {mOTUs}2, an updated and functionally extended profiling tool for microbial abundance, activity and population profiling.},
	pages = {1--11},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Milanese, Alessio and Mende, Daniel R. and Paoli, Lucas and Salazar, Guillem and Ruscheweyh, Hans-Joachim and Cuenca, Miguelangel and Hingamp, Pascal and Alves, Renato and Costea, Paul I. and Coelho, Luis Pedro and Schmidt, Thomas S. B. and Almeida, Alexandre and Mitchell, Alex L. and Finn, Robert D. and Huerta-Cepas, Jaime and Bork, Peer and Zeller, Georg and Sunagawa, Shinichi},
	urldate = {2020-03-03},
	date = {2019-03-04},
	langid = {english},
}

@online{noauthor_studying_nodate,
	title = {Studying Microbial Diversity},
	url = {http://readiab.org/book/latest/3/1#4.1.1},
	urldate = {2020-03-03},
}

@article{loeffler_improving_2020,
	title = {Improving the usability and comprehensiveness of microbial databases},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/497867v5},
	doi = {10.1101/497867},
	abstract = {{\textless}p{\textgreater}Metagenomics studies leverage genomic reference databases to generate discoveries in basic science and translational research. However, current microbial studies use disparate reference databases that lack consistent standards of specimen inclusion, data preparation, taxon labelling and accessibility, hindering their quality and comprehensiveness, and calling for the establishment of recommendations for reference genome database assembly. Here, we analyze existing fungal and bacterial databases and discuss guidelines for the development of a master reference database that promises to improve the quality and quantity of omics research.{\textless}/p{\textgreater}},
	pages = {497867},
	journaltitle = {{bioRxiv}},
	author = {Loeffler, Caitlin and Karlsberg, Aaron and Martin, Lana S. and Eskin, Eleazar and Koslicki, David and Mangul, Serghei},
	urldate = {2020-03-13},
	date = {2020-02-16},
	langid = {english},
}

@article{ziviani_compression_2000,
	title = {Compression: a key for next-generation text retrieval systems},
	volume = {33},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/881693/},
	doi = {10.1109/2.881693},
	shorttitle = {Compression},
	abstract = {In this article we discuss recent methods for compressing the text and the index of text retrieval systems. By compressing both the complete text and the index, the total amount of space is less than half the size of the original text alone. Most surprisingly, the time required to build the index and also to answer a query is much less than if the index and text had not been compressed. This is one of the few cases where there is no space-time trade-o . Moreover, the text can be kept compressed all the time, allowing updates when changes occur in the compressed text.},
	pages = {37--44},
	number = {11},
	journaltitle = {Computer},
	shortjournal = {Computer},
	author = {Ziviani, N. and Silva de Moura, E. and Navarro, G. and Baeza-Yates, R.},
	urldate = {2020-03-18},
	date = {2000-11},
	langid = {english},
}

@software{alienzj_alienzjgtdb2ncbi_2020,
	title = {alienzj/gtdb2ncbi},
	url = {https://github.com/alienzj/gtdb2ncbi},
	abstract = {{GTDB} ={\textgreater} {NCBI}. Contribute to alienzj/gtdb2ncbi development by creating an account on {GitHub}.},
	author = {alienzj},
	urldate = {2020-03-19},
	date = {2020-03-15},
	note = {original-date: 2020-02-16T12:16:02Z}
}

@online{noauthor_irmin_nodate,
	title = {Irmin Tutorial},
	url = {https://irmin.org/tutorial/introduction},
	urldate = {2020-03-23},
	note = {Library Catalog: irmin.org},
}

@online{noauthor_attic-labsnoms_nodate,
	title = {attic-labs/noms},
	url = {https://github.com/attic-labs/noms/blob/master/doc/intro.md#prolly-trees-probabilistic-b-trees},
	abstract = {The versioned, forkable, syncable database. Contribute to attic-labs/noms development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2020-03-23},
	langid = {english},
	note = {Library Catalog: github.com},
}

@article{matthews_weighted_1967,
	title = {Weighted Term Search: A Computer Program for an Inverted Coordinate Index on Magnetic Tape},
	volume = {7},
	issn = {0021-9576, 1541-5732},
	url = {https://pubs.acs.org/doi/abs/10.1021/c160024a015},
	doi = {10.1021/c160024a015},
	shorttitle = {Weighted Term Search},
	pages = {49--56},
	number = {1},
	journaltitle = {Journal of Chemical Documentation},
	shortjournal = {J. Chem. Doc.},
	author = {Matthews, F. W. and Thomson, L.},
	urldate = {2020-03-28},
	date = {1967-02},
	langid = {english},
}

@article{zobel_inverted_2006,
	title = {Inverted files for text search engines},
	volume = {38},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=1132956.1132959},
	doi = {10.1145/1132956.1132959},
	pages = {6--es},
	number = {2},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Zobel, Justin and Moffat, Alistair},
	urldate = {2020-03-28},
	date = {2006-07-25},
	langid = {english},
}

@article{cutting_optimizations_nodate,
	title = {Optimizations for Dynamic Inverted Index Maintenance},
	abstract = {For free-text search over rapidly evolving corpora, dynamic update of inverted indices is a basic requirement. B-trees are an e ective tool in implementing such indices. The Zip an distribution of postings suggests space and time optimizations unique to this task. In particular, we present two novel optimizations, merge update, which performs better than straight forward block update, and pulsing which signi cantly reduces space requirements without sacri cing performance.},
	pages = {7},
	author = {Cutting, Doug and Pedersen, Jan},
	langid = {english},
}

@article{valiant_estimating_2017,
	title = {Estimating the Unseen: Improved Estimators for Entropy and Other Properties},
	volume = {64},
	issn = {00045411},
	url = {http://dl.acm.org/citation.cfm?doid=3145801.3125643},
	doi = {10.1145/3125643},
	shorttitle = {Estimating the Unseen},
	abstract = {Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems.},
	pages = {1--41},
	number = {6},
	journaltitle = {Journal of the {ACM}},
	shortjournal = {J. {ACM}},
	author = {Valiant, Gregory and Valiant, Paul},
	urldate = {2020-03-30},
	date = {2017-10-04},
	langid = {english},
}

@article{navarro_indexing_2020,
	title = {Indexing Highly Repetitive String Collections},
	url = {http://arxiv.org/abs/2004.02781},
	abstract = {Two decades ago, a breakthrough in indexing string collections made it possible to represent them within their compressed space while at the same time offering indexed search functionalities. As this new technology permeated through applications like bioinformatics, the string collections experienced a growth that outperforms Moore's Law and challenges our ability of handling them even in compressed form. It turns out, fortunately, that many of these rapidly growing string collections are highly repetitive, so that their information content is orders of magnitude lower than their plain size. The statistical compression methods used for classical collections, however, are blind to this repetitiveness, and therefore a new set of techniques has been developed in order to properly exploit it. The resulting indexes form a new generation of data structures able to handle the huge repetitive string collections that we are facing. In this survey we cover the algorithmic developments that have led to these data structures. We describe the distinct compression paradigms that have been used to exploit repetitiveness, the fundamental algorithmic ideas that form the base of all the existing indexes, and the various structures that have been proposed, comparing them both in theoretical and practical aspects. We conclude with the current challenges in this fascinating field.},
	journaltitle = {{arXiv}:2004.02781 [cs]},
	author = {Navarro, Gonzalo},
	urldate = {2020-04-15},
	date = {2020-04-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.02781},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@software{botvinnik_olgabotdissertation_2020,
	title = {olgabot/dissertation},
	url = {https://github.com/olgabot/dissertation},
	abstract = {Computational analysis of single-cell alternative splicing: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Bioinformatics and Systems Bi...},
	author = {Botvinnik, Olga},
	urldate = {2020-04-20},
	date = {2020-02-20},
	note = {original-date: 2017-03-10T00:47:23Z}
}

@book{brennan_integrating_2017,
	title = {Integrating physiology and genomics to identify mechanisms of adaptation in killifish},
	isbn = {978-0-355-45133-7},
	url = {https://search.proquest.com/docview/1970032855?accountid=14505},
	abstract = {Salinity is one of the main environmental factors limiting niche boundaries of aquatic species and evolutionary divergence across osmotic niches contributes to generating phyletic diversity within the fishes. While most clades of fish are limited to only one salinity, some groups, such as killifish in the genus Fundulus, have repeatedly diverged from marine to fresh water. Within the euryhaline species F. heteroclitus, genetically distinct, locally adapted populations inhabit salinities ranging from marine to fresh waters, perhaps representing the early stages of speciation across an osmotic boundary. This dissertation seeks to understand the mechanisms enabling evolutionary transitions between osmotic niches in order to illuminate the processes that contribute to adaptive divergence across important ecological boundaries. For F. heteroclitus populations native to both fresh and saline niches, I have integrated and quantified the physiological, transcriptomic, and genomic mechanisms underlying multiple physiological phenotypes to identify the mechanisms contributing to their evolutionary divergence. In chapter 1, I quantify shifts in physiological plasticity of a derived freshwater phenotype and the fate of an ancestral euryhaline phenotype after invasion of a freshwater environment. I use transcriptomic and physiological approaches to identify the mechanistic basis of these shifts. I compared physiological and transcriptomic responses to high and low salinity stress in fresh ({FW}-native) and brackish water ({BW}-native) populations and found an enhanced plasticity to low salinity in the freshwater population coupled with a reduced ability to acclimate to high salinity. Transcriptomic data identified genes with a conserved common response, a conserved salinity dependent response, and responses associated with population divergence. Conserved common acclimation responses revealed stress responses and alterations in cell-cycle regulation as important mechanisms in the general osmotic response. Salinity-specific responses included the regulation of genes involved in ion transport, intracellular calcium, energetic processes, and cellular remodeling. Genes diverged between populations were primarily those showing salinity-specific expression and included those regulating polyamine homeostasis and cell cycle. Additionally, when populations were matched with their native salinity, expression patterns were consistent with the concept of "transcriptomic resilience," suggesting local adaptation. These findings provide insight into the fate of a plastic phenotype after a shift in environmental salinity and help to reveal mechanisms allowing for euryhalinity. Chapter 2 addresses divergence in whole organism osmotic performance. Regulation of internal ion homeostasis is essential for fishes inhabiting environments where salinities differ from their internal concentrations. It is hypothesized that selection will act to reduce energetic costs of osmoregulation in a population's native osmotic habitat, producing patterns of local adaptation. I investigated evidence for local adaptation between the {FW}-native and {BW}-native populations. Aerobic scope (the difference between minimum and maximum metabolic rates), excess post-exercise oxygen consumption, and swimming performance (time and distance to reach exhaustion) were used as proxies for fitness in fresh and brackish water treatments. Swimming performance results supported local adaptation; {BW}-native fish performed significantly better than {FW}-native fish at high salinity while low salinity performance was similar between populations. However, results from metabolic measures did not support this conclusion; both populations showed an increase in resting metabolic rate and a decrease of aerobic scope in fresh water. Similarly, excess post-exercise oxygen consumption was higher for both populations in fresh than in brackish water. While swimming results suggest environmentally dependent performance differences may be a result of selection in divergent osmotic environments, the differences between populations are not coupled with divergence in metabolic performance. The final chapter focuses on identifying the genetic basis of adaptive traits. Detecting loci underlying adaptive phenotypes is a major challenge in evolutionary biology. Typical approaches include genome scans for selection, though these can be difficult to interpret because of incomplete knowledge of the connection between genotype and phenotype. Genome wide association mapping studies are powerful for linking genotype to phenotype, but offer no insight into the evolutionary forces (e.g., natural selection) shaping variation in those genotypes and phenotypes. I attempt to overcome these challenges by merging genome scans for selection and genome wide association studies ({GWAS}). By combining these approaches, I identify loci that are under divergent selection and directly involved in phenotypes that diverge between {FW} and {BW} populations of F. heteroclitus. I focused on the {FW}-native and {BW}-native populations as well as a population that is a natural admixture of those two parental populations. All individuals were phenotyped for multiple physiological traits that differ between populations, and that may contribute to adaptation across osmotic niches: salinity tolerance, hypoxia tolerance, and metabolic rate. All individuals were also genotyped using {RADseq}. I performed selection scans between {BW}-native and {FW}-native populations to identify genomic regions under selection. {GWAS} was conducted in all individuals, including all admixed individuals, to identify loci underlying specific phenotypes. Populations diverge in their physiological capabilities in patterns consistent with local adaptation. Selection is disproportionately targeting loci that influence metabolic rate and salinity tolerance, suggesting that these phenotypes are undergoing adaptive divergence and implicating candidate genes likely involved in physiological capabilities. However, loci associated with hypoxia tolerance do not appear to be evolving by natural selection across osmotic niches. This approach provides a framework to leverage diverse approaches to identify signals of local adaptation in natural populations., Advisor: Andrew Whitehead., Degree granted in Ecology., Ph. D. University of California, Davis 2017., Description based on online resource; title from {PDF} title page ({ProQuest}, viewed 04/10/2018).},
	publisher = {University of California, Davis},
	author = {Brennan, Reid Simon},
	urldate = {2020-04-20},
	date = {2017},
	keywords = {Ecology, Dissertations, Academic, University of California, Davis}
}

@online{noauthor_ipfs_2020,
	title = {{IPFS} Implementation for publication of decentralized peer review},
	url = {https://discuss.ipfs.io/t/ipfs-implementation-for-publication-of-decentralized-peer-review/7692},
	abstract = {Hi All, I wanted to share that I created an application and api for the “decentralized” peer review of scholarly editions, which you can view here: https://dll-review-registry.digitallatin.org/  and I published a little article about it and my use of {IPFS} at http://digitalhumanities.org/dhq/vol/13/4/000438/000438.html.  I would be interested to hear any one’s thoughts, comments, questions, and (constructive and positively articulated) criticism.  There are probably lots of things that can be imp...},
	titleaddon = {discuss.ipfs.io},
	urldate = {2020-05-12},
	date = {2020-04-10},
	langid = {american},
	note = {Library Catalog: discuss.ipfs.io},
}

@article{schneider_decentralization_2019,
	title = {Decentralization: an incomplete ambition},
	volume = {12},
	issn = {1753-0350},
	url = {https://doi.org/10.1080/17530350.2019.1589553},
	doi = {10.1080/17530350.2019.1589553},
	shorttitle = {Decentralization},
	abstract = {Decentralization is a term widely used in a variety of contexts, particularly in political science and discourses surrounding the Internet. It is popular today among advocates of blockchain technology. While frequently employed as if it were a technical term, decentralization more reliably appears to operate as a rhetorical strategy that directs attention toward some aspects of a proposed social order and away from others. It is called for far more than it is theorized or consistently defined. This non-specificity has served to draw diverse participants into common political and technological projects. Yet even the most apparently decentralized systems have shown the capacity to produce economically and structurally centralized outcomes. The rhetoric of decentralization thus obscures other aspects of the re-ordering it claims to describe. It steers attention from where concentrations of power are operating, deferring worthwhile debate about how such power should operate. For decentralization to be a reliable concept in formulating future social arrangements and related technologies, it should come with high standards of specificity. It also cannot substitute for anticipating centralization with appropriate mechanisms of accountability.},
	pages = {265--285},
	number = {4},
	journaltitle = {Journal of Cultural Economy},
	author = {Schneider, Nathan},
	urldate = {2020-05-13},
	date = {2019-07-04},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/17530350.2019.1589553},
	keywords = {internet, decentralization, bitcoin, Cryptocurrency, development},
}

@online{noauthor_privacy_nodate,
	title = {On Privacy versus Freedom},
	url = {https://matrix.org/blog/2020/01/02/on-privacy-versus-freedom},
	titleaddon = {Matrix.org},
	urldate = {2020-05-14},
	langid = {english},
	note = {Library Catalog: matrix.org},
}

@article{manning_introduction_2009,
	title = {Introduction to Information Retrieval},
	pages = {581},
	author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
	date = {2009},
	langid = {english},
}

@incollection{goos_pastry_2001,
	location = {Berlin, Heidelberg},
	title = {Pastry: Scalable, Decentralized Object Location, and Routing for Large-Scale Peer-to-Peer Systems},
	volume = {2218},
	isbn = {978-3-540-42800-8 978-3-540-45518-9},
	url = {http://link.springer.com/10.1007/3-540-45518-3_18},
	shorttitle = {Pastry},
	abstract = {This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing substrate for wide-area peer-to-peer applications. Pastry performs application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a variety of peer-to-peer applications, including global data storage, data sharing, group communication and naming.},
	pages = {329--350},
	booktitle = {Middleware 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Rowstron, Antony and Druschel, Peter},
	editor = {Guerraoui, Rachid},
	editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2020-05-15},
	date = {2001},
	langid = {english},
	doi = {10.1007/3-540-45518-3_18},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{zhao_tapestry_2004,
	title = {Tapestry: A Resilient Global-Scale Overlay for Service Deployment},
	volume = {22},
	issn = {0733-8716},
	url = {http://ieeexplore.ieee.org/document/1258114/},
	doi = {10.1109/JSAC.2003.818784},
	shorttitle = {Tapestry},
	abstract = {We present Tapestry, a peer-to-peer overlay routing infrastructure offering efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using only localized resources. Tapestry supports a generic decentralized object location and routing applications programming interface using a self-repairing, soft-state-based routing layer. This paper presents the Tapestry architecture, algorithms, and implementation. It explores the behavior of a Tapestry deployment on {PlanetLab}, a global testbed of approximately 100 machines. Experimental results show that Tapestry exhibits stable behavior and performance as an overlay, despite the instability of the underlying network layers. Several widely distributed applications have been implemented on Tapestry, illustrating its utility as a deployment infrastructure.},
	pages = {41--53},
	number = {1},
	journaltitle = {{IEEE} Journal on Selected Areas in Communications},
	shortjournal = {{IEEE} J. Select. Areas Commun.},
	author = {Zhao, B.Y. and Huang, L. and Stribling, J. and Rhea, S.C. and Joseph, A.D. and Kubiatowicz, J.D.},
	urldate = {2020-05-15},
	date = {2004-01},
	langid = {english},
}

@article{stoica_chord_2001,
	title = {Chord: A scalable peer-to-peer lookup service for internet applications},
	volume = {31},
	issn = {0146-4833},
	url = {https://doi.org/10.1145/964723.383071},
	doi = {10.1145/964723.383071},
	shorttitle = {Chord},
	abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
	pages = {149--160},
	number = {4},
	journaltitle = {{ACM} {SIGCOMM} Computer Communication Review},
	shortjournal = {{SIGCOMM} Comput. Commun. Rev.},
	author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
	urldate = {2020-05-15},
	date = {2001-08-27},
}

@inproceedings{ratnasamy_scalable_2001,
	location = {San Diego, California, {USA}},
	title = {A scalable content-addressable network},
	isbn = {978-1-58113-411-7},
	url = {https://doi.org/10.1145/383059.383072},
	doi = {10.1145/383059.383072},
	series = {{SIGCOMM} '01},
	abstract = {Hash tables - which map "keys" onto "values" - are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network ({CAN}) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The {CAN} is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation.},
	pages = {161--172},
	booktitle = {Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
	publisher = {Association for Computing Machinery},
	author = {Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
	urldate = {2020-05-15},
	date = {2001-08-27},
}

@article{madduri_reproducible_2018,
	title = {Reproducible big data science: A case study in continuous {FAIRness}},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/268755v1},
	doi = {10.1101/268755},
	shorttitle = {Reproducible big data science},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Big biomedical data create exciting opportunities for discovery, but make it difficult to capture analyses and outputs in forms that are findable, accessible, interoperable, and reusable ({FAIR}). In response, we describe tools that make it easy to capture, and assign identifiers to, data and code throughout the data lifecycle. We illustrate the use of these tools via a case study involving a multi-step analysis that creates an atlas of putative transcription factor binding sites from terabytes of {ENCODE} {DNase} I hypersensitive sites sequencing data. We show how the tools automate routine but complex tasks, capture analysis algorithms in understandable and reusable forms, and harness fast networks and powerful cloud computers to process data rapidly, all without sacrificing usability or reproducibility—thus ensuring that big data are not hard-to-(re)use data. We compare and contrast our approach with other approaches to big data analysis and reproducibility.{\textless}/p{\textgreater}},
	pages = {268755},
	journaltitle = {{bioRxiv}},
	author = {Madduri, Ravi and Chard, Kyle and D’Arcy, Mike and Jung, Segun C. and Rodriguez, Alexis and Sulakhe, Dinanath and Deutsch, Eric W. and Funk, Cory and Heavner, Ben and Richards, Matthew and Shannon, Paul and Dinov, Ivo and Glusman, Gustavo and Price, Nathan and Horn, John D. Van and Kesselman, Carl and Toga, Arthur W. and Foster, Ian},
	urldate = {2020-05-16},
	date = {2018-02-27},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@unpublished{chard_ill_2016,
	title = {I'll take that to go: Big data bags and minimal identifiers for exchange of large, complex datasets},
	url = {https://zenodo.org/record/820878},
	shorttitle = {I'll take that to go},
	abstract = {Big data workflows often require the assembly and exchange of complex, multi-element datasets. For example, in biomedical applications, the input to an analytic pipeline can be a dataset consisting thousands of images and genome sequences assembled from diverse repositories, requiring a description of the contents of the dataset in a concise and unambiguous form. Typical approaches to creating datasets for big data workflows assume that all data reside in a single location, requiring costly data marshaling and permitting errors of omission and commission because dataset members are not explicitly specified. We address these issues by proposing simple methods and tools for assembling, sharing, and analyzing large and complex datasets that scientists can easily integrate into their daily workflows. These tools combine a simple and robust method for describing data collections ({BDBags}), data descriptions (Research Objects), and simple persistent identifiers (Minids) to create a powerful ecosystem of tools and services for big data analysis and sharing. We present these tools and use biomedical case studies to illustrate their use for the rapid assembly, sharing, and analysis of large datasets.},
	author = {Chard, Kyle and D'Arcy, Mike and Heavner, Ben and Foster, Ian and Kesselman, Carl and Madduri, Ravi and Rodriguez, Alexis and Soiland-Reyes, Stian and Goble, Carole and Clark, Kristi and Deutsch, Eric W. and Dinov, Ivo and Price, Nathan and Toga, Arthur},
	urldate = {2020-05-16},
	date = {2016-12-05},
	doi = {10.1109/BigData.2016.7840618},
	note = {{ISBN}: 9781467390057
Library Catalog: Zenodo
Pages: 319-328
Publication Title: 2016 {IEEE} International Conference on Big Data (Big Data)
Publisher: {IEEE}},
	keywords = {Software, Metadata, data analysis, Big Data, bdbag, {BDBags}, Big Data analysis, Big Data bags, Big Data sharing, data assembling, data collections, data descriptions, datasets, Encoding, identifiers, Minid, Payloads, research objects, Robustness, Uniform resource locators},
}

@article{koslicki_improving_2019,
	title = {Improving {MinHash} via the containment index with applications to metagenomic analysis},
	volume = {354},
	issn = {0096-3003},
	url = {http://www.sciencedirect.com/science/article/pii/S009630031930116X},
	doi = {10.1016/j.amc.2019.02.018},
	abstract = {{MinHash} is a probabilistic method for estimating the similarity of two sets in terms of their Jaccard index, defined as the ratio of the size of their intersection to their union. We demonstrate that this method performs best when the sets under consideration are of similar size and the performance degrades considerably when the sets are of very different size. We introduce a new and efficient approach, called the containment {MinHash} approach, that is more suitable for estimating the Jaccard index of sets of very different size. We accomplish this by leveraging another probabilistic method (in particular, Bloom filters) for fast membership queries. We derive bounds on the probability of estimate errors for the containment {MinHash} approach and show it significantly improves upon the classical {MinHash} approach. We also show significant improvements in terms of time and space complexity. As an application, we use this method to detect the presence/absence of organisms in a metagenomic data set, showing that it can detect the presence of very small, low abundance microorganisms.},
	pages = {206--215},
	journaltitle = {Applied Mathematics and Computation},
	shortjournal = {Applied Mathematics and Computation},
	author = {Koslicki, David and Zabeti, Hooman},
	urldate = {2020-05-19},
	date = {2019-08-01},
	langid = {english},
	keywords = {Metagenomics, {MinHash}, Containment, Jaccard index, Taxonomic classification},
}

@article{oleary_reference_2016,
	title = {Reference sequence ({RefSeq}) database at {NCBI}: current status, taxonomic expansion, and functional annotation},
	volume = {44},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/44/D1/D733/2502674},
	doi = {10.1093/nar/gkv1189},
	shorttitle = {Reference sequence ({RefSeq}) database at {NCBI}},
	abstract = {Abstract.  The {RefSeq} project at the National Center for Biotechnology Information ({NCBI}) maintains and curates a publicly available database of annotated genom},
	pages = {D733--D745},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {O'Leary, Nuala A. and Wright, Mathew W. and Brister, J. Rodney and Ciufo, Stacy and Haddad, Diana and {McVeigh}, Rich and Rajput, Bhanu and Robbertse, Barbara and Smith-White, Brian and Ako-Adjei, Danso and Astashyn, Alexander and Badretdin, Azat and Bao, Yiming and Blinkova, Olga and Brover, Vyacheslav and Chetvernin, Vyacheslav and Choi, Jinna and Cox, Eric and Ermolaeva, Olga and Farrell, Catherine M. and Goldfarb, Tamara and Gupta, Tripti and Haft, Daniel and Hatcher, Eneida and Hlavina, Wratko and Joardar, Vinita S. and Kodali, Vamsi K. and Li, Wenjun and Maglott, Donna and Masterson, Patrick and {McGarvey}, Kelly M. and Murphy, Michael R. and O'Neill, Kathleen and Pujar, Shashikant and Rangwala, Sanjida H. and Rausch, Daniel and Riddick, Lillian D. and Schoch, Conrad and Shkeda, Andrei and Storz, Susan S. and Sun, Hanzhen and Thibaud-Nissen, Francoise and Tolstoy, Igor and Tully, Raymond E. and Vatsan, Anjana R. and Wallin, Craig and Webb, David and Wu, Wendy and Landrum, Melissa J. and Kimchi, Avi and Tatusova, Tatiana and {DiCuccio}, Michael and Kitts, Paul and Murphy, Terence D. and Pruitt, Kim D.},
	urldate = {2020-05-25},
	date = {2016-01-04},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{truby_decarbonizing_2018,
	title = {Decarbonizing Bitcoin: Law and policy choices for reducing the energy consumption of Blockchain technologies and digital currencies},
	volume = {44},
	issn = {2214-6296},
	url = {http://www.sciencedirect.com/science/article/pii/S2214629618301750},
	doi = {10.1016/j.erss.2018.06.009},
	shorttitle = {Decarbonizing Bitcoin},
	abstract = {The vast transactional, trust and security advantages of Bitcoin are dwarfed by the intentionally resource-intensive design in its transaction verification process which now threatens the climate we depend upon for survival. Indeed Bitcoin mining and transactions are an application of Blockchain technology employing an inefficient use of scarce energy resources for a financial activity at a point in human development where world governments are scrambling to reduce energy consumption through their Paris Agreement climate change commitments and beyond to mitigate future climate change implications. Without encouraging more sustainable development of the potential applications of Blockchain technologies which can have significant social and economic benefits, their resource-intensive design combined now pose a serious threat to the global commitment to mitigate greenhouse gas emissions. The article examines government intervention choices to desocialise negative environmental externalities caused by high-energy consuming Blockchain technology designs. The research question explores how to promote the environmentally sustainable development of applications of Blockchain without damaging this valuable sector. It studies existing regulatory and fiscal policy approaches towards digital currencies in order to provide a basis for further legal and policy tools targeted at mitigating energy consumption of Blockchain technologies. The article concludes by identifying appropriate fiscal policy options for this purpose, as well as further considerations on the potential for Blockchain technology in climate change mitigation.},
	pages = {399--410},
	journaltitle = {Energy Research \& Social Science},
	shortjournal = {Energy Research \& Social Science},
	author = {Truby, Jon},
	urldate = {2020-05-26},
	date = {2018-10-01},
	langid = {english},
	keywords = {Behavioural change, Bitcoin, Blockchain, Carbon, Decarbonizing, Digital currencies, Energy consumption, Environmental taxation, Financial innovation, Fiscal tools, Government intervention, Technological innovation},
}

@online{digiconomist_bitcoin_nodate,
	title = {Bitcoin Energy Consumption Index},
	url = {https://digiconomist.net/bitcoin-energy-consumption},
	abstract = {The Bitcoin Energy Consumption Index provides the latest estimate of the total energy consumption of the Bitcoin network.},
	titleaddon = {Digiconomist},
	author = {{Digiconomist}},
	urldate = {2020-05-27},
	langid = {american},
	note = {Library Catalog: digiconomist.net},
}

@online{council_of_councils_working_group_on_sequence_read_archive_data_interim_2020,
	title = {Interim Report},
	url = {https://dpcpsi.nih.gov/sites/default/files/CoC_Jan_2020_1025_SRA_Working_Group_Report.pdf},
	author = {{Council of Councils Working Group on Sequence Read Archive Data}},
	urldate = {2020-05-27},
	date = {2020-01-24},
}

@article{kaki_mergeable_2019,
	title = {Mergeable replicated data types},
	volume = {3},
	issn = {2475-1421, 2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3360580},
	doi = {10.1145/3360580},
	pages = {1--29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Kaki, Gowtham and Priya, Swarn and Sivaramakrishnan, Kc and Jagannathan, Suresh},
	urldate = {2020-05-28},
	date = {2019-10-10},
	langid = {english},
}

@inproceedings{shapiro_conflict-free_2011,
	location = {Berlin, Heidelberg},
	title = {Conflict-Free Replicated Data Types},
	isbn = {978-3-642-24550-3},
	doi = {10.1007/978-3-642-24550-3_29},
	series = {Lecture Notes in Computer Science},
	abstract = {Replicating data under Eventual Consistency ({EC}) allows any replica to accept updates without remote synchronisation. This ensures performance and scalability in large-scale distributed systems (e.g., clouds). However, published {EC} approaches are ad-hoc and error-prone. Under a formal Strong Eventual Consistency ({SEC}) model, we study sufficient conditions for convergence. A data type that satisfies these conditions is called a Conflict-free Replicated Data Type ({CRDT}). Replicas of any {CRDT} are guaranteed to converge in a self-stabilising manner, despite any number of failures. This paper formalises two popular approaches (state- and operation-based) and their relevant sufficient conditions. We study a number of useful {CRDTs}, such as sets with clean semantics, supporting both add and remove operations, and consider in depth the more complex Graph data type. {CRDT} types can be composed to develop large-scale distributed applications, and have interesting theoretical properties.},
	pages = {386--400},
	booktitle = {Stabilization, Safety, and Security of Distributed Systems},
	publisher = {Springer},
	author = {Shapiro, Marc and Preguiça, Nuno and Baquero, Carlos and Zawirski, Marek},
	editor = {Défago, Xavier and Petit, Franck and Villain, Vincent},
	date = {2011},
	langid = {english},
	keywords = {Eventual Consistency, Large-Scale Distributed Systems, Replicated Shared Objects},
}

@article{korpela_seti_2001,
	title = {{SETI}@ home—massively distributed computing for {SETI}},
	volume = {3},
	pages = {78--83},
	number = {1},
	journaltitle = {Computing in science \& engineering},
	author = {Korpela, Eric and Werthimer, Dan and Anderson, David and Cobb, Jeff and Lebofsky, Matt},
	date = {2001},
	note = {Publisher: {IEEE} Computer Society},
}

@inproceedings{beberg_folding_2009,
	title = {Folding@ home: Lessons from eight years of volunteer distributed computing},
	shorttitle = {Folding@ home},
	pages = {1--8},
	booktitle = {2009 {IEEE} International Symposium on Parallel \& Distributed Processing},
	publisher = {{IEEE}},
	author = {Beberg, Adam L. and Ensign, Daniel L. and Jayachandran, Guha and Khaliq, Siraj and Pande, Vijay S.},
	date = {2009},
}

@article{pearman_testing_2020,
	title = {Testing the advantages and disadvantages of short- and long- read eukaryotic metagenomics using simulated reads},
	volume = {21},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-020-3528-4},
	doi = {10.1186/s12859-020-3528-4},
	abstract = {The first step in understanding ecological community diversity and dynamics is quantifying community membership. An increasingly common method for doing so is through metagenomics. Because of the rapidly increasing popularity of this approach, a large number of computational tools and pipelines are available for analysing metagenomic data. However, the majority of these tools have been designed and benchmarked using highly accurate short read data (i.e. Illumina), with few studies benchmarking classification accuracy for long error-prone reads ({PacBio} or Oxford Nanopore). In addition, few tools have been benchmarked for non-microbial communities.},
	pages = {220},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Pearman, William S. and Freed, Nikki E. and Silander, Olin K.},
	urldate = {2020-05-29},
	date = {2020-05-29},
}

@online{noauthor_paper_nodate,
	title = {Paper},
	url = {https://proceedings.mlsys.org/book/296.pdf},
	urldate = {2020-05-30},
}

@article{blalock_what_nodate,
	title = {What is the State of Neural Network Pruning?},
	abstract = {Neural network pruning—the task of reducing the size of a network by removing parameters—has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent ﬁndings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest ﬁnding is that the community suffers from a lack of standardized benchmarks and metrics. This deﬁciency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the ﬁeld has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce {ShrinkBench}, an open-source framework to facilitate standardized evaluations of pruning methods. We use {ShrinkBench} to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	pages = {18},
	author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	langid = {english},
}

@article{hu_parameter_2018,
	title = {Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell {RNA} transcriptomics},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/385534v4},
	doi = {10.1101/385534},
	abstract = {{\textless}p{\textgreater}Single-cell {RNA} sequencing ({scRNA}-seq) is a powerful tool to profile the transcriptomes of a large number of individual cells at a high resolution. These data usually contain measurements of gene expression for many genes in thousands or tens of thousands of cells, though some datasets now reach the million-cell mark. Projecting high-dimensional {scRNA}-seq data into a low dimensional space aids downstream analysis and data visualization. Many recent preprints accomplish this using variational autoencoders ({VAE}), generative models that learn underlying structure of data by compress it into a constrained, low dimensional space. The low dimensional spaces generated by {VAEs} have revealed complex patterns and novel biological signals from large-scale gene expression data and drug response predictions. Here, we evaluate a simple {VAE} approach for gene expression data, Tybalt, by training and measuring its performance on sets of simulated {scRNA}-seq data. We find a number of counter-intuitive performance features: i.e., deeper neural networks can struggle when datasets contain more observations under some parameter configurations. We show that these methods are highly sensitive to parameter tuning: when tuned, the performance of the Tybalt model, which was not optimized for {scRNA}-seq data, outperforms other popular dimension reduction approaches – {PCA}, {ZIFA}, {UMAP} and t-{SNE}. On the other hand, without tuning performance can also be remarkably poor on the same data. Our results should discourage authors and reviewers from relying on self-reported performance comparisons to evaluate the relative value of contributions in this area at this time. Instead, we recommend that attempts to compare or benchmark autoencoder methods for {scRNA}-seq data be performed by disinterested third parties or by methods developers only on unseen benchmark data that are provided to all participants simultaneously because the potential for performance differences due to unequal parameter tuning is so high.{\textless}/p{\textgreater}},
	pages = {385534},
	journaltitle = {{bioRxiv}},
	author = {Hu, Qiwen and Greene, Casey S.},
	urldate = {2020-05-30},
	date = {2018-09-20},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@article{altschul_basic_1990,
	title = {Basic local alignment search tool},
	volume = {215},
	pages = {403--410},
	number = {3},
	journaltitle = {Journal of molecular biology},
	author = {Altschul, Stephen F. and Gish, Warren and Miller, Webb and Myers, Eugene W. and Lipman, David J.},
	date = {1990},
	note = {Publisher: Elsevier},
}

@article{bloom_spacetime_1970,
	title = {Space/time trade-offs in hash coding with allowable errors},
	volume = {13},
	issn = {00010782},
	url = {http://portal.acm.org/citation.cfm?doid=362686.362692},
	doi = {10.1145/362686.362692},
	abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hashcoding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to "catch" the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
	pages = {422--426},
	number = {7},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Bloom, Burton H.},
	urldate = {2020-05-30},
	date = {1970-07-01},
	langid = {english},
}

@article{schloss_introducing_2009,
	title = {Introducing mothur: Open-Source, Platform-Independent, Community-Supported Software for Describing and Comparing Microbial Communities},
	volume = {75},
	issn = {0099-2240},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2786419/},
	doi = {10.1128/AEM.01541-09},
	shorttitle = {Introducing mothur},
	abstract = {mothur aims to be a comprehensive software package that allows users to use a single piece of software to analyze community sequence data. It builds upon previous tools to provide a flexible and powerful software package for analyzing sequencing data. As a case study, we used mothur to trim, screen, and align sequences; calculate distances; assign sequences to operational taxonomic units; and describe the α and β diversity of eight marine samples previously characterized by pyrosequencing of 16S {rRNA} gene fragments. This analysis of more than 222,000 sequences was completed in less than 2 h with a laptop computer.},
	pages = {7537--7541},
	number = {23},
	journaltitle = {Applied and Environmental Microbiology},
	shortjournal = {Appl Environ Microbiol},
	author = {Schloss, Patrick D. and Westcott, Sarah L. and Ryabin, Thomas and Hall, Justine R. and Hartmann, Martin and Hollister, Emily B. and Lesniewski, Ryan A. and Oakley, Brian B. and Parks, Donovan H. and Robinson, Courtney J. and Sahl, Jason W. and Stres, Blaz and Thallinger, Gerhard G. and Van Horn, David J. and Weber, Carolyn F.},
	urldate = {2020-05-31},
	date = {2009-12},
	pmid = {19801464},
	pmcid = {PMC2786419},
}

@article{huson_megan_2007,
	title = {{MEGAN} analysis of metagenomic data},
	volume = {17},
	issn = {1088-9051, 1549-5469},
	url = {http://genome.cshlp.org/content/17/3/377},
	doi = {10.1101/gr.5969107},
	abstract = {Metagenomics is the study of the genomic content of a sample of organisms obtained from a common habitat using targeted or random sequencing. Goals include understanding the extent and role of microbial diversity. The taxonomical content of such a sample is usually estimated by comparison against sequence databases of known sequences. Most published studies use the analysis of paired-end reads, complete sequences of environmental fosmid and {BAC} clones, or environmental assemblies. Emerging sequencing-by-synthesis technologies with very high throughput are paving the way to low-cost random “shotgun” approaches. This paper introduces {MEGAN}, a new computer program that allows laptop analysis of large metagenomic data sets. In a preprocessing step, the set of {DNA} sequences is compared against databases of known sequences using {BLAST} or another comparison tool. {MEGAN} is then used to compute and explore the taxonomical content of the data set, employing the {NCBI} taxonomy to summarize and order the results. A simple lowest common ancestor algorithm assigns reads to taxa such that the taxonomical level of the assigned taxon reflects the level of conservation of the sequence. The software allows large data sets to be dissected without the need for assembly or the targeting of specific phylogenetic markers. It provides graphical and statistical output for comparing different data sets. The approach is applied to several data sets, including the Sargasso Sea data set, a recently published metagenomic data set sampled from a mammoth bone, and several complete microbial genomes. Also, simulations that evaluate the performance of the approach for different read lengths are presented.},
	pages = {377--386},
	number = {3},
	journaltitle = {Genome Research},
	shortjournal = {Genome Res.},
	author = {Huson, Daniel H. and Auch, Alexander F. and Qi, Ji and Schuster, Stephan C.},
	urldate = {2020-05-31},
	date = {2007-03-01},
	langid = {english},
	pmid = {17255551},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
}

@article{huson_megan_2016,
	title = {{MEGAN} Community Edition - Interactive Exploration and Analysis of Large-Scale Microbiome Sequencing Data},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004957},
	doi = {10.1371/journal.pcbi.1004957},
	abstract = {There is increasing interest in employing shotgun sequencing, rather than amplicon sequencing, to analyze microbiome samples. Typical projects may involve hundreds of samples and billions of sequencing reads. The comparison of such samples against a protein reference database generates billions of alignments and the analysis of such data is computationally challenging. To address this, we have substantially rewritten and extended our widely-used microbiome analysis tool {MEGAN} so as to facilitate the interactive analysis of the taxonomic and functional content of very large microbiome datasets. Other new features include a functional classifier called {InterPro}2GO, gene-centric read assembly, principal coordinate analysis of taxonomy and function, and support for metadata. The new program is called {MEGAN} Community Edition ({CE}) and is open source. By integrating {MEGAN} {CE} with our high-throughput {DNA}-to-protein alignment tool {DIAMOND} and by providing a new program {MeganServer} that allows access to metagenome analysis files hosted on a server, we provide a straightforward, yet powerful and complete pipeline for the analysis of metagenome shotgun sequences. We illustrate how to perform a full-scale computational analysis of a metagenomic sequencing project, involving 12 samples and 800 million reads, in less than three days on a single server. All source code is available here: https://github.com/danielhuson/megan-ce},
	pages = {e1004957},
	number = {6},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Huson, Daniel H. and Beier, Sina and Flade, Isabell and Górska, Anna and El-Hadidi, Mohamed and Mitra, Suparna and Ruscheweyh, Hans-Joachim and Tappu, Rewati},
	urldate = {2020-05-31},
	date = {2016-06-21},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Microbiome, Sequence databases, Metagenomics, Shotgun sequencing, Taxonomy, Sequence alignment, Computer software, Functional analysis},
}

@online{noauthor_tendo_nodate,
	title = {Tendo vindo do mercado, o que gostaria de ter entendido antes de entrar em um mestrado/doutorado em computação},
	url = {http://brunocartaxo.com/o-que-gostaria-de-ter-entendido-antes-de-entrar-em-um-mestrado-doutorado-em-computacao/},
	urldate = {2020-06-04},
	langid = {american},
	note = {Library Catalog: brunocartaxo.com},
}

@article{kumar_algorithms_2003,
	title = {Algorithms column: sublinear time algorithms},
	volume = {34},
	issn = {0163-5700},
	url = {https://doi.org/10.1145/954092.954103},
	doi = {10.1145/954092.954103},
	shorttitle = {Algorithms column},
	pages = {57--67},
	number = {4},
	journaltitle = {{ACM} {SIGACT} News},
	shortjournal = {{SIGACT} News},
	author = {Kumar, Ravi and Rubinfeld, Ronitt},
	urldate = {2020-06-08},
	date = {2003-12-01},
}

@article{rubinfeld_sublinear_2011,
	title = {Sublinear Time Algorithms},
	volume = {25},
	issn = {0895-4801},
	url = {https://epubs.siam.org/doi/10.1137/100791075},
	doi = {10.1137/100791075},
	abstract = {Sublinear time algorithms represent a new paradigm in computing, where an algorithm must give some sort of an answer after inspecting only a very small portion of the input. We discuss the types of answers that one can hope to achieve in this setting.},
	pages = {1562--1588},
	number = {4},
	journaltitle = {{SIAM} Journal on Discrete Mathematics},
	shortjournal = {{SIAM} J. Discrete Math.},
	author = {Rubinfeld, Ronitt and Shapira, Asaf},
	urldate = {2020-06-08},
	date = {2011-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/28/19/2520/290322},
	doi = {10.1093/bioinformatics/bts480},
	abstract = {Abstract.  Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that},
	pages = {2520--2522},
	number = {19},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	urldate = {2020-06-09},
	date = {2012-10-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{boettiger_introduction_2015,
	title = {An introduction to Docker for reproducible research},
	volume = {49},
	pages = {71--79},
	number = {1},
	journaltitle = {{ACM} {SIGOPS} Operating Systems Review},
	author = {Boettiger, Carl},
	date = {2015},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
}

@article{kurtzer_singularity_2017,
	title = {Singularity: Scientific containers for mobility of compute},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459},
	doi = {10.1371/journal.pone.0177459},
	shorttitle = {Singularity},
	abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and {HPC} centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
	pages = {e0177459},
	number = {5},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
	urldate = {2020-06-09},
	date = {2017-05-11},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Software tools, Computer software, Open source software, Operating systems, Research validity, Software design, Software development, Tar},
}

@article{gruning_bioconda_2018,
	title = {Bioconda: sustainable and comprehensive software distribution for the life sciences},
	volume = {15},
	shorttitle = {Bioconda},
	pages = {475--476},
	number = {7},
	journaltitle = {Nature methods},
	author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A. and Rowe, Jillian and Tomkins-Tinch, Christopher H. and Valieris, Renan and Köster, Johannes},
	date = {2018},
	note = {Publisher: Nature Publishing Group},
}

@article{rossum_diversity_2020,
	title = {Diversity within species: interpreting strains in microbiomes},
	rights = {2020 Springer Nature Limited},
	issn = {1740-1534},
	url = {https://www.nature.com/articles/s41579-020-0368-1},
	doi = {10.1038/s41579-020-0368-1},
	shorttitle = {Diversity within species},
	abstract = {Large-scale metagenomic analyses are vastly increasing the rate of discovery of variation within species but they are also leading to scientific and semantic challenges. Bork and colleagues highlight the advances and challenges that are resulting from the use of metagenomic data to study within-species diversity.},
	pages = {1--16},
	journaltitle = {Nature Reviews Microbiology},
	shortjournal = {Nat Rev Microbiol},
	author = {Rossum, Thea Van and Ferretti, Pamela and Maistrenko, Oleksandr M. and Bork, Peer},
	urldate = {2020-06-10},
	date = {2020-06-04},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
}

@article{quinn_field_2019,
	title = {A field guide for the compositional analysis of any-omics data},
	volume = {8},
	url = {https://academic.oup.com/gigascience/article/8/9/giz107/5572529},
	doi = {10.1093/gigascience/giz107},
	abstract = {{AbstractBackground}.  Next-generation sequencing ({NGS}) has made it possible to determine the sequence and relative abundance of all nucleotides in a biological o},
	number = {9},
	journaltitle = {{GigaScience}},
	shortjournal = {Gigascience},
	author = {Quinn, Thomas P. and Erb, Ionas and Gloor, Greg and Notredame, Cedric and Richardson, Mark F. and Crowley, Tamsyn M.},
	urldate = {2020-06-27},
	date = {2019-09-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{quinn_understanding_2018,
	title = {Understanding sequencing data as compositions: an outlook and review},
	volume = {34},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/34/16/2870/4956011},
	doi = {10.1093/bioinformatics/bty175},
	shorttitle = {Understanding sequencing data as compositions},
	abstract = {{AbstractMotivation}.  Although seldom acknowledged explicitly, count data generated by sequencing platforms exist as compositions for which the abundance of each},
	pages = {2870--2878},
	number = {16},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Quinn, Thomas P. and Erb, Ionas and Richardson, Mark F. and Crowley, Tamsyn M.},
	urldate = {2020-06-27},
	date = {2018-08-15},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{lu_craft_2020,
	title = {{CRAFT}: Compact genome Representation towards largescale Alignment-Free {daTabase}},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.10.196741v1},
	doi = {10.1101/2020.07.10.196741},
	shorttitle = {{CRAFT}},
	abstract = {{\textless}p{\textgreater}Motivation: Rapid developments in sequencing technologies have boosted generating high volumes of sequence data. To archive and analyze those data, one primary step is sequence comparison. Alignmentfree sequence comparison based on k-mer frequencies offers a computationally efficient solution, yet in practice, the k-mer frequency vectors for large k of practical interest lead to excessive memory and storage consumption. Results: We report {CRAFT}, a general genomic/metagenomic search engine to learn compact representations of sequences and perform fast comparison between {DNA} sequences. Specifically, given genome or high throughput sequencing ({HTS}) data as input, {CRAFT} maps the data into a much smaller embedding space and locates the best matching genome in the archived massive sequence repositories. With 102 {\textasciitilde} 104-fold reduction of storage space, {CRAFT} performs fast query for gigabytes of data within seconds or minutes, achieving comparable performance as six state-of-the-art alignment-free measures. Availability: {CRAFT} offers a user-friendly graphical user interface with one-click installation on Windows and Linux operating systems, freely available at https://github.com/jiaxingbai/{CRAFT}.{\textless}/p{\textgreater}},
	pages = {2020.07.10.196741},
	journaltitle = {{bioRxiv}},
	author = {Lu, Yang and Bai, Jiaxing and Wang, Yinwen and Wang, Ying and Sun, Fengzhu},
	urldate = {2020-07-14},
	date = {2020-07-12},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@article{lim_curation_2020,
	title = {Curation of over 10,000 transcriptomic studies to enable data reuse},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.13.201442v1},
	doi = {10.1101/2020.07.13.201442},
	abstract = {{\textless}p{\textgreater}Vast amounts of transcriptomic data reside in public repositories, but effective reuse remains challenging. Issues include unstructured dataset metadata, inconsistent data processing and quality control, and inconsistent probe-gene mappings across microarray technologies. Thus, extensive curation and data reprocessing is necessary prior to any reuse. The Gemma bioinformatics system was created to help address these issues. Gemma consists of a database of curated transcriptomic datasets, analytical software, a web interface, and web services. Here we present an update on Gemma9s holdings, data processing and analysis pipelines, our curation guidelines, and software features. As of June 2020, Gemma contains 10,811 manually curated datasets (primarily human, mouse, and rat), over 395,000 samples and hundreds of curated transcriptomic platforms (both microarray and {RNA}-sequencing). Dataset topics were represented with 10,215 distinct terms from 12 ontologies, for a total of 54,316 topic annotations (mean topics/dataset = 5.2). While Gemma has broad coverage of conditions and tissues, it captures a large majority of available brain-related datasets, accounting for 34\% of its holdings. Users can access the curated data and differential expression analyses through the Gemma website, {RESTful} service, and an R package.{\textless}/p{\textgreater}},
	pages = {2020.07.13.201442},
	journaltitle = {{bioRxiv}},
	author = {Lim, Nathaniel and Tesar, Stepan and Belmadani, Manuel and Poirier-Morency, Guillaume and Mancarci, Burak Ogan and Sicherman, Jordan and Jacobson, Matthew and Leong, Justin and Tan, Patrick and Pavlidis, Paul},
	urldate = {2020-07-15},
	date = {2020-07-14},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@article{comin_comparison_nodate,
	title = {Comparison of microbiome samples: methods and computational challenges},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbaa121/5861761},
	doi = {10.1093/bib/bbaa121},
	shorttitle = {Comparison of microbiome samples},
	abstract = {Abstract.  The study of microbial communities crucially relies on the comparison of metagenomic next-generation sequencing data sets, for which several methods},
	journaltitle = {Briefings in Bioinformatics},
	shortjournal = {Brief Bioinform},
	author = {Comin, Matteo and Di Camillo, Barbara and Pizzi, Cinzia and Vandin, Fabio},
	urldate = {2020-07-30},
	langid = {english},
}

@article{tovo_taxonomic_nodate,
	title = {Taxonomic classification method for metagenomics based on core protein families with Core-Kaiju},
	url = {https://academic.oup.com/nar/article/doi/10.1093/nar/gkaa568/5868338},
	doi = {10.1093/nar/gkaa568},
	abstract = {Abstract.  Characterizing species diversity and composition of bacteria hosted by biota is revolutionizing our understanding of the role of symbiotic interactio},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Tovo, Anna and Menzel, Peter and Krogh, Anders and Cosentino Lagomarsino, Marco and Suweis, Samir},
	urldate = {2020-07-30},
	langid = {english},
}

@article{zhu_combinatorial_2020,
	title = {Combinatorial Algorithms for Strain Level Metagenomic Microbial Detection and Quantification},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.06.12.149245v1},
	doi = {10.1101/2020.06.12.149245},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Identifying and quantifying the microbial composition of a complex biological or environmental sample is one of the primary challenges in microbiology. Many software tools have been developed to classify metagenomic sequencing reads originating from a mixture of bacterial or viral genomes, and to estimate the microbial abundance profile of the mixture. Unfortunately the accuracy of these tools significantly degrade in the presence of large portions of shared content among the genomes in the mixture or the genomic database in use. Here we introduce {CAMMiQ}, a novel combinatorial solution to the microbial identification and abundance estimation problem, which improves all available tools with respect to the number of correctly classified reads (i.e., specificity) by an order of magnitude and resolves possible mixtures of similar genomes, possibly at the strain level. The key contribution of {CAMMiQ} is its use of arbitrary length, doubly-unique substrings, i.e. substrings that appear in exactly two genomes in the input database, instead of fixed-length, unique substrings. In order to resolve the ambiguity in the genomic origin of doubly-unique substrings, {CAMMiQ} employs a combinatorial optimization formulation, which can be solved surprisingly quickly. {CAMMiQ}’s index consists of a sparsified subset of the shortest unique and doubly-unique substrings of each genome in the database, within a user specified length range and as such it is fairly compact. In short, {CAMMiQ} offers more accurate genomic identification and abundance estimation than the best known \textit{k}-mer based and marker gene based alternatives through the use of comparable computational resources.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Availability{\textless}/h3{\textgreater} {\textless}p{\textgreater}https://github.com/algo-cancer/{CAMMiQ}{\textless}/p{\textgreater}},
	pages = {2020.06.12.149245},
	journaltitle = {{bioRxiv}},
	author = {Zhu, Kaiyuan and Xu, Junyan and Ergun, A. Funda and Ye, Yuzhen and Sahinalp, S. Cenk},
	urldate = {2020-07-30},
	date = {2020-06-14},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@article{calgaro_assessment_2020,
	title = {Assessment of statistical methods from single cell, bulk {RNA}-seq, and metagenomics applied to microbiome data},
	volume = {21},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-020-02104-1},
	doi = {10.1186/s13059-020-02104-1},
	abstract = {The correct identification of differentially abundant microbial taxa between experimental conditions is a methodological and computational challenge. Recent work has produced methods to deal with the high sparsity and compositionality characteristic of microbiome data, but independent benchmarks comparing these to alternatives developed for {RNA}-seq data analysis are lacking.},
	pages = {191},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Calgaro, Matteo and Romualdi, Chiara and Waldron, Levi and Risso, Davide and Vitulo, Nicola},
	urldate = {2020-08-05},
	date = {2020-08-03},
}

@article{kim_centrifuge_2016,
	title = {Centrifuge: rapid and sensitive classification of metagenomic sequences},
	volume = {26},
	issn = {1088-9051, 1549-5469},
	url = {http://genome.cshlp.org/content/26/12/1721},
	doi = {10.1101/gr.210641.116},
	shorttitle = {Centrifuge},
	abstract = {Centrifuge is a novel microbial classification engine that enables rapid, accurate, and sensitive labeling of reads and quantification of species on desktop computers. The system uses an indexing scheme based on the Burrows-Wheeler transform ({BWT}) and the Ferragina-Manzini ({FM}) index, optimized specifically for the metagenomic classification problem. Centrifuge requires a relatively small index (4.2 {GB} for 4078 bacterial and 200 archaeal genomes) and classifies sequences at very high speed, allowing it to process the millions of reads from a typical high-throughput {DNA} sequencing run within a few minutes. Together, these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers. Because of its space-optimized indexing schemes, Centrifuge also makes it possible to index the entire {NCBI} nonredundant nucleotide sequence database (a total of 109 billion bases) with an index size of 69 {GB}, in contrast to k-mer-based indexing schemes, which require far more extensive space.},
	pages = {1721--1729},
	number = {12},
	journaltitle = {Genome Research},
	shortjournal = {Genome Res.},
	author = {Kim, Daehwan and Song, Li and Breitwieser, Florian P. and Salzberg, Steven L.},
	urldate = {2020-08-17},
	date = {2016-12-01},
	langid = {english},
	pmid = {27852649},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
}

@report{meyer_tutorial_2020,
	title = {Tutorial: Assessing metagenomics software with the {CAMI} benchmarking toolkit},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.08.11.245712},
	shorttitle = {Tutorial},
	abstract = {Abstract
          Computational methods are key in microbiome research, and obtaining a quantitative and unbiased performance estimate is important for method developers and applied researchers. For meaningful comparisons between methods, to identify best practices, common use cases, and to reduce overhead in benchmarking, it is necessary to have standardized data sets, procedures, and metrics for evaluation. In this tutorial, we describe emerging standards in computational metaomics benchmarking derived and agreed upon by a larger community of researchers. Specifically, we outline recent efforts by the Critical Assessment of Metagenome Interpretation ({CAMI}) initiative, which supplies method developers and applied researchers with exhaustive quantitative data about software performance in realistic scenarios and organizes community-driven benchmarking challenges. We explain the most relevant evaluation metrics to assess metagenome assembly, binning, and profiling results, and provide step-by-step instructions on how to generate them. The instructions use simulated mouse gut metagenome data released in preparation for the second round of {CAMI} challenges and showcase the use of a repository of tool results for {CAMI} data sets. This tutorial will serve as a reference to the community and facilitate informative and reproducible benchmarking in microbiome research.},
	institution = {Bioinformatics},
	type = {preprint},
	author = {Meyer, Fernando and Lesker, Till-Robin and Koslicki, David and Fritz, Adrian and Gurevich, Alexey and Darling, Aaron E. and Sczyrba, Alexander and Bremges, Andreas and {McHardy}, Alice C.},
	urldate = {2020-08-18},
	date = {2020-08-12},
	langid = {english},
	doi = {10.1101/2020.08.11.245712},
}

@article{fritz_camisim_2019,
	title = {{CAMISIM}: simulating metagenomes and microbial communities},
	volume = {7},
	issn = {2049-2618},
	url = {https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-019-0633-6},
	doi = {10.1186/s40168-019-0633-6},
	shorttitle = {{CAMISIM}},
	pages = {17},
	number = {1},
	journaltitle = {Microbiome},
	shortjournal = {Microbiome},
	author = {Fritz, Adrian and Hofmann, Peter and Majda, Stephan and Dahms, Eik and Dröge, Johannes and Fiedler, Jessika and Lesker, Till R. and Belmann, Peter and {DeMaere}, Matthew Z. and Darling, Aaron E. and Sczyrba, Alexander and Bremges, Andreas and {McHardy}, Alice C.},
	urldate = {2020-08-19},
	date = {2019-12},
	langid = {english},
}

@online{congress_this_nodate,
	title = {This is Fine: Optimism \& Emergency in the P2P Network {\textbar} The New Design Congress},
	url = {https://newdesigncongress.org/en/pub/this-is-fine},
	shorttitle = {This is Fine},
	abstract = {As we enter the 2020s, centralised power and decentralised communities are on the verge of outright conflict for the control of the digital public space. The resilience of centralised networks and the political organisation of their owners remains significantly underestimated by protocol activists. At the same time, the peer-to-peer community is dangerously unprepared for a crisis-fuelled future that has very suddenly arrived at their door.},
	author = {Congress, The New Design},
	urldate = {2020-08-24},
}

@online{noauthor_introducing_nodate,
	title = {Introducing Decentralization Off The Shelf: 7 Maxims – Simply Secure},
	url = {https://simplysecure.org/blog/decentralization-off-the-shelf-7-maxims},
	urldate = {2020-08-24},
}

@article{tovo_taxonomic_nodate-1,
	title = {Taxonomic classification method for metagenomics based on core protein families with Core-Kaiju},
	url = {https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkaa568/5868338},
	doi = {10.1093/nar/gkaa568},
	abstract = {Abstract.  Characterizing species diversity and composition of bacteria hosted by biota is revolutionizing our understanding of the role of symbiotic interactio},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Tovo, Anna and Menzel, Peter and Krogh, Anders and Cosentino Lagomarsino, Marco and Suweis, Samir},
	urldate = {2020-08-24},
	langid = {english},
}

@article{kirzhner_evaluating_2020,
	title = {Evaluating the Number of Different Genomes in a Metagenome by Means of the Compositional Spectra Approach},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.23.217364v1},
	doi = {10.1101/2020.07.23.217364},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Determination of metagenome composition is still one of the most interesting problems of bioinformatics. It involves a wide range of mathematical methods, from probabilistic models of combinatorics to cluster analysis and pattern recognition techniques. The successful advance of rapid sequencing methods and fast and precise metagenome analysis will increase the diagnostic value of healthy or pathological human metagenomes. The article presents the theoretical foundations of the algorithm for calculating the number of different genomes in the medium under study. The approach is based on analysis of the compositional spectra of subsequently sequenced samples of the medium. Its essential feature is using random fluctuations in the bacteria number in different samples of the same metagenome. The possibility of effective implementation of the algorithm in the presence of data errors is also discussed. In the work, the algorithm of a metagenome evaluation is described, including the estimation of the genome number and the identification of the genomes with known compositional spectra. It should be emphasized that evaluating the genome number in a metagenome can be always helpful, regardless of the metagenome separation techniques, such as clustering the sequencing results or marker analysis.{\textless}/p{\textgreater}},
	pages = {2020.07.23.217364},
	journaltitle = {{bioRxiv}},
	author = {Kirzhner, Valery and Toledano-Kitai, Dvora and Volkovich, Zeev},
	urldate = {2020-08-24},
	date = {2020-07-23},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@online{noauthor_decentralization_nodate,
	title = {Decentralization Off The Shelf: 7 Maxims},
	url = {https://decentpatterns.xyz/report/},
	abstract = {Decentralization Off The Shelf is a collective effort to further the adoption of decentralized technologies by providing open tooling and resources for the community.},
	titleaddon = {Decentralization Off The Shelf},
	urldate = {2020-08-24},
	langid = {english},
}

@inproceedings{zhang_efficient_2020,
	location = {New York, {NY}, {USA}},
	title = {Efficient Search over Genomic Short Read Data},
	isbn = {978-1-4503-8814-6},
	url = {https://doi.org/10.1145/3400903.3400907},
	doi = {10.1145/3400903.3400907},
	series = {{SSDBM} 2020},
	abstract = {Modern {DNA} sequencing technology produces large volumes of genome strings for various biological and medical applications. To mitigate the space overhead required for storing these genome data, previous research has developed compression schemes to save storage resources and to improve the locality for bioinformatics applications. These approaches, however, typically need significant additional processing to support efficient genome string search, an important step for downstream applications in a bioinformatics pipeline. In this paper, we propose to store raw {DNA} sequence data in a compressed but searchable format, enabling efficient string lookups using database indexes. To build this format, we partition genome strings by computing hash-based minimizers to group overlapping strings together. We carefully optimize our hash function to avoid large buckets caused by repetitive sequences. These buckets are then effectively compressed using local constructed references, and an index is built upon them to guide exact match lookups. Both data compression and string lookups exploit the enhanced locality as a result of the partitioning. We implement storage and search functions over the storage format as a multithreaded library, and perform extensive experiments on real genome sequencing data. The results show that our approach efficiently executes genome string lookups over highly compressed raw read data.},
	pages = {1--12},
	booktitle = {32nd International Conference on Scientific and Statistical Database Management},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Wangda and Lin, Mengdi and Ross, Kenneth A.},
	urldate = {2020-08-24},
	date = {2020-07-07},
	keywords = {{DNA} sequencing, genome analysis, genomic compression, hash minimizer, short read search}
}

@article{pearman_testing_2020-1,
	title = {Testing the advantages and disadvantages of short- and long- read eukaryotic metagenomics using simulated reads},
	volume = {21},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-020-3528-4},
	doi = {10.1186/s12859-020-3528-4},
	abstract = {The first step in understanding ecological community diversity and dynamics is quantifying community membership. An increasingly common method for doing so is through metagenomics. Because of the rapidly increasing popularity of this approach, a large number of computational tools and pipelines are available for analysing metagenomic data. However, the majority of these tools have been designed and benchmarked using highly accurate short read data (i.e. Illumina), with few studies benchmarking classification accuracy for long error-prone reads ({PacBio} or Oxford Nanopore). In addition, few tools have been benchmarked for non-microbial communities.},
	pages = {220},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Pearman, William S. and Freed, Nikki E. and Silander, Olin K.},
	urldate = {2020-08-24},
	date = {2020-05-29},
}

@article{oleary_reference_2016-1,
	title = {Reference sequence ({RefSeq}) database at {NCBI}: current status, taxonomic expansion, and functional annotation},
	volume = {44},
	issn = {0305-1048},
	url = {https://academic.oup.com/nar/article/44/D1/D733/2502674},
	doi = {10.1093/nar/gkv1189},
	shorttitle = {Reference sequence ({RefSeq}) database at {NCBI}},
	abstract = {Abstract.  The {RefSeq} project at the National Center for Biotechnology Information ({NCBI}) maintains and curates a publicly available database of annotated genom},
	pages = {D733--D745},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {O'Leary, Nuala A. and Wright, Mathew W. and Brister, J. Rodney and Ciufo, Stacy and Haddad, Diana and {McVeigh}, Rich and Rajput, Bhanu and Robbertse, Barbara and Smith-White, Brian and Ako-Adjei, Danso and Astashyn, Alexander and Badretdin, Azat and Bao, Yiming and Blinkova, Olga and Brover, Vyacheslav and Chetvernin, Vyacheslav and Choi, Jinna and Cox, Eric and Ermolaeva, Olga and Farrell, Catherine M. and Goldfarb, Tamara and Gupta, Tripti and Haft, Daniel and Hatcher, Eneida and Hlavina, Wratko and Joardar, Vinita S. and Kodali, Vamsi K. and Li, Wenjun and Maglott, Donna and Masterson, Patrick and {McGarvey}, Kelly M. and Murphy, Michael R. and O'Neill, Kathleen and Pujar, Shashikant and Rangwala, Sanjida H. and Rausch, Daniel and Riddick, Lillian D. and Schoch, Conrad and Shkeda, Andrei and Storz, Susan S. and Sun, Hanzhen and Thibaud-Nissen, Francoise and Tolstoy, Igor and Tully, Raymond E. and Vatsan, Anjana R. and Wallin, Craig and Webb, David and Wu, Wendy and Landrum, Melissa J. and Kimchi, Avi and Tatusova, Tatiana and {DiCuccio}, Michael and Kitts, Paul and Murphy, Terence D. and Pruitt, Kim D.},
	urldate = {2020-08-24},
	date = {2016-01-04},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{tyson_community_2004,
	title = {Community structure and metabolism through reconstruction of microbial genomes from the environment},
	volume = {428},
	rights = {2003 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature02340},
	doi = {10.1038/nature02340},
	abstract = {Microbial communities are vital in the functioning of all ecosystems; however, most microorganisms are uncultivated, and their roles in natural systems are unclear. Here, using random shotgun sequencing of {DNA} from a natural acidophilic biofilm, we report reconstruction of near-complete genomes of Leptospirillum group {II} and Ferroplasma type {II}, and partial recovery of three other genomes. This was possible because the biofilm was dominated by a small number of species populations and the frequency of genomic rearrangements and gene insertions or deletions was relatively low. Because each sequence read came from a different individual, we could determine that single-nucleotide polymorphisms are the predominant form of heterogeneity at the strain level. The Leptospirillum group {II} genome had remarkably few nucleotide polymorphisms, despite the existence of low-abundance variants. The Ferroplasma type {II} genome seems to be a composite from three ancestral strains that have undergone homologous recombination to form a large population of mosaic genomes. Analysis of the gene complement for each organism revealed the pathways for carbon and nitrogen fixation and energy generation, and provided insights into survival strategies in an extreme environment.},
	pages = {37--43},
	number = {6978},
	journaltitle = {Nature},
	author = {Tyson, Gene W. and Chapman, Jarrod and Hugenholtz, Philip and Allen, Eric E. and Ram, Rachna J. and Richardson, Paul M. and Solovyev, Victor V. and Rubin, Edward M. and Rokhsar, Daniel S. and Banfield, Jillian F.},
	urldate = {2020-08-25},
	date = {2004-03},
	langid = {english},
	note = {Number: 6978
Publisher: Nature Publishing Group},
}

@online{noauthor_petabase-scale_nodate,
	title = {Petabase-scale sequence alignment catalyses viral discovery {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.07.241729v1},
	urldate = {2020-08-25},
}

@article{storato_improving_2020,
	title = {Improving Metagenomic Classification using Discriminative k-mers from Sequencing Data},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.02.20.957308v1},
	doi = {10.1101/2020.02.20.957308},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}The major problem when analyzing a metagenomic sample is to taxonomically annotate its reads in order to identify the species they contain. Most of the methods currently available focus on the classification of reads using a set of reference genomes and their k-mers. While in terms of precision these methods have reached percentages of correctness close to perfection, in terms of recall (the actual number of classified reads) the performances fall at around 50\%. One of the reasons is the fact that the sequences in a sample can be very different from the corresponding reference genome, e.g. viral genomes are highly mutated. To address this issue, in this paper we study the problem of metagenomic reads classification by improving the reference k-mers library with novel discriminative k-mers from the input sequencing reads. We evaluated the performance in different conditions against several other tools and the results showed an improved F-measure, especially when close reference genomes are not available.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Availability{\textless}/h3{\textgreater} {\textless}p{\textgreater}https://github.com/davide92/K2Mem.git{\textless}/p{\textgreater}},
	pages = {2020.02.20.957308},
	journaltitle = {{bioRxiv}},
	author = {Storato, D. and Comin, M.},
	urldate = {2020-08-26},
	date = {2020-02-20},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@article{tully_reconstruction_2018,
	title = {The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans},
	volume = {5},
	rights = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2017203},
	doi = {10.1038/sdata.2017.203},
	abstract = {Microorganisms play a crucial role in mediating global biogeochemical cycles in the marine environment. By reconstructing the genomes of environmental organisms through metagenomics, researchers are able to study the metabolic potential of Bacteria and Archaea that are resistant to isolation in the laboratory. Utilizing the large metagenomic dataset generated from 234 samples collected during the Tara Oceans circumnavigation expedition, we were able to assemble 102 billion paired-end reads into 562 million contigs, which in turn were co-assembled and consolidated in to 7.2 million contigs ≥2 kb in length. Approximately 1 million of these contigs were binned to reconstruct draft genomes. In total, 2,631 draft genomes with an estimated completion of ≥50\% were generated (1,491 draft genomes {\textgreater}70\% complete; 603 genomes {\textgreater}90\% complete). A majority of the draft genomes were manually assigned phylogeny based on sets of concatenated phylogenetic marker genes and/or 16S {rRNA} gene sequences. The draft genomes are now publically available for the research community at-large.},
	pages = {170203},
	number = {1},
	journaltitle = {Scientific Data},
	author = {Tully, Benjamin J. and Graham, Elaina D. and Heidelberg, John F.},
	urldate = {2020-08-26},
	date = {2018-01-16},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
}

@article{pesant_open_2015,
	title = {Open science resources for the discovery and analysis of Tara Oceans data},
	volume = {2},
	rights = {2015 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201523/},
	doi = {10.1038/sdata.2015.23},
	abstract = {The Tara Oceans expedition (2009–2013) sampled contrasting ecosystems of the world oceans, collecting environmental data and plankton, from viruses to metazoans, for later analysis using modern sequencing and state-of-the-art imaging technologies. It surveyed 210 ecosystems in 20 biogeographic provinces, collecting over 35,000 samples of seawater and plankton. The interpretation of such an extensive collection of samples in their ecological context requires means to explore, assess and access raw and validated data sets. To address this challenge, the Tara Oceans Consortium offers open science resources, including the use of open access archives for nucleotides ({ENA}) and for environmental, biogeochemical, taxonomic and morphological data ({PANGAEA}), and the development of on line discovery tools and collaborative annotation tools for sequences and images. Here, we present an overview of Tara Oceans Data, and we provide detailed registries (data sets) of all campaigns (from port-to-port), stations and sampling events.},
	pages = {150023},
	number = {1},
	journaltitle = {Scientific Data},
	author = {Pesant, Stéphane and Not, Fabrice and Picheral, Marc and Kandels-Lewis, Stefanie and Le Bescot, Noan and Gorsky, Gabriel and Iudicone, Daniele and Karsenti, Eric and Speich, Sabrina and Troublé, Romain and Dimier, Céline and Searson, Sarah},
	urldate = {2020-08-26},
	date = {2015-05-26},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
}

@article{edgar_petabase-scale_2020,
	title = {Petabase-scale sequence alignment catalyses viral discovery},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.07.241729v1},
	doi = {10.1101/2020.08.07.241729},
	abstract = {{\textless}p{\textgreater}Public sequence data represents a major opportunity for viral discovery, but its exploration has been inhibited by a lack of efficient methods for searching this corpus, which is currently at the petabase scale and growing exponentially. To address the ongoing pandemic caused by Severe Acute Respiratory Syndrome Coronavirus 2 and expand the known sequence diversity of viruses, we aligned pangenomes for coronaviruses ({CoV}) and other viral families to 5.6 petabases of public sequencing data from 3.8 million biologically diverse samples. To implement this strategy, we developed a cloud computing architecture, `Serratus`, tailored for ultra-high throughput sequence alignment at the petabase scale. From this search, we identified and assembled thousands of {CoV} and {CoV}-like genomes and genome fragments ranging from known strains to putatively novel genera. We generalise this strategy to other viral families, identifying several novel deltaviruses and huge bacteriophages. To catalyse a new era of viral discovery we made millions of viral alignments and family identifications freely available to the research community (https://serratus.io). Expanding the known diversity and zoonotic reservoirs of {CoV} and other emerging pathogens can accelerate vaccine and therapeutic developments for the current pandemic, and help us anticipate and mitigate future ones.{\textless}/p{\textgreater}},
	pages = {2020.08.07.241729},
	journaltitle = {{bioRxiv}},
	author = {Edgar, Robert C. and Taylor, Jeff and Altman, Tomer and Barbera, Pierre and Meleshko, Dmitry and Lin, Victor and Lohr, Dan and Novakovsky, Gherman and Al-Shayeb, Basem and Banfield, Jillian F. and Korobeynikov, Anton and Chikhi, Rayan and Babaian, Artem},
	urldate = {2020-08-26},
	date = {2020-08-10},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
}

@inproceedings{stewart_jetstream_2015,
	title = {Jetstream: a self-provisioned, scalable science and engineering cloud environment},
	shorttitle = {Jetstream},
	pages = {1--8},
	booktitle = {Proceedings of the 2015 {XSEDE} Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
	author = {Stewart, Craig A. and Cockerill, Timothy M. and Foster, Ian and Hancock, David and Merchant, Nirav and Skidmore, Edwin and Stanzione, Daniel and Taylor, James and Tuecke, Steven and Turner, George},
	date = {2015},
}

@article{integrative_hmp_ihmp_research_network_consortium_integrative_2014,
	title = {The Integrative Human Microbiome Project: dynamic analysis of microbiome-host omics profiles during periods of human health and disease},
	volume = {16},
	issn = {1934-6069},
	doi = {10.1016/j.chom.2014.08.014},
	shorttitle = {The Integrative Human Microbiome Project},
	abstract = {Much has been learned about the diversity and distribution of human-associated microbial communities, but we still know little about the biology of the microbiome, how it interacts with the host, and how the host responds to its resident microbiota. The Integrative Human Microbiome Project ({iHMP}, http://hmp2.org), the second phase of the {NIH} Human Microbiome Project, will study these interactions by analyzing microbiome and host activities in longitudinal studies of disease-specific cohorts and by creating integrated data sets of microbiome and host functional properties. These data sets will serve as experimental test beds to evaluate new models, methods, and analyses on the interactions of host and microbiome. Here we describe the three models of microbiome-associated human conditions, on the dynamics of preterm birth, inflammatory bowel disease, and type 2 diabetes, and their underlying hypotheses, as well as the multi-omic data types to be collected, integrated, and distributed through public repositories as a community resource.},
	pages = {276--289},
	number = {3},
	journaltitle = {Cell Host \& Microbe},
	shortjournal = {Cell Host Microbe},
	author = {{Integrative HMP (iHMP) Research Network Consortium}},
	date = {2014-09-10},
	pmid = {25211071},
	pmcid = {PMC5109542},
	keywords = {Humans, Bacteria, Bacterial Physiological Phenomena, Diabetes Mellitus, Type 2, Health, Inflammatory Bowel Diseases, Microbiota, Premature Birth},
}

@article{weidner_composing_2020,
	title = {Composing and decomposing op-based {CRDTs} with semidirect products},
	volume = {4},
	url = {https://doi.org/10.1145/3408976},
	doi = {10.1145/3408976},
	abstract = {Operation-based Conflict-free Replicated Data Types ({CRDTs}) are eventually consistent replicated data types that automatically resolve conflicts between concurrent operations. Op-based {CRDTs} must be designed differently for each data type, and current designs use ad-hoc techniques to handle concurrent operations that do not naturally commute. We present a new construction, the semidirect product of op-based {CRDTs}, which combines the operations of two {CRDTs} into one while handling conflicts between their concurrent operations in a uniform way. We demonstrate the construction's utility by using it to construct novel {CRDTs}, as well as decomposing several existing {CRDTs} as semidirect products of simpler {CRDTs}. Although it reproduces common {CRDT} semantics, the semidirect product can be viewed as a restricted kind of operational transformation, thus forming a bridge between these two opposing techniques for constructing replicated data types.},
	pages = {94:1--94:27},
	issue = {{ICFP}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Weidner, Matthew and Miller, Heather and Meiklejohn, Christopher},
	urldate = {2020-08-27},
	date = {2020-08-02},
	keywords = {Eventual Consistency, {CRDTs}, Operational Transformation},
}

@article{pierce_large-scale_2019,
	title = {Large-scale sequence comparisons with sourmash},
	volume = {8},
	issn = {2046-1402},
	url = {https://f1000research.com/articles/8-1006/v1},
	doi = {10.12688/f1000research.19675.1},
	abstract = {The sourmash software package uses {MinHash}-based sketching to create “signatures”, compressed representations of {DNA}, {RNA}, and protein sequences, that can be stored, searched, explored, and taxonomically annotated. sourmash signatures can be used to estimate sequence similarity between very large data sets quickly and in low memory, and can be used to search large databases of genomes for matches to query genomes and metagenomes. sourmash is implemented in C++, Rust, and Python, and is freely available under the {BSD} license at http://github.com/dib-lab/sourmash.},
	pages = {1006},
	journaltitle = {F1000Research},
	shortjournal = {F1000Res},
	author = {Pierce, N. Tessa and Irber, Luiz and Reiter, Taylor and Brooks, Phillip and Brown, C. Titus},
	urldate = {2020-08-27},
	date = {2019-07-04},
	langid = {english},
}

@article{li_minimap2_2018,
	title = {Minimap2: pairwise alignment for nucleotide sequences},
	volume = {34},
	shorttitle = {Minimap2},
	pages = {3094--3100},
	number = {18},
	journaltitle = {Bioinformatics},
	author = {Li, Heng},
	date = {2018},
	note = {Publisher: Oxford University Press},
}

@online{noauthor_p1185-zhupdf_nodate,
	title = {p1185-zhu.pdf},
	url = {http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf},
	urldate = {2020-07-20},
}

@inproceedings{pandey_general-purpose_2017,
	title = {A general-purpose counting filter: Making every bit count},
	shorttitle = {A general-purpose counting filter},
	pages = {775--787},
	booktitle = {Proceedings of the 2017 {ACM} international conference on Management of Data},
	author = {Pandey, Prashant and Bender, Michael A. and Johnson, Rob and Patro, Rob},
	date = {2017},
}

@article{latombe_multi-site_2017,
	title = {Multi-site generalised dissimilarity modelling: using zeta diversity to differentiate drivers of turnover in rare and widespread species},
	volume = {8},
	rights = {© 2017 The Authors. Methods in Ecology and Evolution © 2017 British Ecological Society},
	issn = {2041-210X},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12756},
	doi = {10.1111/2041-210X.12756},
	shorttitle = {Multi-site generalised dissimilarity modelling},
	abstract = {Generalised dissimilarity modelling ({GDM}) applies pairwise beta diversity as a measure of species turnover with the purpose of explaining changes in species composition under changing environments or along environmental gradients. Beta diversity only captures turnover across pairs of sites and, therefore, disproportionately represents turnover in rare species across communities. By contrast, zeta diversity, the average number of shared species across multiple sites, captures the full spectrum of rare, intermediate and widespread species as they contribute differently to compositional turnover. We show how integrating zeta diversity into {GDMs} (which we term multi-site generalised dissimilarity modelling, {MS}-{GDM}), provides a more information rich approach to modelling how communities respond to environmental variation and change. We demonstrate the value of including zeta diversity in biodiversity assessment and modelling using {BirdLife} Australia Atlas data. Zeta diversity values for different numbers of sites (the order of zeta) are regressed against environmental differences and distance using two kinds of regressions: shape constrained additive models and a combination of I-splines and generalised linear models. Applying {MS}-{GDM} to different orders of zeta revealed shifts in the importance of environmental variables in explaining species turnover, varying with the order of zeta and thus with the level of co-occurrence of the species and, by extension, their commonness and rarity. In particular, precipitation gradients emerged as drivers in the turnover of rare species, whereas temperature gradients were more important drivers of turnover in widespread species. Appreciation of the factors that drive compositional turnover across multiple sites is necessary for accommodating the full spectrum of compositional turnover across rare to common species. This extends beyond understanding drivers for pairwise beta diversity only. {MS}-{GDM} provides a valuable addition to the toolkit of {GDM}, with further potential for survey gap analysis and prediction of species composition in unsampled sites.},
	pages = {431--442},
	number = {4},
	journaltitle = {Methods in Ecology and Evolution},
	author = {Latombe, Guillaume and Hui, Cang and {McGeoch}, Melodie A.},
	urldate = {2020-09-02},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12756},
	keywords = {alpha diversity, beta diversity, compositional turnover, generalised additive models, generalised dissimilarity modelling, I-splines, richness, shape constrained additive models},
}

@article{almodaresi_efficient_2020,
	title = {An Efficient, Scalable, and Exact Representation of High-Dimensional Color Information Enabled Using de Bruijn Graph Search},
	volume = {27},
	issn = {1557-8666},
	url = {https://www.liebertpub.com/doi/10.1089/cmb.2019.0322},
	doi = {10.1089/cmb.2019.0322},
	pages = {485--499},
	number = {4},
	journaltitle = {Journal of Computational Biology},
	shortjournal = {Journal of Computational Biology},
	author = {Almodaresi, Fatemeh and Pandey, Prashant and Ferdman, Michael and Johnson, Rob and Patro, Rob},
	urldate = {2020-09-02},
	date = {2020-04-01},
	langid = {english},
}

@article{bay_soil_2020,
	title = {Soil Bacterial Communities Exhibit Strong Biogeographic Patterns at Fine Taxonomic Resolution},
	volume = {5},
	rights = {Copyright © 2020 Bay et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license.},
	issn = {2379-5077},
	url = {https://msystems.asm.org/content/5/4/e00540-20},
	doi = {10.1128/mSystems.00540-20},
	abstract = {Bacteria have been inferred to exhibit relatively weak biogeographic patterns. To what extent such findings reflect true biological phenomena or methodological artifacts remains unclear. Here, we addressed this question by analyzing the turnover of soil bacterial communities from three data sets. We applied three methodological innovations: (i) design of a hierarchical sampling scheme to disentangle environmental from spatial factors driving turnover; (ii) resolution of 16S {rRNA} gene amplicon sequence variants to enable higher-resolution community profiling; and (iii) application of the new metric zeta diversity to analyze multisite turnover and drivers. At fine taxonomic resolution, rapid compositional turnover was observed across multiple spatial scales. Turnover was overwhelmingly driven by deterministic processes and influenced by the rare biosphere. The communities also exhibited strong distance decay patterns and taxon-area relationships, with z values within the interquartile range reported for macroorganisms. These biogeographical patterns were weakened upon applying two standard approaches to process community sequencing data: clustering sequences at 97\% identity threshold and/or filtering the rare biosphere (sequences lower than 0.05\% relative abundance). Comparable findings were made across local, regional, and global data sets and when using shotgun metagenomic markers. Altogether, these findings suggest that bacteria exhibit strong biogeographic patterns, but these signals can be obscured by methodological limitations. We advocate various innovations, including using zeta diversity, to advance the study of microbial biogeography.
{IMPORTANCE} It is commonly thought that bacterial distributions show lower spatial variation than for multicellular organisms. In this article, we present evidence that these inferences are artifacts caused by methodological limitations. Through leveraging innovations in sampling design, sequence processing, and diversity analysis, we provide multifaceted evidence that bacterial communities in fact exhibit strong distribution patterns. This is driven by selection due to factors such as local soil characteristics. Altogether, these findings suggest that the processes underpinning diversity patterns are more unified across all domains of life than previously thought, which has broad implications for the understanding and management of soil biodiversity.},
	number = {4},
	journaltitle = {{mSystems}},
	author = {Bay, Sean K. and {McGeoch}, Melodie A. and Gillor, Osnat and Wieler, Nimrod and Palmer, David J. and Baker, David J. and Chown, Steven L. and Greening, Chris},
	urldate = {2020-09-03},
	date = {2020-08-25},
	langid = {english},
	pmid = {32694128},
	note = {Publisher: American Society for Microbiology Journals
Section: Research Article},
}

@article{truong_metaphlan2_2015,
	title = {{MetaPhlAn}2 for enhanced metagenomic taxonomic profiling},
	volume = {12},
	rights = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3589},
	doi = {10.1038/nmeth.3589},
	pages = {902--903},
	number = {10},
	journaltitle = {Nature Methods},
	author = {Truong, Duy Tin and Franzosa, Eric A. and Tickle, Timothy L. and Scholz, Matthias and Weingart, George and Pasolli, Edoardo and Tett, Adrian and Huttenhower, Curtis and Segata, Nicola},
	urldate = {2020-09-04},
	date = {2015-10},
	langid = {english},
	note = {Number: 10
Publisher: Nature Publishing Group},
}

@article{segata_metagenomic_2012,
	title = {Metagenomic microbial community profiling using unique clade-specific marker genes},
	volume = {9},
	rights = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2066},
	doi = {10.1038/nmeth.2066},
	abstract = {{MetaPhlAn} (metagenomic phylogenetic analysis) allows the rapid and accurate identification of microbial species and higher clades from shotgun sequencing data.},
	pages = {811--814},
	number = {8},
	journaltitle = {Nature Methods},
	author = {Segata, Nicola and Waldron, Levi and Ballarini, Annalisa and Narasimhan, Vagheesh and Jousson, Olivier and Huttenhower, Curtis},
	urldate = {2020-09-04},
	date = {2012-08},
	langid = {english},
	note = {Number: 8
Publisher: Nature Publishing Group},
}

@article{meyer_assessing_2019,
	title = {Assessing taxonomic metagenome profilers with {OPAL}},
	volume = {20},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-019-1646-y},
	doi = {10.1186/s13059-019-1646-y},
	abstract = {The explosive growth in taxonomic metagenome profiling methods over the past years has created a need for systematic comparisons using relevant performance criteria. The Open-community Profiling Assessment {tooL} ({OPAL}) implements commonly used performance metrics, including those of the first challenge of the initiative for the Critical Assessment of Metagenome Interpretation ({CAMI}), together with convenient visualizations. In addition, we perform in-depth performance comparisons with seven profilers on datasets of {CAMI} and the Human Microbiome Project. {OPAL} is freely available at https://github.com/{CAMI}-challenge/{OPAL}.},
	pages = {51},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Meyer, Fernando and Bremges, Andreas and Belmann, Peter and Janssen, Stefan and {McHardy}, Alice C. and Koslicki, David},
	urldate = {2020-09-07},
	date = {2019-03-04},
}

@article{ma_patternhunter_2002,
	title = {{PatternHunter}: faster and more sensitive homology search},
	volume = {18},
	shorttitle = {{PatternHunter}},
	pages = {440--445},
	number = {3},
	journaltitle = {Bioinformatics},
	author = {Ma, Bin and Tromp, John and Li, Ming},
	date = {2002},
	note = {Publisher: Oxford University Press},
}

@article{leimeister_fast_2014,
	title = {Fast alignment-free sequence comparison using spaced-word frequencies},
	volume = {30},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/30/14/1991/2391234},
	doi = {10.1093/bioinformatics/btu177},
	abstract = {Abstract.  Motivation: Alignment-free methods for sequence comparison are increasingly used for genome analysis and phylogeny reconstruction; they circumvent va},
	pages = {1991--1999},
	number = {14},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Leimeister, Chris-Andre and Boden, Marcus and Horwege, Sebastian and Lindner, Sebastian and Morgenstern, Burkhard},
	urldate = {2020-09-10},
	date = {2014-07-15},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{dayhoff_model_1978,
	title = {A model of evolutionary change in proteins},
	volume = {5},
	pages = {345--352},
	journaltitle = {Atlas of protein sequence and structure},
	author = {Dayhoff, M. and Schwartz, R. and Orcutt, B.},
	date = {1978},
	note = {Publisher: National Biomedical Research Foundation Silver Spring {MD}},
}

@article{buchfink_fast_2015,
	title = {Fast and sensitive protein alignment using {DIAMOND}},
	volume = {12},
	rights = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3176},
	doi = {10.1038/nmeth.3176},
	abstract = {The open-source {DIAMOND} software provides protein alignment that is 20,000 times faster on short reads than {BLASTX} at similar sensitivity, for rapid analysis of large metagenomics data sets on a desktop computer.},
	pages = {59--60},
	number = {1},
	journaltitle = {Nature Methods},
	author = {Buchfink, Benjamin and Xie, Chao and Huson, Daniel H.},
	urldate = {2020-09-10},
	date = {2015-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
}

@article{gish_identification_1993,
	title = {Identification of protein coding regions by database similarity search},
	volume = {3},
	rights = {1993 Nature Publishing Group},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/ng0393-266},
	doi = {10.1038/ng0393-266},
	abstract = {Sequence similarity between a translated nucleotide sequence and a known biological protein can provide strong evidence for the presence of a homologous coding region, even between distantly related genes. The computer program {BLASTX} performed conceptual translation of a nucleotide query sequence followed by a protein database search in one programmatic step. We characterized the sensitivity of {BLASTX} recognition to the presence of substitution, insertion and deletion errors in the query sequence and to sequence divergence. Reading frames were reliably identified in the presence of 1\% query errors, a rate that is typical for primary sequence data. {BLASTX} is appropriate for use in moderate and large scale sequencing projects at the earliest opportunity, when the data are most prone to containing errors.},
	pages = {266--272},
	number = {3},
	journaltitle = {Nature Genetics},
	author = {Gish, Warren and States, David J.},
	urldate = {2020-09-10},
	date = {1993-03},
	langid = {english},
	note = {Number: 3
Publisher: Nature Publishing Group},
}

@article{peterson_reduced_2009,
	title = {Reduced amino acid alphabets exhibit an improved sensitivity and selectivity in fold assignment},
	volume = {25},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/25/11/1356/330730},
	doi = {10.1093/bioinformatics/btp164},
	abstract = {Abstract.  Motivation: Many proteins with vastly dissimilar sequences are found to share a common fold, as evidenced in the wealth of structures now available i},
	pages = {1356--1362},
	number = {11},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Peterson, Eric L. and Kondev, Jané and Theriot, Julie A. and Phillips, Rob},
	urldate = {2020-09-10},
	date = {2009-06-01},
	langid = {english},
	note = {Publisher: Oxford Academic},
}

@article{parks_standardized_2018,
	title = {A standardized bacterial taxonomy based on genome phylogeny substantially revises the tree of life},
	volume = {36},
	rights = {2018 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.4229},
	doi = {10.1038/nbt.4229},
	abstract = {Interpretation of microbial genome data will be improved by a fully revised bacterial taxonomy.},
	pages = {996--1004},
	number = {10},
	journaltitle = {Nature Biotechnology},
	author = {Parks, Donovan H. and Chuvochina, Maria and Waite, David W. and Rinke, Christian and Skarshewski, Adam and Chaumeil, Pierre-Alain and Hugenholtz, Philip},
	urldate = {2020-09-10},
	date = {2018-11},
	langid = {english},
	note = {Number: 10
Publisher: Nature Publishing Group},
}
